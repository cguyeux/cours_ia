{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0079cb0a-8f86-462f-9ed4-92a886d3bcbd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Les sorties structurées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ffa5cd-8e50-4ea5-83a0-32acc0114322",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Cas d'un booléen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd16a7ce-de15-4342-b612-0bec866e82af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noël est en hiver\n",
      "Réponse : True (type : <class 'bool'>)\n",
      "\n",
      "Il pleut quand il pleut pas\n",
      "Réponse : False (type : <class 'bool'>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    answer: bool\n",
    "\n",
    "prompt_answer = [\n",
    "    (\"system\", \"Tu es un assistant chargé de répondre un booléen (True ou False) à la question d'un utilisateur.\"),\n",
    "    ('human', \"{question}\")\n",
    "]\n",
    "\n",
    "prompt_answer_template = ChatPromptTemplate.from_messages(prompt_answer)\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "chain = prompt_answer_template | llm.with_structured_output(schema=Answer)\n",
    "\n",
    "def repond(question):\n",
    "    return chain.invoke({\"question\": question}).answer\n",
    "    \n",
    "for question in [\"Noël est en hiver\", \"Il pleut quand il pleut pas\"]:\n",
    "    print(question)\n",
    "    reponse = repond(question)\n",
    "    print(f\"Réponse : {reponse} (type : {type(reponse)})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57646f4-a101-441d-a491-cde12e06a8f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Cas d'une classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea1dad8b-78fd-4656-ac3e-e4865ef70c4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peux-tu m'en dire plus\n",
      "action=\"Fournir plus d'éléments à la question précédente\"\n",
      "\n",
      "Que sont les PPV ?\n",
      "action='Répondre à une nouvelle question'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "tasks = [\"Répondre à une nouvelle question\", \"Fournir plus d'éléments à la question précédente\"]\n",
    "\n",
    "class NextTask(BaseModel):\n",
    "    \"\"\"Utilise toujours cet outil pour structurer ta réponse to the user.\"\"\"\n",
    "    action: str = Field(..., \n",
    "                        enum=tasks,\n",
    "                        description=\"La prochaine action à mener\")\n",
    "\n",
    "prompt_message = [\n",
    "    (\"system\", \"Tu es un assistant chargé de classifier la demande d'un utilisateur parmi une \"\n",
    "               \"liste réduite d'actions à mener en tant que chatbot. Tu dois déterminer la \"\n",
    "               \"prochaine action à mener.\"),\n",
    "    ('human', \"{text}\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(prompt_message)\n",
    "llm = ChatMistralAI(model='mistral-large-latest', temperature=0)\n",
    "chain = prompt | llm.with_structured_output(schema=NextTask)\n",
    "\n",
    "for text in [\"Peux-tu m'en dire plus\", \"Que sont les PPV ?\"]:\n",
    "    print(text)\n",
    "    print(chain.invoke({\"text\": text}))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3bd17-0379-467b-9be9-06544afcaf0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Cas d'un entier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8b502d-8200-4ca8-b0c0-3969c86dab19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message : Bonjour, pourrais-tu m'aider s'il te plaît ?\n",
      "note_ton=4\n",
      "\n",
      "Message : J'ai besoin de ça immédiatement.\n",
      "note_ton=1\n",
      "\n",
      "Message : Merci beaucoup pour ton aide précieuse !\n",
      "note_ton=5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "class TonMessage(BaseModel):\n",
    "    \"\"\"Évaluation du ton du message de l'utilisateur.\"\"\"\n",
    "    note_ton: int = Field(\n",
    "        ..., \n",
    "        ge=1,\n",
    "        le=5,\n",
    "        description=\"Note attribuée au ton du message : 1 pour neutre, 5 pour très aimable\"\n",
    "    )\n",
    "\n",
    "prompt_message = [\n",
    "    (\"system\", \"Tu es un assistant chargé d'évaluer le ton d'un message donné par l'utilisateur. \"\n",
    "               \"Attribue une note de 1 à 5 au ton du message, où 1 signifie neutre et 5 signifie très aimable.\"),\n",
    "    ('human', \"{text}\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(prompt_message)\n",
    "llm = ChatMistralAI(model='mistral-large-latest', temperature=0)\n",
    "chain = prompt | llm.with_structured_output(schema=TonMessage)\n",
    "\n",
    "messages = [\n",
    "    \"Bonjour, pourrais-tu m'aider s'il te plaît ?\",\n",
    "    \"J'ai besoin de ça immédiatement.\",\n",
    "    \"Merci beaucoup pour ton aide précieuse !\"\n",
    "]\n",
    "\n",
    "for text in messages:\n",
    "    print(f\"Message : {text}\")\n",
    "    print(chain.invoke({\"text\": text}))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d36f5d-0be7-47cf-8b7d-88e0e28bae71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Pourquoi et comment forcer la sortie du LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76a94afe-6764-46bb-9aaf-7b4583eb1f5c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The translation of \"Quelle est la capitale de l\\'Albanie ?\" in English is:\\n\\n**\"What is the capital of Albania?\"**', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 26, 'total_tokens': 56, 'completion_tokens': 30}, 'model_name': 'mistral-medium-latest', 'model': 'mistral-medium-latest', 'finish_reason': 'stop'}, id='run--c4b5c2c7-5a43-4f1d-a93e-0ed2170a7818-0', usage_metadata={'input_tokens': 26, 'output_tokens': 30, 'total_tokens': 56})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-medium-latest\")\n",
    "message = HumanMessage(content=\"Peux-tu me traduire ce qui suit, en anglais ?\\n\\n Quelle est la capitale de l'Albanie ?\")\n",
    "llm.invoke([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e8c85e9-e6fa-4812-8718-50c1ee92a5cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Albania\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Translation(BaseModel):\n",
    "    original_text: str = Field(..., description=\"The original text before translation in another language\")\n",
    "    original_language: str = Field(..., description=\"The original language before translation\")\n",
    "    translated_text: str = Field(..., description=\"The final text after translation in another language\")\n",
    "    translated_language: str = Field(..., description=\"The language into which the translation must be done\")\n",
    "    \n",
    "def traduit(texte, langue_source=\"français\", langue_cible=\"anglais\"):\n",
    "    llm = ChatMistralAI(model_name=\"mistral-medium-latest\")\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"Je souhaite que tu traduises le texte suivant du {langue_source} vers le {langue_cible}. Ta traduction doit être précise, fluide et naturelle, et préserver parfaitement le sens original. \n",
    "    Retourne-moi la réponse sous forme d'objet JSON avec les champs :\n",
    "      - original_text : le texte original\n",
    "      - original_language : la langue du texte original\n",
    "      - translated_text : la traduction du texte\n",
    "      - translated_language : la langue de la traduction\n",
    "\n",
    "    Voici le texte à traduire :\n",
    "    ----\n",
    "    {texte}\"\"\")\n",
    "    output_parser = StrOutputParser()\n",
    "    extract_translation = RunnableLambda(lambda translation: translation.translated_text)\n",
    "    chain0 = prompt | llm.with_structured_output(Translation) | extract_translation\n",
    "    return chain0.invoke({\"langue_source\": langue_source,\n",
    "                            \"langue_cible\": \"anglais\",\n",
    "                            \"texte\": texte      \n",
    "                            })\n",
    "\n",
    "print(traduit(\"Quelle est la capitale de l'Albanie\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2032b-c358-4aec-b673-43b4e1afed83",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Des sorties structurées aux prémices d'un raisonnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "462e27ab-59d3-4bc3-aab0-37c73e620d48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "class Etape(BaseModel):\n",
    "    explication: str\n",
    "    sortie: str\n",
    "\n",
    "class MathReponse(BaseModel):\n",
    "    etapes: list[Etape]\n",
    "    reponse_finale: str\n",
    "\n",
    "prompt_answer = [\n",
    "    (\"system\", \"Tu es un professeur de mathématiques très pédagogue.\"),\n",
    "    ('human', \"{exercice}\")\n",
    "]\n",
    "\n",
    "prompt_answer_template = ChatPromptTemplate.from_messages(prompt_answer)\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "chain = prompt_answer_template | llm.with_structured_output(schema=MathReponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83f49b83-7fe3-475a-818b-43c232ddaea2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Soustrayons 31 des deux côtés de l'équation pour isoler le terme en x.\n",
      "  Le résultat est alors : 8x + 31 - 31 = 2 - 31\n",
      "8x = -29\n",
      "- Divisons les deux côtés par 8 pour résoudre x.\n",
      "  Le résultat est alors : 8x / 8 = -29 / 8\n",
      "x = -29/8 ou x = -3,625\n",
      "Au final, on trouve : x = -3,625\n"
     ]
    }
   ],
   "source": [
    "explications = chain.invoke({\"exercice\": \"Résous  8x + 31 = 2\"})\n",
    "for etape in explications.etapes:\n",
    "    print(f\"- {etape.explication}\")\n",
    "    print(f\"  Le résultat est alors : {etape.sortie}\")\n",
    "\n",
    "print(f\"Au final, on trouve : {explications.reponse_finale}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}