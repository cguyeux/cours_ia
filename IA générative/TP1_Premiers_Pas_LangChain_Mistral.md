# TP1 â€” Premiers pas avec la gÃ©nÃ©ration de texte (â‰ˆ 1h30)

[â¬…ï¸ Retour au sommaire](../LISEZMOI.md)

## Objectifs

- Comprendre les composants essentiels de LangChain et leur intÃ©rÃªt pour l'orchestration de modÃ¨les de langage (LLM).
- CrÃ©er et configurer un compte Mistral AI afin d'obtenir une clÃ© API personnelle.
- Mettre en place un environnement Python isolÃ© pour expÃ©rimenter avec LangChain et Mistral.
- RÃ©aliser vos premiÃ¨res invocations d'un modÃ¨le Mistral via LangChain, puis industrialiser cette interaction Ã  l'aide de prompts structurÃ©s et de chaÃ®nes.

## PrÃ©requis

- Python 3.10+ installÃ© localement.
- Un Ã©diteur de code (VS Code, PyCharm, etc.).
- Connaissances de base en Python (gestion des paquets, exÃ©cution de scripts).

## Ressources utiles

- **LangChain** : [Documentation](https://python.langchain.com/docs/introduction/) Â· [ChaÃ®ne YouTube](https://www.youtube.com/@LangChain)
- **Mistral AI** : [Documentation officielle](https://docs.mistral.ai/)
- **OpenAI** (rÃ©fÃ©rence optionnelle) : [Documentation](https://platform.openai.com)

## Ã‰tape 0 â€” CrÃ©ation du compte Mistral AI (â‰ˆ 10 min)

1. Rendez-vous sur [https://mistral.ai/](https://mistral.ai/).
2. Cliquez sur **Try the API** puis **S'inscrire**.
3. Choisissez l'abonnement **Gratuit / expÃ©rimental**.
4. Dans l'onglet **API â†’ ClÃ©s API**, crÃ©ez une nouvelle clÃ©.
5. Copiez cette clÃ© : vous en aurez besoin dans vos scripts Python (ne la partagez pas).

> ğŸ’¡ Vous pouvez aussi crÃ©er une clÃ© de test OpenAI pour comparer les comportements des modÃ¨les. Cela reste optionnel.

## Ã‰tape 1 â€” PrÃ©parer l'environnement Python (â‰ˆ 10 min)

1. CrÃ©ez un dossier de travail pour ce TP et ouvrez un terminal dans ce dossier.
2. CrÃ©ez un environnement virtuel :
   ```bash
   python -m venv mon_env
   source mon_env/bin/activate  # Sous Windows : mon_env\Scripts\activate
   ```
3. Installez les bibliothÃ¨ques nÃ©cessaires :
   ```bash
   pip install langchain langchain-mistralai langchain-openai python-dotenv
   ```
4. (Optionnel) Placez vos clÃ©s dans un fichier `.env` :
   ```env
   MISTRAL_API_KEY="votre-cle"
   OPENAI_API_KEY="votre-cle-openai"  # Optionnel
   ```
5. DÃ©sactivez l'environnement via `deactivate` quand vous avez terminÃ©.

> ğŸ’¡ `python-dotenv` permet de charger vos variables d'environnement sans les Ã©crire dans le code. Vous pouvez aussi dÃ©finir `export MISTRAL_API_KEY=...` directement dans le terminal.

## Ã‰tape 2 â€” Comprendre les notions de LangChain (â‰ˆ 10 min)

LangChain est un framework **agnostique** vis-Ã -vis des modÃ¨les qui facilite :

- l'invocation de diffÃ©rents LLM (Mistral, OpenAI, etc.) ;
- l'automatisation des prompts via des **chaÃ®nes** ;
- la gestion d'**agents** (raisonnement multi-Ã©tapes) ;
- l'intÃ©gration de **vector stores** et d'**embeddings** pour les applications RAG ;
- la construction d'architectures avancÃ©es (LangGraph, multi-agents...).

ğŸ‘‰ **Exercice 2.1** â€” En quelques phrases, dÃ©crivez un cas d'usage concret (professionnel ou personnel) qui pourrait tirer parti de LangChain.

ğŸ‘‰ **Exercice 2.2** â€” Identifiez, dans la documentation officielle, un exemple de chaÃ®ne ou d'agent que vous souhaiteriez tester plus tard. Indiquez la page ou la ressource correspondante.

## Ã‰tape 3 â€” PremiÃ¨re invocation Mistral (â‰ˆ 20 min)

CrÃ©ez un script `tp1_mistral.py` avec le contenu suivant :

```python
from langchain_mistralai.chat_models import ChatMistralAI
from langchain_core.messages import HumanMessage

llm = ChatMistralAI(
    model="mistral-large-latest",
    temperature=0,
    # api_key="..."  # facultatif si vous utilisez la variable d'environnement
)

message = HumanMessage(content="Quelle est la capitale de l'Albanie ?")
response = llm.invoke([message])

print(response.content)
```

1. Ajoutez l'import `import os` et utilisez `os.environ["MISTRAL_API_KEY"]` (ou `os.getenv`) pour sÃ©curiser la rÃ©cupÃ©ration de la clÃ©.
2. ExÃ©cutez le script et vÃ©rifiez la rÃ©ponse.
3. Remplacez la question par deux requÃªtes diffÃ©rentes pour observer la cohÃ©rence des sorties.

ğŸ‘‰ **Exercice 3.1** â€” En adaptant le script, affichez Ã©galement `response.usage_metadata` afin d'inspecter le nombre de tokens consommÃ©s. Calculez ensuite un coÃ»t approximatif en vous basant sur le tarif public (`input_tokens` et `output_tokens`).

ğŸ‘‰ **Exercice 3.2** â€” Modifiez la tempÃ©rature (`temperature=0.7`) et observez les diffÃ©rences de rÃ©ponses pour une requÃªte crÃ©ative.

> ğŸ’¡ Pour comparer, vous pouvez Ã©galement invoquer `ChatOpenAI` (modÃ¨le `gpt-4.1`) en adaptant deux lignes de code.

## Ã‰tape 4 â€” ChaÃ®nes et prompts structurÃ©s (â‰ˆ 25 min)

LangChain facilite la composition d'Ã©tapes (prompt â†’ modÃ¨le â†’ parser). Reprenez votre script dans un nouveau fichier `tp1_chain.py` :

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_mistralai.chat_models import ChatMistralAI

model = ChatMistralAI(model="mistral-large-latest", temperature=0)

prompt = ChatPromptTemplate.from_template(
    "Fais-moi une blague sur le sujet : {sujet}"
)
output_parser = StrOutputParser()

chain = prompt | model | output_parser

for sujet in ["pompier", "police"]:
    print(chain.invoke({"sujet": sujet}))
    print("-" * 10)
```

ğŸ‘‰ **Exercice 4.1** â€” Remplacez la liste de sujets par une liste saisie dynamiquement (prompt utilisateur ou lecture d'un fichier). Ajoutez un contrÃ´le qui empÃªche l'envoi d'une chaÃ®ne vide.

ğŸ‘‰ **Exercice 4.2** â€” CrÃ©ez une variante de la chaÃ®ne produisant des rÃ©ponses au format JSON (par exemple avec les clÃ©s `setup` et `punchline`). Pour cela, adaptez le prompt et utilisez `StrOutputParser()` ou un parser JSON.

ğŸ‘‰ **Exercice 4.3** â€” Ajoutez un paramÃ¨tre `temperature` passÃ© dynamiquement via `chain.invoke` pour comparer l'impact sur plusieurs sujets.

## Ã‰tape 5 â€” Prompt hiÃ©rarchisÃ© et mÃ©tadonnÃ©es (â‰ˆ 15 min)

Les prompts peuvent combiner un message systÃ¨me et un message utilisateur :

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_mistralai.chat_models import ChatMistralAI

prompt = ChatPromptTemplate.from_messages([
    ("system", "Vous Ãªtes un rÃ©dacteur de documentation technique de classe mondiale."),
    ("user", "{input}")
])

llm = ChatMistralAI(model="mistral-large-latest")
chain = prompt | llm

result = chain.invoke({"input": "Qu'est-ce que le modÃ¨le mistral-large-latest ?"})
print(result.content)
print(result.usage_metadata)
```

ğŸ‘‰ **Exercice 5.1** â€” Modifiez le message systÃ¨me pour orienter le style (ex. ton humoristique, rÃ©ponse concise, etc.). Testez plusieurs variantes et mesurez Ã  chaque fois l'impact sur `usage_metadata`.

ğŸ‘‰ **Exercice 5.2** â€” Ã€ partir de `usage_metadata`, estimez le coÃ»t de trois requÃªtes diffÃ©rentes. PrÃ©sentez vos calculs dans un tableau synthÃ©tique.

ğŸ‘‰ **Exercice 5.3** *(optionnel)* â€” Inspirez-vous de la documentation LangChain pour ajouter un `StrOutputParser` et ne rÃ©cupÃ©rer que du texte brut, puis enregistrez la sortie dans un fichier `.md`.

## Pour aller plus loin

- Explorez les **vector stores** et les embeddings pour mettre en place un mini-systÃ¨me RAG (Retrieval Augmented Generation).
- Testez les **LangGraphs** ou les **agents** pour orchestrer plusieurs Ã©tapes de raisonnement.
- Comparez les performances et coÃ»ts entre Mistral et un modÃ¨le OpenAI sur une tÃ¢che donnÃ©e.

## Rendu attendu

- Scripts Python (`tp1_mistral.py`, `tp1_chain.py`, variantes Ã©ventuelles).
- Un court compte rendu (1 page max) rÃ©sumant :
  - votre comprÃ©hension de LangChain et Mistral ;
  - les expÃ©rimentations menÃ©es ;
  - les coÃ»ts observÃ©s ;
  - les axes d'amÃ©lioration ou d'applications futures.

Bon TPâ€¯!
