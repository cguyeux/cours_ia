# TP5 â€” MultimodalitÃ© et IA gÃ©nÃ©rative (â‰ˆ 2h)

[â¬…ï¸ Retour au README](../README.md)

## Objectifs

- Comprendre ce que recouvre la multimodalitÃ© dans les API d'IA gÃ©nÃ©rative modernes.
- Mettre en place des pipelines audio (synthÃ¨se vocale â†” transcription) avec l'API OpenAI.
- Exploiter la vision et la gÃ©nÃ©ration d'images pour enrichir une application.
- Composer plusieurs modalitÃ©s dans un mini-projet intÃ©grant texte, image et audio.

## PrÃ©requis

- Python 3.10+ et un shell avec `ffmpeg` installÃ© (requis par `pydub` pour l'audio).
- Une clÃ© d'API OpenAI stockÃ©e dans `OPENAI_API_KEY` (ou un fichier `.env`).
- Avoir suivi les TP1 Ã  TP4 pour Ãªtre Ã  l'aise avec les appels API et la structuration de projets.

## Ressources utiles

- [Documentation OpenAI Python](https://platform.openai.com/docs/guides/setup) â€” clients `responses`, `audio`, `images`.
- [Guide TTS (`audio.speech`)](https://platform.openai.com/docs/guides/text-to-speech).
- [Guide transcription (`audio.transcriptions`)](https://platform.openai.com/docs/guides/speech-to-text).
- [RÃ©fÃ©rence multimodale (`responses`)](https://platform.openai.com/docs/guides/vision).
- [Pydub](https://github.com/jiaaro/pydub) pour manipuler l'audio cÃ´tÃ© Python.

---

## Ã‰tape 0 â€” Pourquoi la multimodalitÃ© ? (â‰ˆ 10 min)

La multimodalitÃ© permet Ã  un modÃ¨le de comprendre ou de produire plusieurs types de donnÃ©es (texte, image, audio, vidÃ©o). Les combinaisons les plus courantes en production restent **texte â†” image** et **texte â†” audio**.

- Les modÃ¨les *vision* interprÃ¨tent des pixels pour rÃ©pondre Ã  un prompt textuel.
- Les modÃ¨les *speech* synthÃ©tisent ou transcrivent une voix.
- Les API modernes (OpenAI `responses`, `audio`) autorisent des entrÃ©es mixtes : texte + image + instructions, ou encore texte + audio.

ğŸ‘‰ **Exercice 0.1** â€” Listez trois cas dâ€™usage multimodaux dans votre contexte professionnel (support client, maintenance, mÃ©diation culturelle...).

ğŸ‘‰ **Exercice 0.2** â€” Pour chacun, identifiez la combinaison de modalitÃ©s (entrÃ©e â†’ sortie) nÃ©cessaire.

---

## Ã‰tape 1 â€” PrÃ©parer lâ€™environnement (â‰ˆ 15 min)

1. CrÃ©ez un environnement isolÃ© et installez les dÃ©pendances :
   ```bash
   python -m venv tp5_multimodal
   source tp5_multimodal/bin/activate  # Windows : tp5_multimodal\Scripts\activate
   pip install --upgrade openai python-dotenv pydub pillow rich
   ```
2. Ajoutez `ffmpeg` Ã  votre PATH. Sous Ubuntu/Debian : `sudo apt install ffmpeg`. Sous Windows, installez la version officielle puis ajoutez le dossier `bin` Ã  la variable dâ€™environnement.
3. CrÃ©ez un fichier `.env` Ã  la racine de votre projet :
   ```env
   OPENAI_API_KEY="votre-cle"
   ```
4. Chargez la clÃ© dans vos scripts :
   ```python
   from dotenv import load_dotenv
   load_dotenv()
   ```

ğŸ‘‰ **Exercice 1.1** â€” VÃ©rifiez que `import openai; from openai import OpenAI` sâ€™exÃ©cutent sans erreur dans un REPL.

ğŸ‘‰ **Exercice 1.2** â€” Ajoutez `rich` comme dÃ©pendance pour colorer vos logs et construisez un petit utilitaire `log.py` rÃ©utilisable dans tout le TP.

---

## Ã‰tape 2 â€” Du texte vers la parole (Text-to-Speech) (â‰ˆ 20 min)

Lâ€™API `audio.speech` synthÃ©tise une voix naturelle Ã  partir dâ€™un prompt textuel. Les modÃ¨les `gpt-4o-mini-tts` et `gpt-4o-audio-preview` offrent un bon compromis qualitÃ©/coÃ»t.

```python
from pathlib import Path
from openai import OpenAI

client = OpenAI()

def synthesize_message(message: str, out_path: str, voice: str = "alloy") -> Path:
    """GÃ©nÃ¨re un fichier MP3 Ã  partir d'un texte."""
    output = Path(out_path)
    output.parent.mkdir(parents=True, exist_ok=True)

    with client.audio.speech.with_streaming_response.create(
        model="gpt-4o-mini-tts",
        voice=voice,
        input=message,
    ) as response:
        response.stream_to_file(output)

    return output

if __name__ == "__main__":
    path = synthesize_message(
        "Bonjour ! PrÃªt Ã  explorer la multimodalitÃ© avec OpenAI ?",
        "sorties/audio/introduction.mp3",
    )
    print(f"Fichier gÃ©nÃ©rÃ© : {path.resolve()}")
```

ğŸ‘‰ **Exercice 2.1** â€” Testez au moins trois voix (`alloy`, `verse`, `nova`, `solaria`...), comparez les timbres et notez vos prÃ©fÃ©rences.

ğŸ‘‰ **Exercice 2.2** â€” Ajoutez un paramÃ¨tre `language` (par exemple `"fr-FR"`, `"en-US"`) pour piloter la prononciation et faites varier le contenu (dialogue, consignes, storytelling).

---

## Ã‰tape 3 â€” De lâ€™audio vers le texte (Speech-to-Text) (â‰ˆ 25 min)

Pour transcrire des fichiers longs, dÃ©coupez-les en segments manipulables. `pydub` gÃ¨re la lecture/export, lâ€™API OpenAI se charge de la transcription.

```python
from pathlib import Path
from typing import List
from openai import OpenAI
from pydub import AudioSegment
import tempfile

client = OpenAI()

def transcribe_mp3(mp3_path: str, chunk_minutes: int = 2) -> str:
    """Transcrit un MP3 long en effectuant un traitement par blocs."""
    audio = AudioSegment.from_file(mp3_path)
    chunk_ms = chunk_minutes * 60 * 1000
    transcripts: List[str] = []

    with tempfile.TemporaryDirectory() as workdir:
        for index, start in enumerate(range(0, len(audio), chunk_ms)):
            segment = audio[start:start + chunk_ms]
            if len(segment) < 1_000:  # ignore les fragments trop courts
                continue

            chunk_path = Path(workdir) / f"chunk_{index}.mp3"
            segment.export(chunk_path, format="mp3")

            with chunk_path.open("rb") as chunk_file:
                result = client.audio.transcriptions.create(
                    model="gpt-4o-mini-transcribe",
                    file=chunk_file,
                    response_format="verbose_json",
                )
            transcripts.append(result.text.strip())

    return " ".join(transcripts)

if __name__ == "__main__":
    texte = transcribe_mp3("donnees/reunion.mp3")
    Path("sorties/transcriptions").mkdir(parents=True, exist_ok=True)
    (Path("sorties/transcriptions") / "reunion.txt").write_text(texte, encoding="utf-8")
    print("Transcription terminÃ©e.")
```

ğŸ‘‰ **Exercice 3.1** â€” Mesurez le temps de traitement par bloc (utilisez `time.perf_counter`). Quelle taille de segment optimise le rapport coÃ»t/dÃ©lai ?

ğŸ‘‰ **Exercice 3.2** â€” ExpÃ©rimentez `response_format="json"` et parsez les horodatages (`segments`) pour reconstituer une timeline dÃ©taillÃ©e.

---

## Ã‰tape 4 â€” Interroger une image (Vision) (â‰ˆ 30 min)

Les appels `responses.create` acceptent des contenus mixtes. Ci-dessous, on convertit une image locale en data URL pour lâ€™envoyer au modÃ¨le `gpt-4.1-mini`.

```python
import base64
import mimetypes
from pathlib import Path
from openai import OpenAI

client = OpenAI()

def encode_image_to_data_url(image_path: str) -> str:
    """Encode une image locale en data URL (base64) prÃªte pour lâ€™API."""
    path = Path(image_path)
    if not path.exists():
        raise FileNotFoundError(path)

    mime_type, _ = mimetypes.guess_type(path.name)
    if mime_type is None:
        raise ValueError(f"Impossible de dÃ©duire le MIME type de {path}")

    image_bytes = path.read_bytes()
    b64 = base64.b64encode(image_bytes).decode("utf-8")
    return f"data:{mime_type};base64,{b64}"


def describe_image(image_path: str, question: str) -> str:
    data_url = encode_image_to_data_url(image_path)
    response = client.responses.create(
        model="gpt-4.1-mini",
        input=[
            {
                "role": "user",
                "content": [
                    {"type": "input_text", "text": question},
                    {"type": "input_image", "image_url": data_url},
                ],
            }
        ],
        temperature=0.1,
        max_output_tokens=300,
    )
    texts = []
    for item in response.output:
        for content in item.content:
            if content.type == "output_text":
                texts.append(content.text.strip())
    return "\n".join(texts)


if __name__ == "__main__":
    analyse = describe_image("img/tableau_de_bord.png", "Que peux-tu dÃ©duire de cette capture d'Ã©cran ?")
    print(analyse)
```

ğŸ‘‰ **Exercice 4.1** â€” Remplacez lâ€™image par une photo de document manuscrit. Comparez la qualitÃ© de lâ€™abrÃ©gÃ© produit par `gpt-4.1-mini` vs `gpt-4.1`.

ğŸ‘‰ **Exercice 4.2** â€” Ajoutez une contrainte de formatage (par exemple : bullet points, JSON) et vÃ©rifiez la robustesse de la rÃ©ponse.

---

## Ã‰tape 5 â€” GÃ©nÃ©rer une image (â‰ˆ 20 min)

ComplÃ©tez la boucle en produisant un visuel Ã  partir dâ€™un prompt textuel. Lâ€™API `images.generate` renvoie du base64 quâ€™il suffit de sauvegarder.

```python
import base64
from pathlib import Path
from openai import OpenAI

client = OpenAI()

def generate_image(prompt: str, out_path: str, size: str = "1024x1024") -> Path:
    """GÃ©nÃ¨re une image et la sauvegarde localement."""
    output = Path(out_path)
    output.parent.mkdir(parents=True, exist_ok=True)

    result = client.images.generate(
        model="gpt-image-1",
        prompt=prompt,
        size=size,
        quality="high",
    )
    image_b64 = result.data[0].b64_json
    output.write_bytes(base64.b64decode(image_b64))
    return output


if __name__ == "__main__":
    chemin = generate_image(
        "Poster minimaliste illustrant la rencontre entre l'audio et la vision en IA.",
        "sorties/images/multimodal.png",
    )
    print(f"Image gÃ©nÃ©rÃ©e : {chemin.resolve()}")
```

ğŸ‘‰ **Exercice 5.1** â€” ParamÃ©trez `size` (`512x512`, `1792x1024`...) et `quality` (`standard`, `high`). Observez lâ€™impact sur le rendu et le temps de gÃ©nÃ©ration.

ğŸ‘‰ **Exercice 5.2** â€” CrÃ©ez trois prompts diffÃ©rents puis rÃ©alisez un vote en binÃ´me pour sÃ©lectionner la meilleure image.

---

## Ã‰tape 6 â€” Composer un pipeline multimodal (â‰ˆ 30 min)

Assemblez les briques audio + vision pour livrer une expÃ©rience complÃ¨te. Commencez par factoriser vos fonctions dans deux modules (`pipeline_audio.py` pour `synthesize_message`, `pipeline_vision.py` pour `describe_image`). Exemple : analyser une infographie, rÃ©sumer en texte puis gÃ©nÃ©rer un audio de synthÃ¨se.

```python
from pathlib import Path
from pipeline_audio import synthesize_message
from pipeline_vision import describe_image

def analyse_visuelle_vers_audio(image_path: str, question: str) -> Path:
    """ChaÃ®ne complÃ¨te vision â†’ texte â†’ audio."""
    resume = describe_image(image_path, question)
    print("RÃ©sumÃ© gÃ©nÃ©rÃ© :\n", resume)
    sortie_audio = synthesize_message(
        message=f"Voici mon analyse : {resume}",
        out_path="sorties/audio/analyse_visuelle.mp3",
        voice="verse",
    )
    return sortie_audio


if __name__ == "__main__":
    audio_path = analyse_visuelle_vers_audio(
        "img/dashboard_industrie.png",
        "Identifie les KPI principaux et donne les points de vigilance.",
    )
    print(f"Analyse audio disponible : {audio_path.resolve()}")
```

ğŸ‘‰ **Exercice 6.1** â€” Ajoutez une Ã©tape de traduction (franÃ§ais â†’ anglais) avant la synthÃ¨se vocale finalisÃ©e.

ğŸ‘‰ **Exercice 6.2** â€” Industrialisez le pipeline via une CLI (`argparse`) ou une API FastAPI minimale.

---

## Pour aller plus loin

- ImplÃ©mentez un chatbot vocal : streaming micro â†’ transcription â†’ rÃ©ponse texte â†’ synthÃ¨se vocale â†’ restitution en temps rÃ©el.
- Comparez OpenAI avec un fournisseur alternatif (par exemple, `Deepgram` pour la transcription ou `Stability AI` pour lâ€™image) et consignez vos observations.
- Mesurez et loggez les mÃ©triques dâ€™usage (`usage.total_tokens`, temps de traitement, coÃ»ts estimÃ©s) pour prÃ©parer une mise en production.

ğŸ‘‰ **Exercice final** â€” Concevez en binÃ´me un mini-projet multimodal complet (ex. guide touristique audio illustrÃ©). PrÃ©sentez aux autres groupes le pipeline, les modÃ¨les choisis, les limites identifiÃ©es et une dÃ©monstration.
