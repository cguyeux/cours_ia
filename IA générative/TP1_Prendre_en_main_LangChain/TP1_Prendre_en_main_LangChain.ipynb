{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313ed1c5-8452-4b3b-ae5a-28e726ff077f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.display import HTML\n",
    "\n",
    "css_adjustment = \"\"\"\n",
    "<style>\n",
    "/* Cacher les prompts de commande */\n",
    "div.prompt {display: none !important;}\n",
    "\n",
    "/* Ajuster la largeur max des cellules de code et de markdown */\n",
    ".jp-RenderedMarkdown, .jp-InputArea {\n",
    "    max-width: 1500px !important; /* Assurez-vous que cela correspond à la largeur du Markdown si nécessaire */\n",
    "}\n",
    "\n",
    "/* Modifier la taille de la police dans les cellules de code */\n",
    ".jp-Notebook .jp-InputArea .input_area {\n",
    "    font-size: 40px !important; /* Ajustez selon vos besoins */\n",
    "}\n",
    "\n",
    "/* Si vous souhaitez ajuster la taille de la police à l'intérieur des blocs de code eux-mêmes */\n",
    ".jp-Notebook .jp-InputArea .input_area pre, \n",
    ".jp-Notebook .jp-InputArea .input_area code {\n",
    "    font-size: 40px !important; /* Ajustez selon vos besoins */\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "HTML(css_adjustment)\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4226d077-353f-4227-8687-4df42cf0b3a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Programmer avec des LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d98f9-cfeb-4dd1-9a3a-a10138b8b941",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Objectif de ces Travaux Pratiques\n",
    "\n",
    "- Courte introduction à \"comment programmer avec des LLM\"\n",
    "- Uniquement de la pratique (pas le temps pour la théorie)\n",
    "- Evaluation : \n",
    "  - Projet de votre choix de programmation par des LLM\n",
    "  - Note = fonction(quantite_de_travail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d7787-7336-46ec-a985-07dfbb294955",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9bfa5-585a-497e-a580-0a682197168b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### LangChain\n",
    "- Framework Python d'intégration de modèles LLM\n",
    "  - Model agnostic \n",
    "  - Chaînes et Agents\n",
    "  - Automatisation des prompts\n",
    "  - Gestion des vectorstores, embeddings...\n",
    "\n",
    "- Cas d’usage : chatbots (RAG), programmes embarquant des LLM, LangGraphs, multi-agents..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3019664-f310-4123-bb6a-e71f6e33864e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Mistral AI\n",
    "\n",
    "1. Créez un compte sur La Plateforme de Mistral AI. https://mistral.ai/\n",
    "\n",
    "   (\"Try the API\" -> \"S'inscrire\")\n",
    "\n",
    "   (Abonnement : Gratuit / expérimental)\n",
    "   \n",
    "3. Générez une clé API personnelle.\n",
    "\n",
    "   (API -> Clés API -> Créer une nouvelle clé -> ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce133e4-5ed4-4e7c-924e-2951af66c2cf",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Environnement virtuel\n",
    "\n",
    "Il y aura potentiellement des bibliothèques à installer, ce qui peut se faire dans un environnement virtuel python.\n",
    "\n",
    "  $ python -m venv mon\\_env\n",
    "  \n",
    "  $ source mon\\_env/bin/activate\n",
    "  \n",
    "  (mon\\_env) $ pip install nom\\_bibliotheque\n",
    "\n",
    "(Et pour désactiver : deactivate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2addb622-d7a9-4847-bef2-b573f2ee8e27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Références\n",
    "- LangChain\n",
    "  - https://python.langchain.com/docs/introduction/\n",
    "  - https://www.youtube.com/@LangChain\n",
    "- Mistral AI\n",
    "  - https://docs.mistral.ai/\n",
    "- OpenAI\n",
    "  - https://platform.openai.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3ec0b-529d-4b3a-8fb8-aa1bdca2b1ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Introduction à LangChain / Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171308f5-cbab-4a41-b1eb-daa5e85a3bc1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Premières invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e601d0-9ba2-44df-b782-a139ef988eca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capitale de l'Albanie est Tirana.\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialiser le modèle\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", \n",
    "                    temperature=0)\n",
    "                    #api_key=\"....\")\n",
    "\n",
    "# Créer le message utilisateur\n",
    "message = HumanMessage(content=\"Quelle est la capitale de l'Albanie ?\")\n",
    "\n",
    "# Obtenir la réponse\n",
    "response = llm.invoke([message])\n",
    "\n",
    "# Afficher la réponse\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84dbc14e-6d82-4efa-a6e1-1ed98ba17048",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capitale de l’Albanie est Tirana.\n"
     ]
    }
   ],
   "source": [
    "# Version OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1\")\n",
    "response = llm.invoke(\"Quelle est la capitale de l'Albanie ?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4bcfe1-8c3e-40b6-97f7-a42dc07baac6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourquoi les pompiers ne jouent-ils jamais à cache-cache ?\n",
      "\n",
      "Parce qu'ils trouvent toujours la cachette en feu !\n",
      "----------\n",
      "Pourquoi les policiers n'aiment-ils pas les puzzles ?\n",
      "\n",
      "Parce qu'ils préfèrent les casse-têtes !\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# 1) Initialisation du modèle Mistral\n",
    "model = ChatMistralAI(model=\"mistral-large-latest\", \n",
    "                      temperature=0)\n",
    "\n",
    "# 2) Construction du prompt + parser\n",
    "prompt = ChatPromptTemplate.from_template(\"Fais-moi une blague sur le sujet : {sujet}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 3) Chaînage (LangChain Expression Language)\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# 4) Exécution\n",
    "for sujet in ['pompier', 'police']:\n",
    "    print(chain.invoke({\"sujet\": sujet}))\n",
    "    print('-'*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a07cb0e4-94b6-4e04-8959-40435196d1ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Vous êtes un rédacteur de documentation technique de classe mondiale.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "chain = prompt | llm\n",
    "result = chain.invoke({\"input\": \"Qu'est-ce que le modèle mistral-large-latest ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dfaa652-299d-4d7a-a480-4e17a079cf19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 38, 'output_tokens': 701, 'total_tokens': 739}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "120dac24-febc-4714-ad88-8033d5bd9d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056156"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "38/1000000*2+701*8/100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb7b1825-15de-4b92-8d7e-7ff0e670e212",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral-large-latest est un modèle de langage développé par Mistral AI, une société de pointe dans le domaine de l'intelligence artificielle. Ce modèle est conçu pour comprendre et générer du texte en réponse à une variété d'entrées, ce qui le rend utile pour diverses applications telles que la rédaction de documentation technique, la traduction, la génération de contenu, et bien plus encore.\n",
      "\n",
      "### Caractéristiques principales de Mistral-large-latest :\n",
      "\n",
      "1. **Taille et Capacité** :\n",
      "   - Le modèle est de grande taille, ce qui lui permet de comprendre et de générer du texte de manière très sophistiquée.\n",
      "   - Il est entraîné sur une vaste quantité de données textuelles, ce qui lui confère une large compréhension du langage naturel.\n",
      "\n",
      "2. **Polyvalence** :\n",
      "   - Mistral-large-latest peut être utilisé pour une variété de tâches, y compris la rédaction de documents techniques, la réponse à des questions, la génération de contenu créatif, et même la traduction entre différentes langues.\n",
      "\n",
      "3. **Précision et Cohérence** :\n",
      "   - Le modèle est conçu pour produire des réponses précises et cohérentes, ce qui est crucial pour des applications professionnelles et techniques.\n",
      "\n",
      "4. **Mise à Jour Régulière** :\n",
      "   - Le terme \"latest\" indique que le modèle est régulièrement mis à jour pour inclure les dernières améliorations et corrections, assurant ainsi des performances optimales.\n",
      "\n",
      "### Utilisations Potentielles :\n",
      "\n",
      "1. **Rédaction de Documentation Technique** :\n",
      "   - Mistral-large-latest peut aider à rédiger des manuels, des guides d'utilisation, des spécifications techniques, et d'autres documents nécessitant une grande précision et clarté.\n",
      "\n",
      "2. **Support Client** :\n",
      "   - Il peut être intégré dans des systèmes de support client pour fournir des réponses automatisées aux questions fréquentes des utilisateurs.\n",
      "\n",
      "3. **Génération de Contenu** :\n",
      "   - Le modèle peut être utilisé pour générer du contenu créatif, comme des articles de blog, des scripts, ou même des histoires.\n",
      "\n",
      "4. **Traduction** :\n",
      "   - Il peut aider à traduire des textes entre différentes langues, facilitant ainsi la communication internationale.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Mistral-large-latest est un outil puissant et polyvalent pour toute personne ou organisation ayant besoin de solutions avancées en traitement du langage naturel. Sa capacité à comprendre et générer du texte de manière précise et cohérente en fait un choix idéal pour une variété d'applications professionnelles et techniques.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}