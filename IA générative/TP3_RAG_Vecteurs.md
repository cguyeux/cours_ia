# TP3 â€” SystÃ¨mes RAG et bases vectorielles (â‰ˆ 2h)

[â¬…ï¸ Retour au sommaire](../LISEZMOI.md)

## Objectifs

- Comprendre comment reprÃ©senter un texte sous forme de vecteur pour raisonner sur sa **sÃ©mantique**.
- Mettre en place une premiÃ¨re pipeline de **Retrieval-Augmented Generation (RAG)** avec LangChain.
- ExpÃ©rimenter diffÃ©rentes familles d'**embeddings** (bag-of-words, Transformers, API OpenAI) et mesurer leurs similaritÃ©s.
- Construire un **vector store** FAISS/Hugging Face puis OpenAI pour interroger un corpus documentaire.

> ğŸ’¡ **Conseil** â€” Ce TP prolonge le TP2 sur les sorties structurÃ©es. Nous allons maintenant structurer le *contexte* fourni au modÃ¨le Ã  l'aide d'embeddings et de moteurs de recherche sÃ©mantique.

![Illustration RAG](img/rag.png)

## PrÃ©requis

- Python â‰¥ 3.10 et un environnement virtuel actif.
- BibliothÃ¨ques recommandÃ©es : `scikit-learn`, `numpy`, `sentence-transformers`, `langchain`, `langchain-community`, `langchain-mistralai`, `langchain-huggingface`, `langchain-openai`, `faiss-cpu`.
- ClÃ©s API valides pour Mistral AI et, si vous souhaitez tester les embeddings distants, OpenAI.
- Avoir consultÃ© le dossier `images/Guyeux_2024.pdf` placÃ© dans `IA gÃ©nÃ©rative/`.

---

## Ã‰tape 1 â€” Pourquoi encoder le texte ? (â‰ˆ 10 min)

Un embedding transforme un texte en vecteur numÃ©rique. Deux textes proches sÃ©mantiquement deviennent deux vecteurs proches dans l'espace vectoriel.

![Vecteurs et sÃ©mantique](img/vectors-and-semantics.png)

ğŸ‘‰ **Exercice 1.1** â€” Listez trois applications concrÃ¨tes (moteur de recherche, recommandation, dÃ©tection de doublons, etc.) oÃ¹ la proximitÃ© sÃ©mantique est dÃ©terminante.

ğŸ‘‰ **Exercice 1.2** â€” Ouvrez `img/rag2.png` et dÃ©crivez, en quelques phrases, le rÃ´le de chaque composant d'une architecture RAG.

---

## Ã‰tape 2 â€” Bag of Words : premiÃ¨re reprÃ©sentation (â‰ˆ 15 min)

CrÃ©ez `tp3_bow.py` avec le code suivant :

```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    "Demonstration text, first document",
    "Demo text, and here's a second document.",
    "And finally, this is the third document."
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)

print("Vocabulary :", vectorizer.get_feature_names_out())
print("BoW vector:\n", X.toarray())
```

![Bag of Words](img/Bag-of-words.png)

ğŸ‘‰ **Exercice 2.1** â€” Ajoutez un quatriÃ¨me document contenant un synonyme d'un mot existant. Que se passe-t-il dans la matrice ?

ğŸ‘‰ **Exercice 2.2** â€” Limites : notez deux faiblesses de cette approche pour capturer la sÃ©mantique.

---

## Ã‰tape 3 â€” Embeddings par Transformers (â‰ˆ 20 min)

Passons Ã  des reprÃ©sentations contextuelles avec `sentence-transformers`.

```python
from sentence_transformers import SentenceTransformer

sentences = [
    "This is an example sentence.",
    "Each sentence is converted into a fixed-sized vector."
]

model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2", device="cpu")
embeddings = model.encode(sentences)

for sentence, embedding in zip(sentences, embeddings):
    print(f"{sentence!r} -> {embedding[:3]}...")

print(f"Embedding size: {len(embeddings[0])}")
```

ğŸ‘‰ **Exercice 3.1** â€” Faites varier le modÃ¨le (`all-MiniLM-L12-v2`, `all-distilroberta-v1`, etc.) et comparez la taille des vecteurs.

ğŸ‘‰ **Exercice 3.2** â€” Pourquoi force-t-on `device="cpu"` ? Essayez sans cette option et observez l'erreur remontÃ©e sur votre machine.

> ğŸ§  **Note** â€” Les modÃ¨les multilingues (ex. `paraphrase-multilingual-MiniLM-L12-v2`) offrent un meilleur support du franÃ§ais.

---

## Ã‰tape 4 â€” Mesurer la similaritÃ© cosinus (â‰ˆ 10 min)

Ajoutez ce bloc dans `tp3_bow.py` ou crÃ©ez `tp3_similarity.py` :

```python
import numpy as np

def cosine_similarity(A, B):
    dot_product = np.dot(A, B)
    norm_A = np.linalg.norm(A)
    norm_B = np.linalg.norm(B)
    return dot_product / (norm_A * norm_B)

print(cosine_similarity(embeddings[0], embeddings[1]))
```

ğŸ‘‰ **Exercice 4.1** â€” Calculez la similaritÃ© entre deux phrases trÃ¨s diffÃ©rentes. Quel comportement observez-vous ?

ğŸ‘‰ **Exercice 4.2** â€” ImplÃ©mentez une fonction qui renvoie la phrase la plus proche d'une requÃªte donnÃ©e parmi une liste.

---

## Ã‰tape 5 â€” Tester les embeddings OpenAI (â‰ˆ 15 min)

```python
from openai import OpenAI

client = OpenAI()

def embed(text, model="text-embedding-3-large", dimensions=3072):
    return client.embeddings.create(
        input=[text],
        model=model,
        dimensions=dimensions
    ).data[0].embedding

vector1 = embed("What is Mycobacterium kansasii ?")
vector2 = embed(
    "To sum up, we have presented a case of Mycobacterium kansasii monoarthritis..."
)

print(cosine_similarity(vector1, vector2))
```

ğŸ‘‰ **Exercice 5.1** â€” Comparez le score de similaritÃ© avec celui obtenu via `sentence-transformers`.

ğŸ‘‰ **Exercice 5.2** â€” RÃ©duisez `dimensions` Ã  256. Quel impact sur la qualitÃ© (Ã  tester empiriquement) ?

> âš ï¸ **Attention** â€” L'utilisation des API OpenAI est payante. Fixez un quota dans votre tableau de bord.

---

## Ã‰tape 6 â€” Premiers pas en RAG (â‰ˆ 15 min)

![SchÃ©ma RAG](img/rag2.png)

CrÃ©ez `tp3_rag_basique.py` :

```python
from langchain_mistralai.chat_models import ChatMistralAI

llm = ChatMistralAI(model_name="mistral-large-latest")

query = "What is Mycobacterium kansasii ?"
context = (
    "To sum up, we have presented a case of Mycobacterium kansasii monoarthritis..."
)

prompt = f"""You are an expert in the Mycobacterium field.\n"""
"""Answer the question using ONLY the context provided.\n\n"""
"""Question: {query}\n\nContext: {context}"""

response = llm.invoke(prompt)
print(response.content)
```

ğŸ‘‰ **Exercice 6.1** â€” Modifiez la consigne pour forcer l'IA Ã  rÃ©pondre en franÃ§ais.

ğŸ‘‰ **Exercice 6.2** â€” Ajoutez une Ã©tape de validation : si le contexte ne contient pas l'information, le modÃ¨le doit rÃ©pondre Â« Je ne sais pas Â».

---

## Ã‰tape 7 â€” Construire un vector store FAISS (â‰ˆ 25 min)

Nous allons indexer le PDF `images/Guyeux_2024.pdf`.

```python
import warnings
from textwrap import shorten, fill

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

warnings.simplefilter("ignore")

loader = PyPDFLoader("images/Guyeux_2024.pdf")
pages = loader.load_and_split()

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
faiss_index = FAISS.from_documents(pages, embeddings)

docs = faiss_index.similarity_search(
    "Is there a lineage 10 in M.tuberculosis?",
    k=2
)

for doc in docs:
    print(f"Page {doc.metadata['page']}: {fill(shorten(doc.page_content, 500), 80)}\n")
```

ğŸ‘‰ **Exercice 7.1** â€” Changez la requÃªte pour trouver la localisation gÃ©ographique d'une lignÃ©e.

ğŸ‘‰ **Exercice 7.2** â€” Ajoutez une Ã©tape RAG complÃ¨te : formatez un prompt `Question + Contexte concatÃ©nÃ©` et envoyez-le au LLM.

![Base vectorielle Milvus](img/Milvus.png)

> ğŸ“· **Astuce** â€” Comparez les fonctionnalitÃ©s de Milvus Ã  celles de FAISS (filtrage, multi-embeddings, gestion distribuÃ©e, etc.).

---

## Ã‰tape 8 â€” Utiliser les embeddings OpenAI dans FAISS (â‰ˆ 10 min)

```python
from langchain_openai import OpenAIEmbeddings

loader = PyPDFLoader("images/Guyeux_2024.pdf")
pages = loader.load_and_split()

faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())
docs = faiss_index.similarity_search("Is there a lineage 10 in M.tuberculosis?", k=2)

for doc in docs:
    print(doc.metadata, doc.page_content[:200], "...")
```

ğŸ‘‰ **Exercice 8.1** â€” Mesurez le temps d'indexation avec les embeddings OpenAI vs HuggingFace.

ğŸ‘‰ **Exercice 8.2** â€” Ajoutez un mÃ©canisme de cache local pour Ã©viter les appels rÃ©pÃ©tÃ©s (suggestion : `langchain-core.cache`).

---

## Ã‰tape 9 â€” Splitting et nettoyage des textes (â‰ˆ 15 min)

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

text = """Vous pouvez partager un article en cliquant sur les icÃ´nes..."""

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=200,
    keep_separator=False,
    separators=["\n\n", "\n", ". "]
)

texts = text_splitter.create_documents([text])

for chunk in texts[:5]:
    print(chunk.page_content)
    print("=" * 20)
```

ğŸ‘‰ **Exercice 9.1** â€” Testez diffÃ©rents couples `chunk_size` / `chunk_overlap`. Comment Ã©volue la cohÃ©rence des rÃ©ponses ?

ğŸ‘‰ **Exercice 9.2** â€” Impliquez le splitter dans votre pipeline FAISS : re-splitez le PDF avant indexation.

---

## Ã‰tape 10 â€” Autres loaders et vector stores (â‰ˆ 10 min)

```python
from langchain_community.document_loaders import YoutubeLoader

loader = YoutubeLoader.from_youtube_url(
    "https://www.youtube.com/watch?v=YcIbZGTRMjI",
    language=["fr"],
    add_video_info=False
)

print(loader.load())
```

ğŸ‘‰ **Exercice 10.1** â€” Identifiez d'autres loaders utiles pour vos donnÃ©es (Markdown, HTML, bases SQL...).

ğŸ‘‰ **Exercice 10.2** â€” Comparez FAISS Ã  d'autres vector stores (Chroma, Milvus...). Quels scÃ©narios justifient un dÃ©ploiement plus complexe ?

---

## SynthÃ¨se

1. **Embeddings** : vous avez manipulÃ© plusieurs reprÃ©sentations vectorielles (BoW, Transformers, API).
2. **SimilaritÃ©** : vous savez mesurer la proximitÃ© sÃ©mantique via le cosinus.
3. **RAG** : vous avez orchestrÃ© un pipeline allant du chargement de documents Ã  la gÃ©nÃ©ration guidÃ©e.
4. **Industrialisation** : les notions de text splitting, loaders et vector stores ouvrent la voie Ã  des applications robustes.

![Prompting structurÃ©](img/promptulate.png)

## Pour aller plus loin

- Ã‰valuez vos rÃ©sultats avec des mÃ©triques dÃ©diÃ©es (ex. `RetrievalQAWithSourcesChain` + mesures de prÃ©cision/rappel).
- ExpÃ©rimentez Milvus ou Weaviate pour gÃ©rer plusieurs espaces d'embeddings et du filtrage mÃ©tadonnÃ©es.
- Combinez ce TP avec le TP2 pour contraindre la forme finale des rÃ©ponses gÃ©nÃ©rÃ©es.

Bon TP et bonnes explorations !
