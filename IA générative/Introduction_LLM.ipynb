{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "313ed1c5-8452-4b3b-ae5a-28e726ff077f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.display import HTML\n",
    "\n",
    "css_adjustment = \"\"\"\n",
    "<style>\n",
    "/* Cacher les prompts de commande */\n",
    "div.prompt {display: none !important;}\n",
    "\n",
    "/* Ajuster la largeur max des cellules de code et de markdown */\n",
    ".jp-RenderedMarkdown, .jp-InputArea {\n",
    "    max-width: 1500px !important; /* Assurez-vous que cela correspond √† la largeur du Markdown si n√©cessaire */\n",
    "}\n",
    "\n",
    "/* Modifier la taille de la police dans les cellules de code */\n",
    ".jp-Notebook .jp-InputArea .input_area {\n",
    "    font-size: 40px !important; /* Ajustez selon vos besoins */\n",
    "}\n",
    "\n",
    "/* Si vous souhaitez ajuster la taille de la police √† l'int√©rieur des blocs de code eux-m√™mes */\n",
    ".jp-Notebook .jp-InputArea .input_area pre, \n",
    ".jp-Notebook .jp-InputArea .input_area code {\n",
    "    font-size: 40px !important; /* Ajustez selon vos besoins */\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "HTML(css_adjustment)\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4226d077-353f-4227-8687-4df42cf0b3a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Programmer avec des LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d98f9-cfeb-4dd1-9a3a-a10138b8b941",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Objectif de ces Travaux Pratiques\n",
    "\n",
    "- Courte introduction √† \"comment programmer avec des LLM\"\n",
    "- Uniquement de la pratique (pas le temps pour la th√©orie)\n",
    "- Evaluation : \n",
    "  - Projet de votre choix de programmation par des LLM\n",
    "  - Note = fonction(quantite_de_travail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d7787-7336-46ec-a985-07dfbb294955",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9bfa5-585a-497e-a580-0a682197168b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### LangChain\n",
    "- Framework Python d'int√©gration de mod√®les LLM\n",
    "  - Model agnostic \n",
    "  - Cha√Ænes et Agents\n",
    "  - Automatisation des prompts\n",
    "  - Gestion des vectorstores, embeddings...\n",
    "\n",
    "- Cas d‚Äôusage : chatbots (RAG), programmes embarquant des LLM, LangGraphs, multi-agents..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3019664-f310-4123-bb6a-e71f6e33864e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Mistral AI\n",
    "\n",
    "1. Cr√©ez un compte sur La Plateforme de Mistral AI. https://mistral.ai/\n",
    "\n",
    "   (\"Try the API\" -> \"S'inscrire\")\n",
    "\n",
    "   (Abonnement : Gratuit / exp√©rimental)\n",
    "   \n",
    "3. G√©n√©rez une cl√© API personnelle.\n",
    "\n",
    "   (API -> Cl√©s API -> Cr√©er une nouvelle cl√© -> ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e098df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # lit le fichier .env\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce133e4-5ed4-4e7c-924e-2951af66c2cf",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Environnement virtuel\n",
    "\n",
    "Il y aura potentiellement des biblioth√®ques √† installer, ce qui peut se faire dans un environnement virtuel python.\n",
    "\n",
    "  $ python -m venv mon\\_env\n",
    "  \n",
    "  $ source mon\\_env/bin/activate\n",
    "  \n",
    "  (mon\\_env) $ pip install nom\\_bibliotheque\n",
    "\n",
    "(Et pour d√©sactiver : deactivate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2addb622-d7a9-4847-bef2-b573f2ee8e27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### R√©f√©rences\n",
    "- LangChain\n",
    "  - https://python.langchain.com/docs/introduction/\n",
    "  - https://www.youtube.com/@LangChain\n",
    "- Mistral AI\n",
    "  - https://docs.mistral.ai/\n",
    "- OpenAI\n",
    "  - https://platform.openai.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3ec0b-529d-4b3a-8fb8-aa1bdca2b1ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Introduction √† LangChain / Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171308f5-cbab-4a41-b1eb-daa5e85a3bc1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Premi√®res invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e601d0-9ba2-44df-b782-a139ef988eca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capitale de l'Albanie est **Tirana**.\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialiser le mod√®le\n",
    "llm = ChatMistralAI(model=\"mistral-medium-2508\", \n",
    "                    temperature=0, api_key=api_key)\n",
    "\n",
    "# Cr√©er le message utilisateur\n",
    "message = HumanMessage(content=\"Quelle est la capitale de l'Albanie ?\")\n",
    "\n",
    "# Obtenir la r√©ponse\n",
    "response = llm.invoke([message])\n",
    "\n",
    "# Afficher la r√©ponse\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84dbc14e-6d82-4efa-a6e1-1ed98ba17048",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capitale de l‚ÄôAlbanie est Tirana.\n"
     ]
    }
   ],
   "source": [
    "# Version OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1\")\n",
    "response = llm.invoke(\"Quelle est la capitale de l'Albanie ?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b4bcfe1-8c3e-40b6-97f7-a42dc07baac6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bien s√ªr ! En voici une :\n",
      "\n",
      "**Pourquoi les pompiers ne jouent-ils jamais √† cache-cache ?**\n",
      "*Parce que les autres n‚Äôaiment pas quand ils crient : \"J‚Äôai trouv√© !\" en arrivant avec la grande √©chelle !* üöíüî•\n",
      "\n",
      "(Et une autre pour la route :)\n",
      "**Que dit un pompier quand il arrive en retard ?**\n",
      "*\"D√©sol√©, j‚Äôai eu un feu rouge !\"* üòÑ\n",
      "\n",
      "Tu veux une autre ? üòÜ\n",
      "----------\n",
      "Bien s√ªr ! En voici une :\n",
      "\n",
      "**Pourquoi les policiers n‚Äôaiment-ils pas les livres ?**\n",
      "*Parce qu‚Äôils pr√©f√®rent les* **couvre-feux** ! üòÑ\n",
      "\n",
      "(Jeu de mots entre *couvre-livres* et *couvre-feux*‚Ä¶ bon, ok, c‚Äôest un peu tir√©e par les cheveux !)\n",
      "\n",
      "---\n",
      "**Autre version (plus courte) :**\n",
      "*Un voleur se fait arr√™ter par la police.*\n",
      "*Le policier lui dit :*\n",
      "‚Äî *\"On vous a vu voler √† l‚Äô√©talage !\"*\n",
      "*‚Äî \"Non, c‚Äôest faux‚Ä¶ Je n‚Äôai rien pris, je l‚Äôai juste* **emprunt√©** *!\"*\n",
      "*‚Äî \"Ah bon ? Alors rendez-le !\"*\n",
      "*‚Äî \"‚Ä¶ Je ne me souviens plus o√π je l‚Äôai* **d√©pos√©** *.\"* üòÜ\n",
      "\n",
      "---\n",
      "Tu veux une autre th√©matique (gendarme, radar, etc.) ? üòâ\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# 1) Initialisation du mod√®le Mistral\n",
    "model = ChatMistralAI(model=\"mistral-medium-2508\", \n",
    "                      temperature=0, api_key=api_key)\n",
    "\n",
    "# 2) Construction du prompt + parser\n",
    "prompt = ChatPromptTemplate.from_template(\"Fais-moi une blague sur le sujet : {sujet}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 3) Cha√Ænage (LangChain Expression Language)\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# 4) Ex√©cution\n",
    "for sujet in ['pompier', 'police']:\n",
    "    print(chain.invoke({\"sujet\": sujet}))\n",
    "    print('-'*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a07cb0e4-94b6-4e04-8959-40435196d1ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Mistral-large-latest** est un mod√®le de langage large (LLM - *Large Language Model*) d√©velopp√© par **Mistral AI**, une startup fran√ßaise sp√©cialis√©e dans l\\'intelligence artificielle. Voici une description technique d√©taill√©e de ce mod√®le, adapt√©e √† une documentation professionnelle :\\n\\n---\\n\\n### **1. Pr√©sentation G√©n√©rale**\\n**Mistral-large-latest** est une version optimis√©e et mise √† jour du mod√®le **Mistral Large**, con√ßu pour offrir des performances de pointe en compr√©hension, g√©n√©ration et raisonnement sur des t√¢ches complexes en langage naturel. Il s‚Äôappuie sur une **architecture de type *Transformer d√©codeur*** (similaire √† GPT-4 ou Llama 2), avec des am√©liorations sp√©cifiques pour :\\n- **La pr√©cision contextuelle** : Meilleure gestion des instructions longues ou ambigu√´s.\\n- **La sp√©cialisation multilingue** : Prise en charge native de plusieurs langues (dont le fran√ßais, l‚Äôanglais, l‚Äôespagnol, l‚Äôallemand, etc.), avec une attention particuli√®re pour les nuances culturelles et linguistiques.\\n- **L‚Äôefficacit√© computationnelle** : Optimisation pour un d√©ploiement en production (latence r√©duite, consommation m√©moire ma√Ætris√©e).\\n\\n---\\n\\n### **2. Caract√©ristiques Techniques Cl√©s**\\n#### **a. Architecture et Taille**\\n- **Type** : Mod√®le *d√©codeur-only* (comme la famille GPT), bas√© sur l‚Äôarchitecture **Transformer**.\\n- **Taille** :\\n  - Le suffixe *\"large\"* indique un mod√®le de grande taille (estim√© entre **70B et 120B de param√®tres**, bien que Mistral AI ne communique pas toujours les d√©tails exacts pour des raisons strat√©giques).\\n  - Version *\"latest\"* : Int√®gre les derni√®res mises √† jour (fine-tuning, corrections de biais, extensions de contexte).\\n- **Contexte** : Fen√™tre contextuelle √©tendue (jusqu‚Äô√† **32k tokens** ou plus, selon la version), permettant de traiter des documents longs ou des conversations complexes sans perte d‚Äôinformation.\\n\\n#### **b. Entra√Ænement**\\n- **Donn√©es** :\\n  - Corpus multilingue diversifi√© (textes publics, code source, litt√©rature technique, conversations, etc.).\\n  - Filtres stricts pour limiter les biais et les contenus toxiques (alignement √©thique via *Reinforcement Learning from Human Feedback* ‚Äì RLHF).\\n- **M√©thodes** :\\n  - **Pr√©tra√Ænement auto-supervis√©** sur des objectifs de pr√©diction de tokens (comme le *causal language modeling*).\\n  - **Post-entra√Ænement** (*fine-tuning*) pour des t√¢ches sp√©cifiques (ex : g√©n√©ration de code, r√©ponse aux questions, r√©sum√©).\\n  - **Optimisation pour l‚Äôinf√©rence** : Techniques comme la *quantisation* (r√©duction de la pr√©cision des poids pour acc√©l√©rer le calcul) ou le *speculative decoding* (g√©n√©ration acc√©l√©r√©e de tokens).\\n\\n#### **c. Performances**\\n- **Benchmarks** :\\n  - Class√© parmi les **meilleurs mod√®les ouverts/commerciaux** sur des tests standardis√©s comme **MT-Bench** (pour les t√¢ches multi-disciplinaires), **HumanEval** (g√©n√©ration de code), ou **MMLU** (raisonnement g√©n√©ral).\\n  - Surpasse souvent des mod√®les comme **Llama 2 70B** ou **Claude 2** sur des t√¢ches en fran√ßais.\\n- **Cas d‚Äôusage optimaux** :\\n  - G√©n√©ration de texte cr√©atif ou technique (documentation, code, emails).\\n  - Assistance conversationnelle (chatbots, support client).\\n  - Analyse et synth√®se de donn√©es non structur√©es (rapports, articles).\\n  - Traduction et localisation de contenu.\\n\\n---\\n\\n### **3. Points Forts par Rapport aux Autres Mod√®les**\\n| **Crit√®re**               | **Mistral-large-latest**                          | **Alternatives (ex : GPT-4, Llama 2, Claude)**       |\\n|---------------------------|---------------------------------------------------|------------------------------------------------------|\\n| **Multilingue**           | Excellente prise en charge du fran√ßais et autres langues europ√©ennes. | GPT-4 performant mais optimis√© pour l‚Äôanglais ; Llama 2 moins pr√©cis en fran√ßais. |\\n| **Ouverture**             | Mod√®le partiellement ouvert (acc√®s via API ou auto-h√©bergement). | GPT-4 ferm√© (API seulement) ; Llama 2 ouvert mais moins performant. |\\n| **Efficacit√©**            | Latence r√©duite et co√ªt optimis√© pour l‚Äôinf√©rence. | GPT-4 co√ªteux ; Llama 2 n√©cessite souvent des optimisations manuelles. |\\n| **Alignement √©thique**    | RLHF robuste pour limiter les biais/hallucinations. | Variable selon les mod√®les (ex : Claude ax√© sur la s√©curit√©). |\\n| **Contexte √©tendu**       | Jusqu‚Äô√† 32k+ tokens (id√©al pour les documents longs). | GPT-4-32k similaire ; Llama 2 limit√© √† 4k tokens. |\\n\\n---\\n\\n### **4. Limites et Consid√©rations**\\n- **Hallucinations** : Comme tous les LLM, peut g√©n√©rer des informations incorrectes ou non v√©rifi√©es (n√©cessite une **validation humaine** pour les usages critiques).\\n- **Donn√©es sensibles** : Non adapt√© au traitement de donn√©es personnelles ou confidentielles sans garanties de chiffrement/anonimisation.\\n- **Co√ªt** : La version *\"large\"* peut √™tre co√ªteuse en ressources (GPU/TPU) pour un d√©ploiement local. Mistral propose des options h√©berg√©es via son [API](https://mistral.ai/).\\n- **Mises √† jour** : Le suffixe *\"latest\"* implique des changements r√©guliers ‚Äì v√©rifier la [documentation officielle](https://docs.mistral.ai/) pour les sp√©cifications exactes de la version d√©ploy√©e.\\n\\n---\\n\\n### **5. Comment l‚ÄôUtiliser ?**\\n#### **a. Via l‚ÄôAPI Mistral**\\n- **Endpoint** : `https://api.mistral.ai/v1/models`\\n- **Exemple de requ√™te (Python)** :\\n  ```python\\n  from mistralai.client import MistralClient\\n  from mistralai.models.chat_completion import ChatMessage\\n\\n  client = MistralClient(api_key=\"votre_cle_api\")\\n  messages = [ChatMessage(role=\"user\", content=\"Expliquez la relativit√© restreinte en 3 phrases.\")]\\n  response = client.chat(model=\"mistral-large-latest\", messages=messages)\\n  print(response.choices[0].message.content)\\n  ```\\n- **Tarification** : Pay-as-you-go (voir [pricing](https://mistral.ai/pricing/)).\\n\\n#### **b. Auto-h√©bergement (pour les versions open-weight)**\\n- **Pr√©requis** : GPU haut de gamme (ex : A100/H100) ou infrastructure cloud (AWS, GCP).\\n- **Outils** :\\n  - Framework **vLLM** pour l‚Äôinf√©rence optimis√©e.\\n  - Conteneurs Docker officiels (si disponibles).\\n- **Exemple avec `transformers`** :\\n  ```python\\n  from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n  tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-large-latest\")\\n  model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-large-latest\", device_map=\"auto\")\\n  inputs = tokenizer(\"Pourquoi le ciel est bleu ?\", return_tensors=\"pt\").to(\"cuda\")\\n  outputs = model.generate(**inputs, max_new_tokens=100)\\n  print(tokenizer.decode(outputs[0], skip_special_tokens=True))\\n  ```\\n\\n#### **c. Int√©grations**\\n- **Plugins** : Compatible avec des outils comme **LangChain**, **LlamaIndex**, ou **FastAPI** pour construire des applications custom.\\n- **Fine-tuning** : Mistral propose des services pour adapter le mod√®le √† des domaines sp√©cifiques (ex : m√©dical, juridique).\\n\\n---\\n\\n### **6. Bonnes Pratiques**\\n- **Prompt Engineering** :\\n  - Utiliser des instructions claires et structur√©es (ex : *\"R√©ponds en 3 points avec des exemples\"*).\\n  - Pour le code : pr√©ciser le langage (`# Python`, `// JavaScript`).\\n- **Gestion des tokens** :\\n  - Limiter les requ√™tes √† la fen√™tre contextuelle (√©viter les d√©passements).\\n  - Compresser les prompts longs avec des techniques de *summarization*.\\n- **√âvaluation** :\\n  - Tester syst√©matiquement les sorties (m√©triques comme **ROUGE** pour le r√©sum√©, **BLEU** pour la traduction).\\n  - Surveiller les biais via des outils comme **Delphi** ou **Fairlearn**.\\n\\n---\\n### **7. Ressources Officielles**\\n- **Documentation** : [https://docs.mistral.ai/](https://docs.mistral.ai/)\\n- **Blog technique** : [https://mistral.ai/news/](https://mistral.ai/news/)\\n- **Communaut√©** : Serveur Discord [Mistral AI](https://discord.gg/mistralai) pour le support.\\n- **Paper** : Pr√©publications sur [arXiv](https://arxiv.org/) (rechercher \"Mistral Large\").\\n\\n---\\n### **8. Alternatives et Comparaison**\\n| **Mod√®le**               | **Avantages**                                  | **Inconv√©nients**                          |\\n|--------------------------|-----------------------------------------------|--------------------------------------------|\\n| **Mistral-large-latest** | Meilleur fran√ßais, ouvert, contexte √©tendu.   | Co√ªt √©lev√© pour l‚Äôauto-h√©bergement.       |\\n| **GPT-4 (OpenAI)**       | Performances g√©n√©rales sup√©rieures.          | Ferm√©, cher, biais vers l‚Äôanglais.         |\\n| **Llama 2 70B**          | Open source, gratuit.                         | Moins pr√©cis en fran√ßais, contexte limit√©.|\\n| **Claude 2 (Anthropic)** | S√©curit√© et alignement √©thique renforc√©s.    | Acc√®s restreint, moins multilingue.       |\\n\\n---\\n### **9. Cas d‚ÄôUsage Concrets**\\n1. **G√©n√©ration de Documentation Technique** :\\n   - Automatiser la r√©daction de manuels ou de FAQ √† partir de sp√©cifications.\\n   - Exemple de prompt :\\n     *\"R√©dige une section de documentation pour expliquer l‚ÄôAPI REST de notre produit. Utilise des exemples en Python et JavaScript. Structure : 1. Authentification 2. Endpoints 3. Gestion des erreurs.\"*\\n\\n2. **Support Client Multilingue** :\\n   - D√©ployer un chatbot capable de r√©pondre en fran√ßais, anglais et espagnol avec une tonalit√© adapt√©e.\\n\\n3. **Analyse de Donn√©es Non Structur√©es** :\\n   - Extraire des insights √† partir de retours clients ou de rapports PDF.\\n\\n4. **D√©veloppement Assist√©** :\\n   - G√©n√©ration de tests unitaires, d√©bogage, ou explication de code.\\n   - Exemple :\\n     *\"Ce code Python a une fuite m√©moire. Identifie le probl√®me et propose une correction : [coller le code].\"*\\n\\n---\\n### **10. Roadmap et √âvolutions**\\nMistral AI travaille activement sur :\\n- **Mistral Next** : Mod√®les encore plus grands (potentiellement >200B param√®tres).\\n- **Am√©lioration du RAG** (*Retrieval-Augmented Generation*) pour r√©duire les hallucinations.\\n- **Outils no-code** pour d√©mocratiser l‚Äôacc√®s aux LLM.\\n\\n---\\n### **Conclusion**\\n**Mistral-large-latest** est un choix id√©al pour les organisations recherchant un **LLM hautement performant en multilingue**, avec un focus particulier sur l‚Äôefficacit√© et l‚Äôouverture relative. Son √©quilibre entre puissance, co√ªt et flexibilit√© en fait une alternative s√©rieuse √† GPT-4 pour de nombreux cas d‚Äôusage, surtout en Europe.\\n\\nPour un d√©ploiement r√©ussi, il est recommand√© de :\\n1. **Tester en conditions r√©elles** avec des jeux de donn√©es repr√©sentatifs.\\n2. **Surveiller les performances** via des m√©triques adapt√©es.\\n3. **Combiner avec des outils externes** (bases de connaissances, v√©rification humaine) pour les t√¢ches critiques.\\n\\n---\\n**Besoin de pr√©cisions sur un aspect technique sp√©cifique (ex : optimisation de l‚Äôinf√©rence, fine-tuning, benchmarking) ?** Je peux approfondir !' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 28, 'total_tokens': 2696, 'completion_tokens': 2668}, 'model_name': 'mistral-medium-2508', 'model': 'mistral-medium-2508', 'finish_reason': 'stop'} id='run--529ab154-6d0e-4f04-9d42-210604ca132f-0' usage_metadata={'input_tokens': 28, 'output_tokens': 2668, 'total_tokens': 2696}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Vous √™tes un r√©dacteur de documentation technique de classe mondiale.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-medium-2508\", api_key=api_key)\n",
    "chain = prompt | llm \n",
    "result = chain.invoke({\"input\": \"Qu'est-ce que le mod√®le mistral-large-latest ?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dfaa652-299d-4d7a-a480-4e17a079cf19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 28, 'output_tokens': 2668, 'total_tokens': 2696}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "120dac24-febc-4714-ad88-8033d5bd9d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056156"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "38/1000000*2+701*8/100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb7b1825-15de-4b92-8d7e-7ff0e670e212",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Mistral-large-latest** est un mod√®le de langage large (LLM - *Large Language Model*) d√©velopp√© par **Mistral AI**, une startup fran√ßaise sp√©cialis√©e dans l'intelligence artificielle. Voici une description technique d√©taill√©e de ce mod√®le, adapt√©e √† une documentation professionnelle :\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Pr√©sentation G√©n√©rale**\n",
      "**Mistral-large-latest** est une version optimis√©e et mise √† jour du mod√®le **Mistral Large**, con√ßu pour offrir des performances de pointe en compr√©hension, g√©n√©ration et raisonnement sur des t√¢ches complexes en langage naturel. Il s‚Äôappuie sur une **architecture de type *Transformer d√©codeur*** (similaire √† GPT-4 ou Llama 2), avec des am√©liorations sp√©cifiques pour :\n",
      "- **La pr√©cision contextuelle** : Meilleure gestion des instructions longues ou ambigu√´s.\n",
      "- **La sp√©cialisation multilingue** : Prise en charge native de plusieurs langues (dont le fran√ßais, l‚Äôanglais, l‚Äôespagnol, l‚Äôallemand, etc.), avec une attention particuli√®re pour les nuances culturelles et linguistiques.\n",
      "- **L‚Äôefficacit√© computationnelle** : Optimisation pour un d√©ploiement en production (latence r√©duite, consommation m√©moire ma√Ætris√©e).\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Caract√©ristiques Techniques Cl√©s**\n",
      "#### **a. Architecture et Taille**\n",
      "- **Type** : Mod√®le *d√©codeur-only* (comme la famille GPT), bas√© sur l‚Äôarchitecture **Transformer**.\n",
      "- **Taille** :\n",
      "  - Le suffixe *\"large\"* indique un mod√®le de grande taille (estim√© entre **70B et 120B de param√®tres**, bien que Mistral AI ne communique pas toujours les d√©tails exacts pour des raisons strat√©giques).\n",
      "  - Version *\"latest\"* : Int√®gre les derni√®res mises √† jour (fine-tuning, corrections de biais, extensions de contexte).\n",
      "- **Contexte** : Fen√™tre contextuelle √©tendue (jusqu‚Äô√† **32k tokens** ou plus, selon la version), permettant de traiter des documents longs ou des conversations complexes sans perte d‚Äôinformation.\n",
      "\n",
      "#### **b. Entra√Ænement**\n",
      "- **Donn√©es** :\n",
      "  - Corpus multilingue diversifi√© (textes publics, code source, litt√©rature technique, conversations, etc.).\n",
      "  - Filtres stricts pour limiter les biais et les contenus toxiques (alignement √©thique via *Reinforcement Learning from Human Feedback* ‚Äì RLHF).\n",
      "- **M√©thodes** :\n",
      "  - **Pr√©tra√Ænement auto-supervis√©** sur des objectifs de pr√©diction de tokens (comme le *causal language modeling*).\n",
      "  - **Post-entra√Ænement** (*fine-tuning*) pour des t√¢ches sp√©cifiques (ex : g√©n√©ration de code, r√©ponse aux questions, r√©sum√©).\n",
      "  - **Optimisation pour l‚Äôinf√©rence** : Techniques comme la *quantisation* (r√©duction de la pr√©cision des poids pour acc√©l√©rer le calcul) ou le *speculative decoding* (g√©n√©ration acc√©l√©r√©e de tokens).\n",
      "\n",
      "#### **c. Performances**\n",
      "- **Benchmarks** :\n",
      "  - Class√© parmi les **meilleurs mod√®les ouverts/commerciaux** sur des tests standardis√©s comme **MT-Bench** (pour les t√¢ches multi-disciplinaires), **HumanEval** (g√©n√©ration de code), ou **MMLU** (raisonnement g√©n√©ral).\n",
      "  - Surpasse souvent des mod√®les comme **Llama 2 70B** ou **Claude 2** sur des t√¢ches en fran√ßais.\n",
      "- **Cas d‚Äôusage optimaux** :\n",
      "  - G√©n√©ration de texte cr√©atif ou technique (documentation, code, emails).\n",
      "  - Assistance conversationnelle (chatbots, support client).\n",
      "  - Analyse et synth√®se de donn√©es non structur√©es (rapports, articles).\n",
      "  - Traduction et localisation de contenu.\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Points Forts par Rapport aux Autres Mod√®les**\n",
      "| **Crit√®re**               | **Mistral-large-latest**                          | **Alternatives (ex : GPT-4, Llama 2, Claude)**       |\n",
      "|---------------------------|---------------------------------------------------|------------------------------------------------------|\n",
      "| **Multilingue**           | Excellente prise en charge du fran√ßais et autres langues europ√©ennes. | GPT-4 performant mais optimis√© pour l‚Äôanglais ; Llama 2 moins pr√©cis en fran√ßais. |\n",
      "| **Ouverture**             | Mod√®le partiellement ouvert (acc√®s via API ou auto-h√©bergement). | GPT-4 ferm√© (API seulement) ; Llama 2 ouvert mais moins performant. |\n",
      "| **Efficacit√©**            | Latence r√©duite et co√ªt optimis√© pour l‚Äôinf√©rence. | GPT-4 co√ªteux ; Llama 2 n√©cessite souvent des optimisations manuelles. |\n",
      "| **Alignement √©thique**    | RLHF robuste pour limiter les biais/hallucinations. | Variable selon les mod√®les (ex : Claude ax√© sur la s√©curit√©). |\n",
      "| **Contexte √©tendu**       | Jusqu‚Äô√† 32k+ tokens (id√©al pour les documents longs). | GPT-4-32k similaire ; Llama 2 limit√© √† 4k tokens. |\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Limites et Consid√©rations**\n",
      "- **Hallucinations** : Comme tous les LLM, peut g√©n√©rer des informations incorrectes ou non v√©rifi√©es (n√©cessite une **validation humaine** pour les usages critiques).\n",
      "- **Donn√©es sensibles** : Non adapt√© au traitement de donn√©es personnelles ou confidentielles sans garanties de chiffrement/anonimisation.\n",
      "- **Co√ªt** : La version *\"large\"* peut √™tre co√ªteuse en ressources (GPU/TPU) pour un d√©ploiement local. Mistral propose des options h√©berg√©es via son [API](https://mistral.ai/).\n",
      "- **Mises √† jour** : Le suffixe *\"latest\"* implique des changements r√©guliers ‚Äì v√©rifier la [documentation officielle](https://docs.mistral.ai/) pour les sp√©cifications exactes de la version d√©ploy√©e.\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Comment l‚ÄôUtiliser ?**\n",
      "#### **a. Via l‚ÄôAPI Mistral**\n",
      "- **Endpoint** : `https://api.mistral.ai/v1/models`\n",
      "- **Exemple de requ√™te (Python)** :\n",
      "  ```python\n",
      "  from mistralai.client import MistralClient\n",
      "  from mistralai.models.chat_completion import ChatMessage\n",
      "\n",
      "  client = MistralClient(api_key=\"votre_cle_api\")\n",
      "  messages = [ChatMessage(role=\"user\", content=\"Expliquez la relativit√© restreinte en 3 phrases.\")]\n",
      "  response = client.chat(model=\"mistral-large-latest\", messages=messages)\n",
      "  print(response.choices[0].message.content)\n",
      "  ```\n",
      "- **Tarification** : Pay-as-you-go (voir [pricing](https://mistral.ai/pricing/)).\n",
      "\n",
      "#### **b. Auto-h√©bergement (pour les versions open-weight)**\n",
      "- **Pr√©requis** : GPU haut de gamme (ex : A100/H100) ou infrastructure cloud (AWS, GCP).\n",
      "- **Outils** :\n",
      "  - Framework **vLLM** pour l‚Äôinf√©rence optimis√©e.\n",
      "  - Conteneurs Docker officiels (si disponibles).\n",
      "- **Exemple avec `transformers`** :\n",
      "  ```python\n",
      "  from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "\n",
      "  tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-large-latest\")\n",
      "  model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-large-latest\", device_map=\"auto\")\n",
      "  inputs = tokenizer(\"Pourquoi le ciel est bleu ?\", return_tensors=\"pt\").to(\"cuda\")\n",
      "  outputs = model.generate(**inputs, max_new_tokens=100)\n",
      "  print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
      "  ```\n",
      "\n",
      "#### **c. Int√©grations**\n",
      "- **Plugins** : Compatible avec des outils comme **LangChain**, **LlamaIndex**, ou **FastAPI** pour construire des applications custom.\n",
      "- **Fine-tuning** : Mistral propose des services pour adapter le mod√®le √† des domaines sp√©cifiques (ex : m√©dical, juridique).\n",
      "\n",
      "---\n",
      "\n",
      "### **6. Bonnes Pratiques**\n",
      "- **Prompt Engineering** :\n",
      "  - Utiliser des instructions claires et structur√©es (ex : *\"R√©ponds en 3 points avec des exemples\"*).\n",
      "  - Pour le code : pr√©ciser le langage (`# Python`, `// JavaScript`).\n",
      "- **Gestion des tokens** :\n",
      "  - Limiter les requ√™tes √† la fen√™tre contextuelle (√©viter les d√©passements).\n",
      "  - Compresser les prompts longs avec des techniques de *summarization*.\n",
      "- **√âvaluation** :\n",
      "  - Tester syst√©matiquement les sorties (m√©triques comme **ROUGE** pour le r√©sum√©, **BLEU** pour la traduction).\n",
      "  - Surveiller les biais via des outils comme **Delphi** ou **Fairlearn**.\n",
      "\n",
      "---\n",
      "### **7. Ressources Officielles**\n",
      "- **Documentation** : [https://docs.mistral.ai/](https://docs.mistral.ai/)\n",
      "- **Blog technique** : [https://mistral.ai/news/](https://mistral.ai/news/)\n",
      "- **Communaut√©** : Serveur Discord [Mistral AI](https://discord.gg/mistralai) pour le support.\n",
      "- **Paper** : Pr√©publications sur [arXiv](https://arxiv.org/) (rechercher \"Mistral Large\").\n",
      "\n",
      "---\n",
      "### **8. Alternatives et Comparaison**\n",
      "| **Mod√®le**               | **Avantages**                                  | **Inconv√©nients**                          |\n",
      "|--------------------------|-----------------------------------------------|--------------------------------------------|\n",
      "| **Mistral-large-latest** | Meilleur fran√ßais, ouvert, contexte √©tendu.   | Co√ªt √©lev√© pour l‚Äôauto-h√©bergement.       |\n",
      "| **GPT-4 (OpenAI)**       | Performances g√©n√©rales sup√©rieures.          | Ferm√©, cher, biais vers l‚Äôanglais.         |\n",
      "| **Llama 2 70B**          | Open source, gratuit.                         | Moins pr√©cis en fran√ßais, contexte limit√©.|\n",
      "| **Claude 2 (Anthropic)** | S√©curit√© et alignement √©thique renforc√©s.    | Acc√®s restreint, moins multilingue.       |\n",
      "\n",
      "---\n",
      "### **9. Cas d‚ÄôUsage Concrets**\n",
      "1. **G√©n√©ration de Documentation Technique** :\n",
      "   - Automatiser la r√©daction de manuels ou de FAQ √† partir de sp√©cifications.\n",
      "   - Exemple de prompt :\n",
      "     *\"R√©dige une section de documentation pour expliquer l‚ÄôAPI REST de notre produit. Utilise des exemples en Python et JavaScript. Structure : 1. Authentification 2. Endpoints 3. Gestion des erreurs.\"*\n",
      "\n",
      "2. **Support Client Multilingue** :\n",
      "   - D√©ployer un chatbot capable de r√©pondre en fran√ßais, anglais et espagnol avec une tonalit√© adapt√©e.\n",
      "\n",
      "3. **Analyse de Donn√©es Non Structur√©es** :\n",
      "   - Extraire des insights √† partir de retours clients ou de rapports PDF.\n",
      "\n",
      "4. **D√©veloppement Assist√©** :\n",
      "   - G√©n√©ration de tests unitaires, d√©bogage, ou explication de code.\n",
      "   - Exemple :\n",
      "     *\"Ce code Python a une fuite m√©moire. Identifie le probl√®me et propose une correction : [coller le code].\"*\n",
      "\n",
      "---\n",
      "### **10. Roadmap et √âvolutions**\n",
      "Mistral AI travaille activement sur :\n",
      "- **Mistral Next** : Mod√®les encore plus grands (potentiellement >200B param√®tres).\n",
      "- **Am√©lioration du RAG** (*Retrieval-Augmented Generation*) pour r√©duire les hallucinations.\n",
      "- **Outils no-code** pour d√©mocratiser l‚Äôacc√®s aux LLM.\n",
      "\n",
      "---\n",
      "### **Conclusion**\n",
      "**Mistral-large-latest** est un choix id√©al pour les organisations recherchant un **LLM hautement performant en multilingue**, avec un focus particulier sur l‚Äôefficacit√© et l‚Äôouverture relative. Son √©quilibre entre puissance, co√ªt et flexibilit√© en fait une alternative s√©rieuse √† GPT-4 pour de nombreux cas d‚Äôusage, surtout en Europe.\n",
      "\n",
      "Pour un d√©ploiement r√©ussi, il est recommand√© de :\n",
      "1. **Tester en conditions r√©elles** avec des jeux de donn√©es repr√©sentatifs.\n",
      "2. **Surveiller les performances** via des m√©triques adapt√©es.\n",
      "3. **Combiner avec des outils externes** (bases de connaissances, v√©rification humaine) pour les t√¢ches critiques.\n",
      "\n",
      "---\n",
      "**Besoin de pr√©cisions sur un aspect technique sp√©cifique (ex : optimisation de l‚Äôinf√©rence, fine-tuning, benchmarking) ?** Je peux approfondir !\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0079cb0a-8f86-462f-9ed4-92a886d3bcbd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Les sorties structur√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ffa5cd-8e50-4ea5-83a0-32acc0114322",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Cas d'un bool√©en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd16a7ce-de15-4342-b612-0bec866e82af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No√´l est en hiver\n",
      "R√©ponse : True (type : <class 'bool'>)\n",
      "\n",
      "Il pleut quand il pleut pas\n",
      "R√©ponse : False (type : <class 'bool'>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    answer: bool\n",
    "\n",
    "prompt_answer = [\n",
    "    (\"system\", \"Tu es un assistant charg√© de r√©pondre un bool√©en (True ou False) √† la question d'un utilisateur.\"),\n",
    "    ('human', \"{question}\")\n",
    "]\n",
    "\n",
    "prompt_answer_template = ChatPromptTemplate.from_messages(prompt_answer)\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "chain = prompt_answer_template | llm.with_structured_output(schema=Answer)\n",
    "\n",
    "def repond(question):\n",
    "    return chain.invoke({\"question\": question}).answer\n",
    "    \n",
    "for question in [\"No√´l est en hiver\", \"Il pleut quand il pleut pas\"]:\n",
    "    print(question)\n",
    "    reponse = repond(question)\n",
    "    print(f\"R√©ponse : {reponse} (type : {type(reponse)})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57646f4-a101-441d-a491-cde12e06a8f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Cas d'une classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea1dad8b-78fd-4656-ac3e-e4865ef70c4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peux-tu m'en dire plus\n",
      "action=\"Fournir plus d'√©l√©ments √† la question pr√©c√©dente\"\n",
      "\n",
      "Que sont les PPV ?\n",
      "action='R√©pondre √† une nouvelle question'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "tasks = [\"R√©pondre √† une nouvelle question\", \"Fournir plus d'√©l√©ments √† la question pr√©c√©dente\"]\n",
    "\n",
    "class NextTask(BaseModel):\n",
    "    \"\"\"Utilise toujours cet outil pour structurer ta r√©ponse to the user.\"\"\"\n",
    "    action: str = Field(..., \n",
    "                        enum=tasks,\n",
    "                        description=\"La prochaine action √† mener\")\n",
    "\n",
    "prompt_message = [\n",
    "    (\"system\", \"Tu es un assistant charg√© de classifier la demande d'un utilisateur parmi une \"\n",
    "               \"liste r√©duite d'actions √† mener en tant que chatbot. Tu dois d√©terminer la \"\n",
    "               \"prochaine action √† mener.\"),\n",
    "    ('human', \"{text}\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(prompt_message)\n",
    "llm = ChatMistralAI(model='mistral-large-latest', temperature=0)\n",
    "chain = prompt | llm.with_structured_output(schema=NextTask)\n",
    "\n",
    "for text in [\"Peux-tu m'en dire plus\", \"Que sont les PPV ?\"]:\n",
    "    print(text)\n",
    "    print(chain.invoke({\"text\": text}))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3bd17-0379-467b-9be9-06544afcaf0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Cas d'un entier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d8b502d-8200-4ca8-b0c0-3969c86dab19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message : Bonjour, pourrais-tu m'aider s'il te pla√Æt ?\n",
      "note_ton=5\n",
      "\n",
      "Message : J'ai besoin de √ßa imm√©diatement.\n",
      "note_ton=1\n",
      "\n",
      "Message : Merci beaucoup pour ton aide pr√©cieuse !\n",
      "note_ton=5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "class TonMessage(BaseModel):\n",
    "    \"\"\"√âvaluation du ton du message de l'utilisateur.\"\"\"\n",
    "    note_ton: int = Field(\n",
    "        ..., \n",
    "        ge=1,\n",
    "        le=5,\n",
    "        description=\"Note attribu√©e au ton du message : 1 pour neutre, 5 pour tr√®s aimable\"\n",
    "    )\n",
    "\n",
    "prompt_message = [\n",
    "    (\"system\", \"Tu es un assistant charg√© d'√©valuer le ton d'un message donn√© par l'utilisateur. \"\n",
    "               \"Attribue une note de 1 √† 5 au ton du message, o√π 1 signifie neutre et 5 signifie tr√®s aimable.\"),\n",
    "    ('human', \"{text}\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(prompt_message)\n",
    "llm = ChatMistralAI(model='mistral-large-latest', temperature=0)\n",
    "chain = prompt | llm.with_structured_output(schema=TonMessage)\n",
    "\n",
    "messages = [\n",
    "    \"Bonjour, pourrais-tu m'aider s'il te pla√Æt ?\",\n",
    "    \"J'ai besoin de √ßa imm√©diatement.\",\n",
    "    \"Merci beaucoup pour ton aide pr√©cieuse !\"\n",
    "]\n",
    "\n",
    "for text in messages:\n",
    "    print(f\"Message : {text}\")\n",
    "    print(chain.invoke({\"text\": text}))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d36f5d-0be7-47cf-8b7d-88e0e28bae71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Pourquoi et comment forcer la sortie du LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76a94afe-6764-46bb-9aaf-7b4583eb1f5c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The translation of your sentence into English is:\\n\\n**\"What is the capital of Albania?\"**\\n\\n*(The answer is **Tirana**, by the way!)* üòä', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 26, 'total_tokens': 61, 'completion_tokens': 35}, 'model_name': 'mistral-medium-latest', 'model': 'mistral-medium-latest', 'finish_reason': 'stop'}, id='run--8b436054-40b4-4325-92e7-afd444e6bf04-0', usage_metadata={'input_tokens': 26, 'output_tokens': 35, 'total_tokens': 61})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-medium-latest\")\n",
    "message = HumanMessage(content=\"Peux-tu me traduire ce qui suit, en anglais ?\\n\\n Quelle est la capitale de l'Albanie ?\")\n",
    "llm.invoke([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e8c85e9-e6fa-4812-8718-50c1ee92a5cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Albania?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Translation(BaseModel):\n",
    "    original_text: str = Field(..., description=\"The original text before translation in another language\")\n",
    "    original_language: str = Field(..., description=\"The original language before translation\")\n",
    "    translated_text: str = Field(..., description=\"The final text after translation in another language\")\n",
    "    translated_language: str = Field(..., description=\"The language into which the translation must be done\")\n",
    "    \n",
    "def traduit(texte, langue_source=\"fran√ßais\", langue_cible=\"anglais\"):\n",
    "    llm = ChatMistralAI(model_name=\"mistral-medium-latest\")\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"Je souhaite que tu traduises le texte suivant du {langue_source} vers le {langue_cible}. Ta traduction doit √™tre pr√©cise, fluide et naturelle, et pr√©server parfaitement le sens original. \n",
    "    Retourne-moi la r√©ponse sous forme d'objet JSON avec les champs :\n",
    "      - original_text : le texte original\n",
    "      - original_language : la langue du texte original\n",
    "      - translated_text : la traduction du texte\n",
    "      - translated_language : la langue de la traduction\n",
    "\n",
    "    Voici le texte √† traduire :\n",
    "    ----\n",
    "    {texte}\"\"\")\n",
    "    output_parser = StrOutputParser()\n",
    "    extract_translation = RunnableLambda(lambda translation: translation.translated_text)\n",
    "    chain0 = prompt | llm.with_structured_output(Translation) | extract_translation\n",
    "    return chain0.invoke({\"langue_source\": langue_source,\n",
    "                            \"langue_cible\": \"anglais\",\n",
    "                            \"texte\": texte      \n",
    "                            })\n",
    "\n",
    "print(traduit(\"Quelle est la capitale de l'Albanie\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2032b-c358-4aec-b673-43b4e1afed83",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Des sorties structur√©es aux pr√©mices d'un raisonnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "462e27ab-59d3-4bc3-aab0-37c73e620d48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "class Etape(BaseModel):\n",
    "    explication: str\n",
    "    sortie: str\n",
    "\n",
    "class MathReponse(BaseModel):\n",
    "    etapes: list[Etape]\n",
    "    reponse_finale: str\n",
    "\n",
    "prompt_answer = [\n",
    "    (\"system\", \"Tu es un professeur de math√©matiques tr√®s p√©dagogue.\"),\n",
    "    ('human', \"{exercice}\")\n",
    "]\n",
    "\n",
    "prompt_answer_template = ChatPromptTemplate.from_messages(prompt_answer)\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "chain = prompt_answer_template | llm.with_structured_output(schema=MathReponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83f49b83-7fe3-475a-818b-43c232ddaea2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- L'objectif est d'isoler x. Pour cela, commen√ßons par soustraire 31 des deux c√¥t√©s de l'√©quation pour √©liminer le terme constant du c√¥t√© gauche.\n",
      "  Le r√©sultat est alors : 8x = 2 - 31\n",
      "- Effectuons la soustraction √† droite pour simplifier l'√©quation.\n",
      "  Le r√©sultat est alors : 8x = -29\n",
      "- Maintenant, divisons les deux c√¥t√©s de l'√©quation par 8 pour isoler x.\n",
      "  Le r√©sultat est alors : x = -29 / 8\n",
      "- Effectuons la division pour obtenir la valeur de x.\n",
      "  Le r√©sultat est alors : x = -3.625\n",
      "Au final, on trouve : x = -3.625\n"
     ]
    }
   ],
   "source": [
    "explications = chain.invoke({\"exercice\": \"R√©sous  8x + 31 = 2\"})\n",
    "for etape in explications.etapes:\n",
    "    print(f\"- {etape.explication}\")\n",
    "    print(f\"  Le r√©sultat est alors : {etape.sortie}\")\n",
    "\n",
    "print(f\"Au final, on trouve : {explications.reponse_finale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7905089f-3155-4381-91e5-17a2ba2aac9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918186c0-294c-4a8f-b0f8-65afd6f06a42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/rag.png\" alt=\"RAG\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589d138-b105-4567-aed0-ab5fb462ca51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Embeddings et semantique\n",
    "\n",
    "But : encoder un texte sous la forme d'un vecteur, de sorte que deux textes voisins s√©mantiquement soient encod√©s en deux vecteurs proches.\n",
    "\n",
    "![Texte alternatif](images/vectors-and-semantics.png \"Vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee12db-412b-43bb-b0bd-9c680335617a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Embeddings : Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6a1c67-5364-4d43-bdb9-cf1f6bf7dc6d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "![Texte alternatif](images/Bag-of-words.png \"BoW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "188424dd-b206-46a9-bcfb-3c5c2277cebb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary : ['and' 'demo' 'demonstration' 'document' 'finally' 'first' 'here' 'is'\n",
      " 'second' 'text' 'the' 'third' 'this']\n",
      "BoW vector:\n",
      " [[0 0 1 1 0 1 0 0 0 1 0 0 0]\n",
      " [1 1 0 1 0 0 1 0 1 1 0 0 0]\n",
      " [1 0 0 1 1 0 0 1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'Demonstration text, first document',\n",
    "    \"Demo text, and here's a second document.\",\n",
    "    'And finally, this is the third document.'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary :\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW vector:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11281bfe-e00f-49e0-8ae1-108605a139cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Embeddings par transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34d487f9-12a6-47ff-8609-0fedc2cb7064",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"This is an example sentence.\" -> [0.09812459 0.06781267 0.06252321]...\n",
      "\"Each sentence is converted into a fixed-sized vector.\" -> [0.06216577 0.05840717 0.00820479]...\n",
      "Embedding size: 384\n"
     ]
    }
   ],
   "source": [
    "# pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = [\"This is an example sentence.\", \"Each sentence is converted into a fixed-sized vector.\"]\n",
    "\n",
    "# Entra√Æn√© sur des donn√©es essentiellement anglophones.\n",
    "# Con√ßu pour √™tre l√©ger et rapide, tout en gardant une bonne pr√©cision pour l‚Äôanglais.\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(f'\"{sentence}\" -> {embedding[:3]}...')\n",
    "\n",
    "print(f\"Embedding size: {len(embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfb7a8bf-fc00-4b03-a5bf-32890b7f45ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.65207729e-02  7.26545528e-02 -3.65566462e-01 -7.02737719e-02\n",
      "  -2.06112370e-01  5.19921295e-02 -1.28497016e-02 -1.39605641e-01\n",
      "  -4.71076183e-02  1.24628298e-01 -7.31549039e-02 -2.52605230e-01\n",
      "   2.85230219e-01 -1.12165160e-01  9.12081599e-02 -1.15145050e-01\n",
      "  -5.70061989e-02  3.20486993e-01 -1.61287278e-01 -4.73647386e-01\n",
      "   3.03176671e-01  4.64675017e-02  3.84551734e-01 -5.37366159e-02\n",
      "  -9.78466347e-02  3.22349221e-01 -3.62097204e-01 -9.99814272e-02\n",
      "  -1.07472390e-02 -1.74769863e-01 -7.64722973e-02 -3.83381434e-02\n",
      "   4.25416440e-01  1.90166920e-01  1.54887754e-02  3.84460360e-01\n",
      "   2.09153861e-01  1.30659854e-02  6.54842705e-02  9.23802704e-02\n",
      "  -2.48034760e-01  2.97549397e-01 -1.82683066e-01  2.15694353e-01\n",
      "   2.30816737e-01  1.02084570e-01 -1.54894486e-01  1.22451119e-01\n",
      "  -1.96864322e-01  5.10899350e-02  8.70069116e-02  1.73673987e-01\n",
      "  -2.90965825e-01  1.98579445e-01  7.86646977e-02 -3.96869481e-01\n",
      "  -8.99151042e-02  1.15304410e-01 -3.47552123e-03  3.48371454e-02\n",
      "  -1.60975322e-01 -1.58165947e-01 -2.76869655e-01  4.26388979e-01\n",
      "   5.98660186e-02  6.18471839e-02  2.72837222e-01  1.96666270e-01\n",
      "  -1.17939189e-01 -1.33643046e-01 -2.67337888e-01 -3.36821347e-01\n",
      "   2.29286119e-01  3.97668362e-01  1.13105297e-01  1.00522399e-01\n",
      "  -2.63185263e-01 -9.64541808e-02 -3.05367380e-01 -9.76834968e-02\n",
      "  -1.13328159e-01  1.54144075e-02  2.95926243e-01  2.19961330e-01\n",
      "   2.46220931e-01  3.16740647e-02  7.73194358e-02  7.18099400e-02\n",
      "   1.40424266e-01  5.11829555e-02 -1.68341532e-01 -5.95128715e-01\n",
      "   2.40329251e-01  3.17607015e-01 -3.88529561e-02 -2.94892848e-01\n",
      "   1.80528611e-01  3.61951888e-02  2.57690400e-01  8.23560715e-01\n",
      "   1.40070647e-01 -8.03383812e-02 -6.18174933e-02 -9.01752189e-02\n",
      "   1.14790730e-01 -2.90556252e-01  3.30534488e-01 -2.12557182e-01\n",
      "   1.58265188e-01 -4.12546784e-01 -8.60647485e-02  1.22264242e-02\n",
      "   7.80155882e-02  4.66110706e-02  1.20251276e-01  1.15379043e-01\n",
      "   3.58826369e-01 -2.95536071e-01  2.40629718e-01  3.28016758e-01\n",
      "  -3.74648608e-02  2.16838513e-02 -3.26381236e-01  2.22172633e-01\n",
      "   5.41193224e-02 -3.46985668e-01  6.41295761e-02  6.68766871e-02\n",
      "   1.61394030e-01  1.44798443e-01  6.42243102e-02  1.70563251e-01\n",
      "   7.97803551e-02  1.90901861e-01 -2.15357751e-01 -2.94394821e-01\n",
      "  -3.04533690e-01  1.91265434e-01  9.96950492e-02 -4.44939196e-01\n",
      "   1.66512564e-01  3.60089242e-01  1.23127781e-01  3.49543057e-02\n",
      "   1.43274534e-02  2.08197068e-02 -6.12885058e-02  1.09736305e-02\n",
      "   8.59057307e-02  1.98545158e-02  1.86347678e-01 -1.14208736e-01\n",
      "  -2.96818405e-01  7.24941269e-02  1.58054382e-01 -8.90302882e-02\n",
      "   2.68423796e-01  4.57218811e-02 -4.18559730e-01 -1.94560215e-01\n",
      "   4.83291186e-02  3.04925977e-03 -1.85847446e-01  9.70826373e-02\n",
      "   1.95084780e-01 -1.30889239e-02 -5.37880622e-02  1.69087157e-01\n",
      "  -5.60183404e-03 -1.03137560e-01  3.72133911e-01 -1.43242285e-01\n",
      "   2.56733239e-01 -2.45170686e-02 -5.95078431e-02 -4.52222303e-02\n",
      "   4.02538963e-02 -4.63011041e-02  3.71608809e-02 -2.88201980e-02\n",
      "   1.08527444e-01 -1.45067647e-01  1.49910212e-01  1.44873178e-02\n",
      "  -2.73075491e-01  3.12966138e-01 -1.00796930e-01  5.74326254e-02\n",
      "  -3.42364833e-02  1.39859542e-01  9.60069895e-02 -2.33742800e-02\n",
      "   1.31520867e-01 -1.51918992e-01 -4.46628571e-01 -4.41853374e-01\n",
      "  -1.53541639e-01 -3.33519489e-01 -5.36727488e-01 -1.15265854e-01\n",
      "   1.35443464e-01  1.03348441e-01 -1.19261213e-01 -3.31817307e-02\n",
      "  -9.43362638e-02 -1.10796221e-01 -1.18562937e-01 -1.50282728e-02\n",
      "   5.34428358e-02  2.86488146e-01 -3.00379563e-02 -4.31574415e-03\n",
      "   1.42691983e-02 -2.72330910e-01  1.84161827e-01 -3.21028322e-01\n",
      "  -1.57563090e-02  1.43873215e-01  6.91008940e-02  2.25662012e-02\n",
      "   7.33844563e-02 -5.94415605e-01  9.29291621e-02 -3.82539004e-01\n",
      "   2.44259927e-02  3.61744791e-01 -3.22392255e-01  5.74960038e-02\n",
      "  -5.39429784e-01  1.41370758e-01  7.32738450e-02  2.56943144e-02\n",
      "   1.37124166e-01 -1.92796811e-01  4.03660610e-02  4.57467958e-02\n",
      "  -7.21263215e-02  1.57910243e-01 -1.81753919e-01  5.73866162e-03\n",
      "   2.18394443e-01  5.03768981e-01  8.34504887e-02 -2.20524713e-01\n",
      "  -2.61221118e-02 -3.34462851e-01  2.70725917e-02  1.15487374e-01\n",
      "  -9.04312655e-02  2.83468306e-01 -1.59079239e-01  4.31691229e-01\n",
      "   1.66640639e-01 -5.51334061e-02  1.90114513e-01  1.41633272e-01\n",
      "  -3.16261560e-01 -1.00674748e-01 -1.34072348e-01 -3.77071232e-01\n",
      "   3.46790850e-01  2.88731426e-01 -1.91390917e-01  2.76690751e-01\n",
      "   2.85495162e-01  9.66794975e-03 -7.72283003e-02  4.94440570e-02\n",
      "  -1.83544159e-01  5.78330085e-02 -6.51681602e-01  2.76058167e-01\n",
      "  -7.00452104e-02 -2.49790195e-02  2.23135099e-01 -1.21700346e-01\n",
      "   6.76466152e-02 -4.78527397e-01 -8.49619880e-02 -8.91137961e-03\n",
      "  -2.15847746e-01 -1.30860299e-01  1.55468106e-01 -2.13819429e-01\n",
      "   1.29306465e-02 -4.41335961e-02  1.71468705e-01 -6.07402213e-02\n",
      "   2.78392822e-01  6.82281330e-02  5.11239693e-02 -5.65553308e-02\n",
      "   2.63293058e-01  4.43367325e-02  2.61353761e-01  2.86699198e-02\n",
      "  -2.76685327e-01  1.05651908e-01 -2.40517855e-02 -1.14407383e-01\n",
      "  -6.48243055e-02 -3.15210968e-01 -5.40970266e-01  2.29274169e-01\n",
      "   9.54980701e-02 -2.26439953e-01 -2.52014518e-01  6.75504804e-02\n",
      "   5.92279971e-01 -8.25073943e-02 -4.54146490e-02  1.38587564e-01\n",
      "  -1.80672966e-02  1.28688782e-01  2.28648260e-01  3.94542277e-01\n",
      "  -2.19153553e-01 -3.34911942e-02 -4.77698967e-02 -3.56259495e-01\n",
      "  -1.75635274e-02 -4.03865039e-01 -4.15371150e-01 -1.81488708e-01\n",
      "   1.29387170e-01  1.23704709e-01 -2.06078604e-01 -5.45871913e-01\n",
      "  -1.79381505e-01 -3.16396087e-01 -5.81365228e-02  1.93893403e-01\n",
      "  -5.74567676e-01 -2.33864054e-01 -2.15599611e-01  9.91214290e-02\n",
      "   2.89086968e-01 -1.24845266e-01  9.15751047e-03  7.45235905e-02\n",
      "  -3.91964393e-04  1.50950104e-01 -3.16433728e-01  7.77892992e-02\n",
      "  -1.11134931e-01  3.33259314e-01  9.66479853e-02  3.06638986e-01\n",
      "  -1.49429291e-02 -8.06261748e-02  3.11280310e-01  1.83992997e-01\n",
      "   1.60052568e-01  3.54117826e-02 -1.60683870e-01  4.72488366e-02\n",
      "   9.96773615e-02 -8.06597713e-03  1.34799749e-01  3.51539582e-01\n",
      "   2.24098280e-01 -2.31395945e-01 -3.65943640e-01 -7.83854537e-03\n",
      "   2.84377038e-01 -2.45100725e-02  7.73569616e-03  4.46129544e-03\n",
      "   1.19799651e-01  1.88376278e-01  1.04297228e-01 -1.66100740e-01\n",
      "   1.11056820e-01  1.03396691e-01 -2.79940903e-01 -9.58340839e-02\n",
      "   1.65699109e-01  7.03751715e-03  2.77101576e-01  4.65342104e-01\n",
      "  -1.23015508e-01 -1.85533315e-02  2.70202458e-01 -2.60436296e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#Entra√Æn√© avec un objectif de d√©tection de paraphrases sur un corpus multilingue.\n",
    "#Performances √©quilibr√©es pour la similarit√© s√©mantique, la recherche d‚Äôinformation et la classification zero-shot en plusieurs langues.\n",
    "\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "print(model.encode([\"Texte √† encoder\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2bed45-6703-4228-8c0e-0815eb36f7b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Similarit√© s√©mantique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "caab2ac4-816b-4b6e-965c-656338c708e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.37034696)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_A = np.linalg.norm(A)\n",
    "    norm_B = np.linalg.norm(B)\n",
    "    return dot_product / (norm_A * norm_B)\n",
    "\n",
    "cosine_similarity(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7e39c-c1af-4389-a8d1-5266facf70e4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Embeddings OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04fc4801-ba9c-49ce-a405-cc15b8480213",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5604925298797377)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "def embed(text, model=\"text-embedding-3-large\", dimensions=3072): #3072: dimension maximale\n",
    "    return openai.embeddings.create(input = [text], model=model, dimensions=dimensions).data[0].embedding\n",
    "\n",
    "vector1 = embed(\"What is Mycobacterium kansasii ?\")\n",
    "vector2 = embed(\"To sum up, we have presented a case of Mycobacterium kansasii monoarthritis of the elbow complicated with unusual clinical and radiological findings. A combination of synovectomy and multidrug antimycobacterial treatment yielded a favorable clinical course without recurrence of arthritis after 10 months of follow-up. This case emphasizes the need to consider this rare infection in the differential diagnosis of intra-articular soft tissue tumor-like lesions of the elbow even in immunocompetent patients.\")\n",
    "cosine_similarity(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f18a96-816d-4c9b-a34b-e6d50220b36f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### RAG : principe de base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283ac8c-eb44-49d9-afe1-a83df312fc48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/rag2.png\" alt=\"RAG\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d363528a-e2de-42b6-b552-2d8d5642df6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mycobacterium kansasii\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(model_name=\"mistral-large-latest\")\n",
    "\n",
    "query = \"What is Mycobacterium kansasii ?\"\n",
    "context = \"To sum up, we have presented a case of Mycobacterium kansasii monoarthritis of the elbow complicated with unusual clinical and radiological findings. A combination of synovectomy and multidrug antimycobacterial treatment yielded a favorable clinical course without recurrence of arthritis after 10 months of follow-up. This case emphasizes the need to consider this rare infection in the differential diagnosis of intra-articular soft tissue tumor-like lesions of the elbow even in immunocompetent patients.\"\n",
    "\n",
    "text = f\"\"\"You are an expert in the Mycobacterium field. \n",
    "Answer to the following question by only using the context below.\n",
    "\n",
    "question: {query}\n",
    "\n",
    "context : {context}\"\"\"\n",
    "\n",
    "response = llm.invoke(text)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d552db-cd59-49c8-9059-8ef5816aebc7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Impl√©mentation d'un vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0dab196-c19e-4681-8851-eaacbdf7e7e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install langchain-community langchain-openai faiss-cpu\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "loader = PyPDFLoader(\"images/Guyeux_2024.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "faiss_index = FAISS.from_documents(pages, embeddings)\n",
    "docs = faiss_index.similarity_search(\"Is there a lineage 10 in M.tuberculosis?\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39a2555d-7d51-43fc-b060-a9890b8aefe6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 3: M. africanum Lineage 10, Central Africa Conclusions Through the extensive mining\n",
      "of WGS and genotyp- ing databases, we newly identified a thus far rare M.\n",
      "tuberculosis complex lineage, L10 (proposed), pres- ent in central Africa. The\n",
      "lineage is characterized by a new region of deletion, IS6110 insertions, and 243\n",
      "SNPs, including gyrA G7901T, recN C1920096T, and dnaG C2621730T. L10 represents\n",
      "a sister clade to L6, found mainly in western Africa, and L9, specifically in\n",
      "eastern Africa, and [...]\n",
      "\n",
      "Page 0: nity of Lille, Lille, France (P. Supply, C. Gaudin); London School of Hygiene\n",
      "and Tropical Medicine, London, UK (J.E. Phelan, T.G. Clark, L. Rigouts, B. de\n",
      "Jong); Universit√© Paris-Saclay, Saint- Aubin, France (C. Sola); Universit√© Paris\n",
      "Cit√©, Paris (C. Sola) DOI: https://doi.org/10.3201/eid3003.231466 Analysis of\n",
      "genome sequencing data from >100,000 genomes of Mycobacterium tuberculosis\n",
      "complex using TB-Annotator software revealed a previously unknown lineage,\n",
      "proposed name L10, in central [...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textwrap import shorten, fill\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"Page {doc.metadata[\"page\"]}: {fill(shorten(doc.page_content, 500), 80)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b272e73a-28b8-485a-bcce-948f1713cebd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Version OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a429dce-16c1-4744-a246-8c7c63fe4398",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install langchain-community langchain-openai faiss-cpu\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "loader = PyPDFLoader(\"images/Guyeux_2024.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())\n",
    "docs = faiss_index.similarity_search(\"Is there a lineage 10 in M.tuberculosis?\", k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32045cf8-3161-48da-9f0c-bdb3617f2aba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "###¬†Text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1efcc3d-7d8e-457c-a1a0-6a1786634749",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = '''Vous pouvez partager un article en cliquant sur les ic√¥nes de partage en haut √† droite de celui-ci. \n",
    "La reproduction totale ou partielle d‚Äôun article, sans l‚Äôautorisation √©crite et pr√©alable du Monde, est strictement interdite. \n",
    "Pour plus d‚Äôinformations, consultez nos conditions g√©n√©rales de vente. \n",
    "\n",
    "Comme la finance, la politique est parfois affaire d‚Äôopportunit√©s. Aux Etats-Unis, l‚Äôopposition d√©mocrate √† Donald Trump a en tout cas trouv√© un nouvel angle d‚Äôattaque apr√®s l‚Äôannonce par le pr√©sident am√©ricain d‚Äôune pause dans sa guerre commerciale : elle le soup√ßonne d‚Äôavoir manipul√© les march√©s boursiers et d‚Äôavoir ainsi favoris√© des d√©lits d‚Äôiniti√©.\n",
    "Lire aussi | Article r√©serv√© √† nos abonn√©s Droits de douane : les Bourses rechutent, l‚Äôinqui√©tude s‚Äô√©tend aux emprunts d‚ÄôEtat\n",
    "\n",
    "Le s√©nateur Adam Schiff a √©crit, jeudi 10 avril, au directeur par int√©rim du Bureau pour l‚Äô√©thique gouvernementale (Office of Government Ethics, OGE), une agence f√©d√©rale ind√©pendante, et √† Susan Wiles, la cheffe de cabinet de la Maison Blanche, pour leur demander d‚Äôouvrir une enqu√™te ¬´ urgente ¬ª afin de d√©terminer si ¬´ le pr√©sident Trump, sa famille ou d‚Äôautres membres de [son] administration ¬ª ont commis la veille des d√©lits d‚Äôiniti√© en profitant d‚Äôinformations confidentielles sur le revirement de sa politique commerciale.\n",
    "\n",
    "'''\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=200,\n",
    "    keep_separator=False,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \"]\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([text])\n",
    "\n",
    "for k in texts[:7]:\n",
    "    print(k.page_content)\n",
    "    print(\"=\"*20+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed54b26-02a2-4d34-b410-b2aed554385a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Loaders (LangChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c28c1c-6e98-4aa6-8a37-e5ccd72331d6",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=YcIbZGTRMjI\", \n",
    "    language=['fr'],\n",
    "    add_video_info=False\n",
    ")\n",
    "\n",
    "print(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02172e7d-480e-4013-af70-b7e12e46cc2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Vectorstores\n",
    "\n",
    "Nombreux et multiples...\n",
    " - FAISS, Chroma : faciles √† ma√Ætriser, d√©ployer...\n",
    " - Milvus : multi-embeddings, BM25, filtrage par colonne...\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/Milvus.png\" alt=\"RAG\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ff316-e795-4cb6-9ac6-4edd4437de66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Les agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f8c84e-b005-4dd3-a906-a49202986f80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Un agent est un syst√®me autonome aliment√© par un mod√®le de langage (comme GPT-4) qui prend des d√©cisions sur les actions √† entreprendre en fonction des donn√©es d'entr√©e et des instructions programm√©es.\n",
    "\n",
    "Fonction :\n",
    "- Prise de d√©cision : L'agent analyse les donn√©es d'entr√©e et utilise des algorithmes et des mod√®les pour d√©cider quelle action entreprendre.\n",
    "- Ex√©cution d'actions : L'agent peut effectuer diverses actions comme r√©pondre √† une question, rechercher des informations, ou interagir avec d'autres syst√®mes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cce33f-8d83-4d9a-9af5-46e06c5526f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Un \"agent\", c'est un LLM avec des \"outils\" :\n",
    " - recherche sur internet,\n",
    " - calculatrice,\n",
    " - interrogation de pdf (RAG),\n",
    " - outil fait maison\n",
    " - ...\n",
    "\n",
    "Le mieux est de faire des agents sp√©cialis√©s, et de les orchestrer ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0fab8a-ede8-4811-bb30-4cfe0b73bafd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Des outils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8355b7-335d-4132-9ee7-598a13f1acbb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea0ea3e-c8fb-4da4-bb41-0ca8a623ff15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install --upgrade --quiet  wikipedia\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "wikipedia.run(\"Alan Turing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f9250e-68a5-43f5-bacd-01ee95a6ea2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Tavili (recherche internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd80876-c248-4d29-a73e-b361472ba727",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install -qU langchain-tavily\n",
    "# Pour une cl√© d'API : https://www.tavily.com/\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "import os\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "search = TavilySearchResults(max_results=2)\n",
    "search_results = search.invoke(\"Quel est le temps √† Belfort ?\")\n",
    "print(search_results[0]['content'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e996e910-9b21-41e6-ac8f-81dff60aa4f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Agents LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6996f99-d3bd-4903-879c-c16338141e43",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Ex√©cuteur d'agent (Agent Executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75632036-6402-4c73-a35f-64989fce0a04",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "L'ex√©cuteur d'agent est un composant ou un syst√®me qui orchestre et ex√©cute les actions d√©termin√©es par l'agent.\n",
    "\n",
    "Fonction :\n",
    " - Gestion de l'ex√©cution : Il re√ßoit les d√©cisions de l'agent, ex√©cute les actions correspondantes et g√®re la transition entre diff√©rentes √©tapes de l'ex√©cution.\n",
    " - Traitement des r√©sultats : Il collecte les r√©sultats des actions ex√©cut√©es et les transmet √† l'agent pour de nouvelles d√©cisions ou √† l'utilisateur final.\n",
    "\n",
    "Exemple : Dans un syst√®me de recommandation, l'ex√©cuteur d'agent pourrait orchestrer l'appel √† diff√©rentes API pour recueillir les informations n√©cessaires (comme les pr√©f√©rences de l'utilisateur et les donn√©es sur les produits) et les combiner pour g√©n√©rer une recommandation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423be8f8-8f9b-48c6-a031-698ac8d13ef8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Avec outil Tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c599b022-cda1-41e3-944f-ac1718152a89",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "search = TavilySearchResults(max_results=2)\n",
    "tools = [search]\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "\n",
    "prompt = hub.pull(\"amalnuaimi/react-mistral\")\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "# Ajout de max_iterations pour √©viter les boucles infinies\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, max_iterations=5)\n",
    "\n",
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        'input': \"Dois-je prendre un parapluie, sachant que je me rends aujourd'hui et demain √† Belfort ?\",\n",
    "        'chat_history': []\n",
    "    })\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0593da3-5ab3-475e-adb9-f077fc50ab03",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://smith.langchain.com/hub\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a201966-1ebe-48a7-9464-b086c0361740",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Version OpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "search = TavilySearchResults(max_results=2)\n",
    "tools = [search]\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1\")\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        'input': \"Dois-je prendre un parapluie, sachant que je me rends aujourd'hui et demain √† Belfort ?\"\n",
    "    })\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d37196e-8e45-4dcc-b3f9-5fb91fbc52c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Avec outil arXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c6254-6f0e-4bfd-bc77-46e078c6a6da",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install arxiv\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "#llm = ChatOpenAI(temperature=0.0)\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "tools = load_tools([\"arxiv\"])\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"R√©sume l'article 1605.08386 en fran√ßais\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b151aecf-48ed-4042-9363-b812319db351",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f045ba-f698-474b-b3eb-6186b55de946",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tools[0].name)\n",
    "print(tools[0].description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8178e85-cb4b-4612-aa5b-34e0817f79dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "####¬†Avec Python REPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5469e-5034-4a73-a1e2-f19b6c23acdc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "tools = [PythonREPLTool()]\n",
    "\n",
    "instructions = \"\"\"You are an agent designed to write and execute python code to answer questions.\n",
    "You have access to a python REPL, which you can use to execute python code.\n",
    "If you get an error, debug your code and try again.\n",
    "Only use the output of your code to answer the question. \n",
    "You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "\"\"\"\n",
    "\n",
    "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)\n",
    "agent = create_openai_functions_agent(ChatOpenAI(temperature=0), tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8950531-534f-402a-b634-a474c6029525",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"Quel est le milli√®me nombre de Fibonacci ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b658b229-7e6f-46f7-95ba-9e09cff78cd8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2b8b2-4ebe-4f1a-aec2-0a941ae3f4f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Plusieurs outils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb01ef0-a7e7-4d50-9d04-35f46503de2d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, create_react_agent\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "tools = load_tools([\"llm-math\", \"wikipedia\"], llm=llm)\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, \n",
    "                               tools=tools, \n",
    "                               handle_parsing_errors=True, \n",
    "                               verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281cf6c1-bb0c-4b45-847d-f404800ae22a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor.invoke({'input': \"Qu'est-ce que 25% de 300?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93bab48-86d3-43ba-b43e-503d69013b0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "###¬†Ses propres outils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6700d8-15cb-4946-83af-a945ed28a613",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"Multiplie deux entiers.\"\"\"\n",
    "    return first_int * second_int\n",
    "\n",
    "print(multiply.name)\n",
    "print(multiply.description)\n",
    "print(multiply.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5432395-1131-407c-82c3-069904e9fba2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(first_int: int, second_int: int) -> int:\n",
    "    \"Ajoute deux entiers.\"\n",
    "    return first_int + second_int\n",
    "\n",
    "@tool\n",
    "def exponentiate(base: int, exponent: int) -> int:\n",
    "    \"Calcule la puissance d'un entier donn√©.\"\n",
    "    return base**exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28db36-9235-4256-906f-3b9645685cb2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1\")\n",
    "\n",
    "tools = [multiply, add, exponentiate]\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Porter 3 √† la puissance 5 et multiplier le r√©sultat par la somme de douze et de trois, puis √©lever le tout au carr√©.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57718046-2846-4ec5-8b77-7098ac55fe81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57b020-d22f-48b1-9bae-110d80e1e376",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Assemblage d'agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712aefa-4e26-4b1e-9ab8-e5d1ba99f8a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![Texte alternatif](images/promptulate.png \"Promptulate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a202d00c-765d-420d-bae9-17fc5f3a6c6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## L'audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af0ac02-c87a-44f0-9b63-ae0560619bb3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896cded-5641-402a-b7c8-2e9a62020ad9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "response = OpenAI().audio.speech.create(\n",
    "  model=\"tts-1-hd\",\n",
    "  voice=\"onyx\", # alloy, onyx, fable, echo\n",
    "  input=\"Coucou, comment allez-vous ?\"\n",
    ")\n",
    "response.with_streaming_response.method('mon_audio.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf0222f-716e-4d3f-9f07-957ab65bd491",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e6de8-2eeb-44e2-88e3-28be8b4f579c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OpenAI().audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=open(\"mon_audio.mp3\", \"rb\"),\n",
    "  language=\"fr\"\n",
    ").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee9c72-f087-4909-b072-43f6f6811c9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydub import AudioSegment\n",
    "import time\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "song = AudioSegment.from_mp3(\"mon_fichier.mp3\")\n",
    "transcription = ''\n",
    "for pas in range(0, 120, 20):\n",
    "    debut, fin = pas * 60 * 1000, (pas+20) * 60 * 1000\n",
    "    extrait = song[debut:fin]\n",
    "    if len(extrait) > 100:\n",
    "        try:\n",
    "            extrait.export(f\"extrait_{pas}.mp3\", format=\"mp3\")\n",
    "            audio_file = open(f\"extrait_{pas}.mp3\", \"rb\")\n",
    "            # Ajout d'un timeout pour √©viter les blocages\n",
    "            start_time = time.time()\n",
    "            result = client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\", \n",
    "                file=audio_file,\n",
    "                timeout=30  # 30 secondes de timeout\n",
    "            )\n",
    "            transcription += result.text\n",
    "            print(f\"Segment {pas} trait√© en {time.time() - start_time:.2f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement du segment {pas}: {e}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e0f2b0-c61a-4411-86c5-448410a9dfc4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0ca3a26-a54d-4b78-a330-3ece1415e665",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image_to_data_url(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Lit l'image et renvoie une data URL pr√™te √† √™tre ins√©r√©e dans le payload.\n",
    "    \"\"\"\n",
    "    # Lecture du fichier image\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        image_bytes = img_file.read()  # Lecture binaire du contenu\n",
    "    # Encodage base64\n",
    "    b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "    # D√©tection du MIME type selon l'extension\n",
    "    ext = os.path.splitext(image_path)[1].lower().lstrip(\".\")\n",
    "    mime = f\"image/{ext if ext != 'jpg' else 'jpeg'}\"\n",
    "    return f\"data:{mime};base64,{b64}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93ae8a23-f595-4254-8b97-f6798878394d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[0;32m----> 5\u001b[0m data_url \u001b[38;5;241m=\u001b[39m \u001b[43mencode_image_to_data_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmon_image.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      8\u001b[0m     {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     }\n\u001b[1;32m     15\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[33], line 13\u001b[0m, in \u001b[0;36mencode_image_to_data_url\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m b64 \u001b[38;5;241m=\u001b[39m base64\u001b[38;5;241m.\u001b[39mb64encode(image_bytes)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# D√©tection du MIME type selon l'extension\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m ext \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(image_path)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m mime \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mext\u001b[38;5;250m \u001b[39m\u001b[38;5;241m!=\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpeg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;base64,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb64\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "data_url = encode_image_to_data_url(\"mon_image.png\")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Que vois-tu sur cette image ?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998bf937-6eeb-41fb-94f3-6bb4621264bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=messages,\n",
    "        max_tokens=500,\n",
    "        temperature=0.0,\n",
    "        timeout=30  # 30 secondes de timeout\n",
    "    )\n",
    "    response.choices[0].message.content\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de l'appel API: {e}\")\n",
    "    \"Erreur lors de la g√©n√©ration de la r√©ponse\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "but3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
