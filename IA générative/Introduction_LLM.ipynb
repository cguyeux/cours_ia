{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313ed1c5-8452-4b3b-ae5a-28e726ff077f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.display import HTML\n",
    "\n",
    "css_adjustment = \"\"\"\n",
    "<style>\n",
    "/* Cacher les prompts de commande */\n",
    "div.prompt {display: none !important;}\n",
    "\n",
    "/* Ajuster la largeur max des cellules de code et de markdown */\n",
    ".jp-RenderedMarkdown, .jp-InputArea {\n",
    "    max-width: 1500px !important; /* Assurez-vous que cela correspond √† la largeur du Markdown si n√©cessaire */\n",
    "}\n",
    "\n",
    "/* Modifier la taille de la police dans les cellules de code */\n",
    ".jp-Notebook .jp-InputArea .input_area {\n",
    "    font-size: 40px !important; /* Ajustez selon vos besoins */\n",
    "}\n",
    "\n",
    "/* Si vous souhaitez ajuster la taille de la police √† l'int√©rieur des blocs de code eux-m√™mes */\n",
    ".jp-Notebook .jp-InputArea .input_area pre, \n",
    ".jp-Notebook .jp-InputArea .input_area code {\n",
    "    font-size: 40px !important; /* Ajustez selon vos besoins */\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "HTML(css_adjustment)\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4226d077-353f-4227-8687-4df42cf0b3a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Programmer avec des LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d98f9-cfeb-4dd1-9a3a-a10138b8b941",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Objectif de ces Travaux Pratiques\n",
    "\n",
    "- Courte introduction √† \"comment programmer avec des LLM\"\n",
    "- Uniquement de la pratique (pas le temps pour la th√©orie)\n",
    "- Evaluation : \n",
    "  - Projet de votre choix de programmation par des LLM\n",
    "  - Note = fonction(quantite_de_travail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d7787-7336-46ec-a985-07dfbb294955",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9bfa5-585a-497e-a580-0a682197168b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### LangChain\n",
    "- Framework Python d'int√©gration de mod√®les LLM\n",
    "  - Model agnostic \n",
    "  - Cha√Ænes et Agents\n",
    "  - Automatisation des prompts\n",
    "  - Gestion des vectorstores, embeddings...\n",
    "\n",
    "- Cas d‚Äôusage : chatbots (RAG), programmes embarquant des LLM, LangGraphs, multi-agents..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3019664-f310-4123-bb6a-e71f6e33864e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Mistral AI\n",
    "\n",
    "1. Cr√©ez un compte sur La Plateforme de Mistral AI. https://mistral.ai/\n",
    "\n",
    "   (\"Try the API\" -> \"S'inscrire\")\n",
    "\n",
    "   (Abonnement : Gratuit / exp√©rimental)\n",
    "   \n",
    "3. G√©n√©rez une cl√© API personnelle.\n",
    "\n",
    "   (API -> Cl√©s API -> Cr√©er une nouvelle cl√© -> ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e098df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # lit le fichier .env\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce133e4-5ed4-4e7c-924e-2951af66c2cf",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Environnement virtuel\n",
    "\n",
    "Il y aura potentiellement des biblioth√®ques √† installer, ce qui peut se faire dans un environnement virtuel python.\n",
    "\n",
    "  $ python -m venv mon\\_env\n",
    "  \n",
    "  $ source mon\\_env/bin/activate\n",
    "  \n",
    "  (mon\\_env) $ pip install nom\\_bibliotheque\n",
    "\n",
    "(Et pour d√©sactiver : deactivate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2addb622-d7a9-4847-bef2-b573f2ee8e27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### R√©f√©rences\n",
    "- LangChain\n",
    "  - https://python.langchain.com/docs/introduction/\n",
    "  - https://www.youtube.com/@LangChain\n",
    "- Mistral AI\n",
    "  - https://docs.mistral.ai/\n",
    "- OpenAI\n",
    "  - https://platform.openai.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3ec0b-529d-4b3a-8fb8-aa1bdca2b1ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Introduction √† LangChain / Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171308f5-cbab-4a41-b1eb-daa5e85a3bc1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Premi√®res invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98e601d0-9ba2-44df-b782-a139ef988eca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capitale de l'Albanie est **Tirana**. üá¶üá±\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialiser le mod√®le\n",
    "llm = ChatMistralAI(model=\"mistral-medium-2508\", \n",
    "                    temperature=0, api_key=api_key)\n",
    "\n",
    "# Cr√©er le message utilisateur\n",
    "message = HumanMessage(content=\"Quelle est la capitale de l'Albanie ?\")\n",
    "\n",
    "# Obtenir la r√©ponse\n",
    "response = llm.invoke([message])\n",
    "\n",
    "# Afficher la r√©ponse\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84dbc14e-6d82-4efa-a6e1-1ed98ba17048",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capitale de l‚ÄôAlbanie est Tirana.\n"
     ]
    }
   ],
   "source": [
    "# Version OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1\")\n",
    "response = llm.invoke(\"Quelle est la capitale de l'Albanie ?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b4bcfe1-8c3e-40b6-97f7-a42dc07baac6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bien s√ªr ! En voici une :\n",
      "\n",
      "**Pourquoi les pompiers ne jouent-ils jamais √† cache-cache ?**\n",
      "*Parce que le bonhomme est toujours facile √† trouver !* üöíüî•\n",
      "\n",
      "(Jeu de mots avec \"le bonhomme\" qui peut √©voquer le mannequin d'entra√Ænement des pompiers ou le \"bonhomme\" en g√©n√©ral.)\n",
      "\n",
      "Tu veux une autre ? üòÑ\n",
      "----------\n",
      "Bien s√ªr ! En voici une :\n",
      "\n",
      "**Pourquoi les policiers n‚Äôaiment-ils pas les livres ?**\n",
      "*Parce qu‚Äôils pr√©f√®rent les* **couvre-feux** ! üòÑ\n",
      "\n",
      "(Jeu de mots entre *couvre-feu* et *couverture* de livre.)\n",
      "\n",
      "---\n",
      "Tu veux une autre ? üòâ\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# 1) Initialisation du mod√®le Mistral\n",
    "model = ChatMistralAI(model=\"mistral-medium-2508\", \n",
    "                      temperature=0, api_key=api_key)\n",
    "\n",
    "# 2) Construction du prompt + parser\n",
    "prompt = ChatPromptTemplate.from_template(\"Fais-moi une blague sur le sujet : {sujet}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 3) Cha√Ænage (LangChain Expression Language)\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# 4) Ex√©cution\n",
    "for sujet in ['pompier', 'police']:\n",
    "    print(chain.invoke({\"sujet\": sujet}))\n",
    "    print('-'*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07cb0e4-94b6-4e04-8959-40435196d1ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m llm = ChatMistralAI(model=\u001b[33m\"\u001b[39m\u001b[33mmistral-medium-2508\u001b[39m\u001b[33m\"\u001b[39m, api_key=api_key)\n\u001b[32m     10\u001b[39m chain = prompt | llm \n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m result = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mest-ce que le mod√®le mistral-large-latest ?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/langchain_core/runnables/base.py:3246\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3244\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3245\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3246\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3247\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3248\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1025\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     **kwargs: Any,\n\u001b[32m   1023\u001b[39m ) -> LLMResult:\n\u001b[32m   1024\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:842\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    841\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m         )\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    850\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1091\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1089\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1095\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/langchain_mistralai/chat_models.py:577\u001b[39m, in \u001b[36mChatMistralAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[39m\n\u001b[32m    575\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    576\u001b[39m params = {**params, **kwargs}\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/langchain_mistralai/chat_models.py:494\u001b[39m, in \u001b[36mChatMistralAI.completion_with_retry\u001b[39m\u001b[34m(self, run_manager, **kwargs)\u001b[39m\n\u001b[32m    491\u001b[39m     _raise_on_error(response)\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/langchain_mistralai/chat_models.py:490\u001b[39m, in \u001b[36mChatMistralAI.completion_with_retry.<locals>._completion_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    487\u001b[39m                 \u001b[38;5;28;01myield\u001b[39;00m event.json()\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m iter_sse()\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m _raise_on_error(response)\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/httpx/_client.py:1144\u001b[39m, in \u001b[36mClient.post\u001b[39m\u001b[34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1124\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1125\u001b[39m     url: URL | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1137\u001b[39m     extensions: RequestExtensions | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1138\u001b[39m ) -> Response:\n\u001b[32m   1139\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1140\u001b[39m \u001b[33;03m    Send a `POST` request.\u001b[39;00m\n\u001b[32m   1141\u001b[39m \n\u001b[32m   1142\u001b[39m \u001b[33;03m    **Parameters**: See `httpx.request`.\u001b[39;00m\n\u001b[32m   1143\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/but3/lib/python3.13/ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Vous √™tes un r√©dacteur de documentation technique de classe mondiale.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-medium-2508\", api_key=api_key)\n",
    "chain = prompt | llm \n",
    "result = chain.invoke({\"input\": \"Qu'est-ce que le mod√®le mistral-large-latest ?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dfaa652-299d-4d7a-a480-4e17a079cf19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 38, 'output_tokens': 701, 'total_tokens': 739}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "120dac24-febc-4714-ad88-8033d5bd9d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056156"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "38/1000000*2+701*8/100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb7b1825-15de-4b92-8d7e-7ff0e670e212",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral-large-latest est un mod√®le de langage d√©velopp√© par Mistral AI, une soci√©t√© de pointe dans le domaine de l'intelligence artificielle. Ce mod√®le est con√ßu pour comprendre et g√©n√©rer du texte en r√©ponse √† une vari√©t√© d'entr√©es, ce qui le rend utile pour diverses applications telles que la r√©daction de documentation technique, la traduction, la g√©n√©ration de contenu, et bien plus encore.\n",
      "\n",
      "### Caract√©ristiques principales de Mistral-large-latest :\n",
      "\n",
      "1. **Taille et Capacit√©** :\n",
      "   - Le mod√®le est de grande taille, ce qui lui permet de comprendre et de g√©n√©rer du texte de mani√®re tr√®s sophistiqu√©e.\n",
      "   - Il est entra√Æn√© sur une vaste quantit√© de donn√©es textuelles, ce qui lui conf√®re une large compr√©hension du langage naturel.\n",
      "\n",
      "2. **Polyvalence** :\n",
      "   - Mistral-large-latest peut √™tre utilis√© pour une vari√©t√© de t√¢ches, y compris la r√©daction de documents techniques, la r√©ponse √† des questions, la g√©n√©ration de contenu cr√©atif, et m√™me la traduction entre diff√©rentes langues.\n",
      "\n",
      "3. **Pr√©cision et Coh√©rence** :\n",
      "   - Le mod√®le est con√ßu pour produire des r√©ponses pr√©cises et coh√©rentes, ce qui est crucial pour des applications professionnelles et techniques.\n",
      "\n",
      "4. **Mise √† Jour R√©guli√®re** :\n",
      "   - Le terme \"latest\" indique que le mod√®le est r√©guli√®rement mis √† jour pour inclure les derni√®res am√©liorations et corrections, assurant ainsi des performances optimales.\n",
      "\n",
      "### Utilisations Potentielles :\n",
      "\n",
      "1. **R√©daction de Documentation Technique** :\n",
      "   - Mistral-large-latest peut aider √† r√©diger des manuels, des guides d'utilisation, des sp√©cifications techniques, et d'autres documents n√©cessitant une grande pr√©cision et clart√©.\n",
      "\n",
      "2. **Support Client** :\n",
      "   - Il peut √™tre int√©gr√© dans des syst√®mes de support client pour fournir des r√©ponses automatis√©es aux questions fr√©quentes des utilisateurs.\n",
      "\n",
      "3. **G√©n√©ration de Contenu** :\n",
      "   - Le mod√®le peut √™tre utilis√© pour g√©n√©rer du contenu cr√©atif, comme des articles de blog, des scripts, ou m√™me des histoires.\n",
      "\n",
      "4. **Traduction** :\n",
      "   - Il peut aider √† traduire des textes entre diff√©rentes langues, facilitant ainsi la communication internationale.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Mistral-large-latest est un outil puissant et polyvalent pour toute personne ou organisation ayant besoin de solutions avanc√©es en traitement du langage naturel. Sa capacit√© √† comprendre et g√©n√©rer du texte de mani√®re pr√©cise et coh√©rente en fait un choix id√©al pour une vari√©t√© d'applications professionnelles et techniques.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0079cb0a-8f86-462f-9ed4-92a886d3bcbd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Les sorties structur√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ffa5cd-8e50-4ea5-83a0-32acc0114322",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Cas d'un bool√©en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd16a7ce-de15-4342-b612-0bec866e82af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No√´l est en hiver\n",
      "R√©ponse : True (type : <class 'bool'>)\n",
      "\n",
      "Il pleut quand il pleut pas\n",
      "R√©ponse : False (type : <class 'bool'>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    answer: bool\n",
    "\n",
    "prompt_answer = [\n",
    "    (\"system\", \"Tu es un assistant charg√© de r√©pondre un bool√©en (True ou False) √† la question d'un utilisateur.\"),\n",
    "    ('human', \"{question}\")\n",
    "]\n",
    "\n",
    "prompt_answer_template = ChatPromptTemplate.from_messages(prompt_answer)\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "chain = prompt_answer_template | llm.with_structured_output(schema=Answer)\n",
    "\n",
    "def repond(question):\n",
    "    return chain.invoke({\"question\": question}).answer\n",
    "    \n",
    "for question in [\"No√´l est en hiver\", \"Il pleut quand il pleut pas\"]:\n",
    "    print(question)\n",
    "    reponse = repond(question)\n",
    "    print(f\"R√©ponse : {reponse} (type : {type(reponse)})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57646f4-a101-441d-a491-cde12e06a8f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Cas d'une classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea1dad8b-78fd-4656-ac3e-e4865ef70c4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peux-tu m'en dire plus\n",
      "action=\"Fournir plus d'√©l√©ments √† la question pr√©c√©dente\"\n",
      "\n",
      "Que sont les PPV ?\n",
      "action='R√©pondre √† une nouvelle question'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "tasks = [\"R√©pondre √† une nouvelle question\", \"Fournir plus d'√©l√©ments √† la question pr√©c√©dente\"]\n",
    "\n",
    "class NextTask(BaseModel):\n",
    "    \"\"\"Utilise toujours cet outil pour structurer ta r√©ponse to the user.\"\"\"\n",
    "    action: str = Field(..., \n",
    "                        enum=tasks,\n",
    "                        description=\"La prochaine action √† mener\")\n",
    "\n",
    "prompt_message = [\n",
    "    (\"system\", \"Tu es un assistant charg√© de classifier la demande d'un utilisateur parmi une \"\n",
    "               \"liste r√©duite d'actions √† mener en tant que chatbot. Tu dois d√©terminer la \"\n",
    "               \"prochaine action √† mener.\"),\n",
    "    ('human', \"{text}\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(prompt_message)\n",
    "llm = ChatMistralAI(model='mistral-large-latest', temperature=0)\n",
    "chain = prompt | llm.with_structured_output(schema=NextTask)\n",
    "\n",
    "for text in [\"Peux-tu m'en dire plus\", \"Que sont les PPV ?\"]:\n",
    "    print(text)\n",
    "    print(chain.invoke({\"text\": text}))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3bd17-0379-467b-9be9-06544afcaf0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Cas d'un entier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8b502d-8200-4ca8-b0c0-3969c86dab19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message : Bonjour, pourrais-tu m'aider s'il te pla√Æt ?\n",
      "note_ton=4\n",
      "\n",
      "Message : J'ai besoin de √ßa imm√©diatement.\n",
      "note_ton=1\n",
      "\n",
      "Message : Merci beaucoup pour ton aide pr√©cieuse !\n",
      "note_ton=5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "class TonMessage(BaseModel):\n",
    "    \"\"\"√âvaluation du ton du message de l'utilisateur.\"\"\"\n",
    "    note_ton: int = Field(\n",
    "        ..., \n",
    "        ge=1,\n",
    "        le=5,\n",
    "        description=\"Note attribu√©e au ton du message : 1 pour neutre, 5 pour tr√®s aimable\"\n",
    "    )\n",
    "\n",
    "prompt_message = [\n",
    "    (\"system\", \"Tu es un assistant charg√© d'√©valuer le ton d'un message donn√© par l'utilisateur. \"\n",
    "               \"Attribue une note de 1 √† 5 au ton du message, o√π 1 signifie neutre et 5 signifie tr√®s aimable.\"),\n",
    "    ('human', \"{text}\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(prompt_message)\n",
    "llm = ChatMistralAI(model='mistral-large-latest', temperature=0)\n",
    "chain = prompt | llm.with_structured_output(schema=TonMessage)\n",
    "\n",
    "messages = [\n",
    "    \"Bonjour, pourrais-tu m'aider s'il te pla√Æt ?\",\n",
    "    \"J'ai besoin de √ßa imm√©diatement.\",\n",
    "    \"Merci beaucoup pour ton aide pr√©cieuse !\"\n",
    "]\n",
    "\n",
    "for text in messages:\n",
    "    print(f\"Message : {text}\")\n",
    "    print(chain.invoke({\"text\": text}))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d36f5d-0be7-47cf-8b7d-88e0e28bae71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Pourquoi et comment forcer la sortie du LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76a94afe-6764-46bb-9aaf-7b4583eb1f5c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The translation of \"Quelle est la capitale de l\\'Albanie ?\" in English is:\\n\\n**\"What is the capital of Albania?\"**', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 26, 'total_tokens': 56, 'completion_tokens': 30}, 'model_name': 'mistral-medium-latest', 'model': 'mistral-medium-latest', 'finish_reason': 'stop'}, id='run--c4b5c2c7-5a43-4f1d-a93e-0ed2170a7818-0', usage_metadata={'input_tokens': 26, 'output_tokens': 30, 'total_tokens': 56})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-medium-latest\")\n",
    "message = HumanMessage(content=\"Peux-tu me traduire ce qui suit, en anglais ?\\n\\n Quelle est la capitale de l'Albanie ?\")\n",
    "llm.invoke([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e8c85e9-e6fa-4812-8718-50c1ee92a5cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Albania\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Translation(BaseModel):\n",
    "    original_text: str = Field(..., description=\"The original text before translation in another language\")\n",
    "    original_language: str = Field(..., description=\"The original language before translation\")\n",
    "    translated_text: str = Field(..., description=\"The final text after translation in another language\")\n",
    "    translated_language: str = Field(..., description=\"The language into which the translation must be done\")\n",
    "    \n",
    "def traduit(texte, langue_source=\"fran√ßais\", langue_cible=\"anglais\"):\n",
    "    llm = ChatMistralAI(model_name=\"mistral-medium-latest\")\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"Je souhaite que tu traduises le texte suivant du {langue_source} vers le {langue_cible}. Ta traduction doit √™tre pr√©cise, fluide et naturelle, et pr√©server parfaitement le sens original. \n",
    "    Retourne-moi la r√©ponse sous forme d'objet JSON avec les champs :\n",
    "      - original_text : le texte original\n",
    "      - original_language : la langue du texte original\n",
    "      - translated_text : la traduction du texte\n",
    "      - translated_language : la langue de la traduction\n",
    "\n",
    "    Voici le texte √† traduire :\n",
    "    ----\n",
    "    {texte}\"\"\")\n",
    "    output_parser = StrOutputParser()\n",
    "    extract_translation = RunnableLambda(lambda translation: translation.translated_text)\n",
    "    chain0 = prompt | llm.with_structured_output(Translation) | extract_translation\n",
    "    return chain0.invoke({\"langue_source\": langue_source,\n",
    "                            \"langue_cible\": \"anglais\",\n",
    "                            \"texte\": texte      \n",
    "                            })\n",
    "\n",
    "print(traduit(\"Quelle est la capitale de l'Albanie\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2032b-c358-4aec-b673-43b4e1afed83",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Des sorties structur√©es aux pr√©mices d'un raisonnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "462e27ab-59d3-4bc3-aab0-37c73e620d48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "class Etape(BaseModel):\n",
    "    explication: str\n",
    "    sortie: str\n",
    "\n",
    "class MathReponse(BaseModel):\n",
    "    etapes: list[Etape]\n",
    "    reponse_finale: str\n",
    "\n",
    "prompt_answer = [\n",
    "    (\"system\", \"Tu es un professeur de math√©matiques tr√®s p√©dagogue.\"),\n",
    "    ('human', \"{exercice}\")\n",
    "]\n",
    "\n",
    "prompt_answer_template = ChatPromptTemplate.from_messages(prompt_answer)\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "chain = prompt_answer_template | llm.with_structured_output(schema=MathReponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83f49b83-7fe3-475a-818b-43c232ddaea2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Soustrayons 31 des deux c√¥t√©s de l'√©quation pour isoler le terme en x.\n",
      "  Le r√©sultat est alors : 8x + 31 - 31 = 2 - 31\n",
      "8x = -29\n",
      "- Divisons les deux c√¥t√©s par 8 pour r√©soudre x.\n",
      "  Le r√©sultat est alors : 8x / 8 = -29 / 8\n",
      "x = -29/8 ou x = -3,625\n",
      "Au final, on trouve : x = -3,625\n"
     ]
    }
   ],
   "source": [
    "explications = chain.invoke({\"exercice\": \"R√©sous  8x + 31 = 2\"})\n",
    "for etape in explications.etapes:\n",
    "    print(f\"- {etape.explication}\")\n",
    "    print(f\"  Le r√©sultat est alors : {etape.sortie}\")\n",
    "\n",
    "print(f\"Au final, on trouve : {explications.reponse_finale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7905089f-3155-4381-91e5-17a2ba2aac9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918186c0-294c-4a8f-b0f8-65afd6f06a42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/rag.png\" alt=\"RAG\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589d138-b105-4567-aed0-ab5fb462ca51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Embeddings et semantique\n",
    "\n",
    "But : encoder un texte sous la forme d'un vecteur, de sorte que deux textes voisins s√©mantiquement soient encod√©s en deux vecteurs proches.\n",
    "\n",
    "![Texte alternatif](images/vectors-and-semantics.png \"Vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee12db-412b-43bb-b0bd-9c680335617a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Embeddings : Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6a1c67-5364-4d43-bdb9-cf1f6bf7dc6d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "![Texte alternatif](images/Bag-of-words.png \"BoW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "188424dd-b206-46a9-bcfb-3c5c2277cebb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary : ['and' 'demo' 'demonstration' 'document' 'finally' 'first' 'here' 'is'\n",
      " 'second' 'text' 'the' 'third' 'this']\n",
      "BoW vector:\n",
      " [[0 0 1 1 0 1 0 0 0 1 0 0 0]\n",
      " [1 1 0 1 0 0 1 0 1 1 0 0 0]\n",
      " [1 0 0 1 1 0 0 1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'Demonstration text, first document',\n",
    "    \"Demo text, and here's a second document.\",\n",
    "    'And finally, this is the third document.'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary :\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW vector:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11281bfe-e00f-49e0-8ae1-108605a139cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Embeddings par transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34d487f9-12a6-47ff-8609-0fedc2cb7064",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is an example sentence.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEach sentence is converted into a fixed-sized vector.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Entra√Æn√© sur des donn√©es essentiellement anglophones.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Con√ßu pour √™tre l√©ger et rapide, tout en gardant une bonne pr√©cision pour l‚Äôanglais.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence-transformers/all-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(sentences)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence, embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sentences, embeddings):\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:367\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_hpu_graph_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts:\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 915 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = [\"This is an example sentence.\", \"Each sentence is converted into a fixed-sized vector.\"]\n",
    "\n",
    "# Entra√Æn√© sur des donn√©es essentiellement anglophones.\n",
    "# Con√ßu pour √™tre l√©ger et rapide, tout en gardant une bonne pr√©cision pour l‚Äôanglais.\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(f'\"{sentence}\" -> {embedding[:3]}...')\n",
    "\n",
    "print(f\"Embedding size: {len(embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb7a8bf-fc00-4b03-a5bf-32890b7f45ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#Entra√Æn√© avec un objectif de d√©tection de paraphrases sur un corpus multilingue.\n",
    "#Performances √©quilibr√©es pour la similarit√© s√©mantique, la recherche d‚Äôinformation et la classification zero-shot en plusieurs langues.\n",
    "\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "print(model.encode([\"Texte √† encoder\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2bed45-6703-4228-8c0e-0815eb36f7b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Similarit√© s√©mantique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "caab2ac4-816b-4b6e-965c-656338c708e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.37034684)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_A = np.linalg.norm(A)\n",
    "    norm_B = np.linalg.norm(B)\n",
    "    return dot_product / (norm_A * norm_B)\n",
    "\n",
    "cosine_similarity(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7e39c-c1af-4389-a8d1-5266facf70e4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Embeddings OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04fc4801-ba9c-49ce-a405-cc15b8480213",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5604925298797377)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "def embed(text, model=\"text-embedding-3-large\", dimensions=3072): #3072: dimension maximale\n",
    "    return openai.embeddings.create(input = [text], model=model, dimensions=dimensions).data[0].embedding\n",
    "\n",
    "vector1 = embed(\"What is Mycobacterium kansasii ?\")\n",
    "vector2 = embed(\"To sum up, we have presented a case of Mycobacterium kansasii monoarthritis of the elbow complicated with unusual clinical and radiological findings. A combination of synovectomy and multidrug antimycobacterial treatment yielded a favorable clinical course without recurrence of arthritis after 10 months of follow-up. This case emphasizes the need to consider this rare infection in the differential diagnosis of intra-articular soft tissue tumor-like lesions of the elbow even in immunocompetent patients.\")\n",
    "cosine_similarity(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f18a96-816d-4c9b-a34b-e6d50220b36f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### RAG : principe de base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283ac8c-eb44-49d9-afe1-a83df312fc48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/rag2.png\" alt=\"RAG\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d363528a-e2de-42b6-b552-2d8d5642df6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mycobacterium kansasii\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(model_name=\"mistral-large-latest\")\n",
    "\n",
    "query = \"What is Mycobacterium kansasii ?\"\n",
    "context = \"To sum up, we have presented a case of Mycobacterium kansasii monoarthritis of the elbow complicated with unusual clinical and radiological findings. A combination of synovectomy and multidrug antimycobacterial treatment yielded a favorable clinical course without recurrence of arthritis after 10 months of follow-up. This case emphasizes the need to consider this rare infection in the differential diagnosis of intra-articular soft tissue tumor-like lesions of the elbow even in immunocompetent patients.\"\n",
    "\n",
    "text = f\"\"\"You are an expert in the Mycobacterium field. \n",
    "Answer to the following question by only using the context below.\n",
    "\n",
    "question: {query}\n",
    "\n",
    "context : {context}\"\"\"\n",
    "\n",
    "response = llm.invoke(text)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d552db-cd59-49c8-9059-8ef5816aebc7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Impl√©mentation d'un vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0dab196-c19e-4681-8851-eaacbdf7e7e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install langchain-community langchain-openai faiss-cpu\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "loader = PyPDFLoader(\"images/Guyeux_2024.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "faiss_index = FAISS.from_documents(pages, embeddings)\n",
    "docs = faiss_index.similarity_search(\"Is there a lineage 10 in M.tuberculosis?\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39a2555d-7d51-43fc-b060-a9890b8aefe6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 3: M. africanum Lineage 10, Central Africa Conclusions Through the extensive mining\n",
      "of WGS and genotyp- ing databases, we newly identified a thus far rare M.\n",
      "tuberculosis complex lineage, L10 (proposed), pres- ent in central Africa. The\n",
      "lineage is characterized by a new region of deletion, IS6110 insertions, and 243\n",
      "SNPs, including gyrA G7901T, recN C1920096T, and dnaG C2621730T. L10 represents\n",
      "a sister clade to L6, found mainly in western Africa, and L9, specifically in\n",
      "eastern Africa, and [...]\n",
      "\n",
      "Page 0: nity of Lille, Lille, France (P. Supply, C. Gaudin); London School of Hygiene\n",
      "and Tropical Medicine, London, UK (J.E. Phelan, T.G. Clark, L. Rigouts, B. de\n",
      "Jong); Universit√© Paris-Saclay, Saint- Aubin, France (C. Sola); Universit√© Paris\n",
      "Cit√©, Paris (C. Sola) DOI: https://doi.org/10.3201/eid3003.231466 Analysis of\n",
      "genome sequencing data from >100,000 genomes of Mycobacterium tuberculosis\n",
      "complex using TB-Annotator software revealed a previously unknown lineage,\n",
      "proposed name L10, in central [...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textwrap import shorten, fill\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"Page {doc.metadata[\"page\"]}: {fill(shorten(doc.page_content, 500), 80)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b272e73a-28b8-485a-bcce-948f1713cebd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Version OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a429dce-16c1-4744-a246-8c7c63fe4398",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install langchain-community langchain-openai faiss-cpu\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "loader = PyPDFLoader(\"images/Guyeux_2024.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())\n",
    "docs = faiss_index.similarity_search(\"Is there a lineage 10 in M.tuberculosis?\", k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32045cf8-3161-48da-9f0c-bdb3617f2aba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "###¬†Text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1efcc3d-7d8e-457c-a1a0-6a1786634749",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = '''Vous pouvez partager un article en cliquant sur les ic√¥nes de partage en haut √† droite de celui-ci. \n",
    "La reproduction totale ou partielle d‚Äôun article, sans l‚Äôautorisation √©crite et pr√©alable du Monde, est strictement interdite. \n",
    "Pour plus d‚Äôinformations, consultez nos conditions g√©n√©rales de vente. \n",
    "\n",
    "Comme la finance, la politique est parfois affaire d‚Äôopportunit√©s. Aux Etats-Unis, l‚Äôopposition d√©mocrate √† Donald Trump a en tout cas trouv√© un nouvel angle d‚Äôattaque apr√®s l‚Äôannonce par le pr√©sident am√©ricain d‚Äôune pause dans sa guerre commerciale : elle le soup√ßonne d‚Äôavoir manipul√© les march√©s boursiers et d‚Äôavoir ainsi favoris√© des d√©lits d‚Äôiniti√©.\n",
    "Lire aussi | Article r√©serv√© √† nos abonn√©s Droits de douane : les Bourses rechutent, l‚Äôinqui√©tude s‚Äô√©tend aux emprunts d‚ÄôEtat\n",
    "\n",
    "Le s√©nateur Adam Schiff a √©crit, jeudi 10 avril, au directeur par int√©rim du Bureau pour l‚Äô√©thique gouvernementale (Office of Government Ethics, OGE), une agence f√©d√©rale ind√©pendante, et √† Susan Wiles, la cheffe de cabinet de la Maison Blanche, pour leur demander d‚Äôouvrir une enqu√™te ¬´ urgente ¬ª afin de d√©terminer si ¬´ le pr√©sident Trump, sa famille ou d‚Äôautres membres de [son] administration ¬ª ont commis la veille des d√©lits d‚Äôiniti√© en profitant d‚Äôinformations confidentielles sur le revirement de sa politique commerciale.\n",
    "\n",
    "'''\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=200,\n",
    "    keep_separator=False,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \"]\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([text])\n",
    "\n",
    "for k in texts[:7]:\n",
    "    print(k.page_content)\n",
    "    print(\"=\"*20+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed54b26-02a2-4d34-b410-b2aed554385a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Loaders (LangChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c28c1c-6e98-4aa6-8a37-e5ccd72331d6",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=YcIbZGTRMjI\", \n",
    "    language=['fr'],\n",
    "    add_video_info=False\n",
    ")\n",
    "\n",
    "print(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02172e7d-480e-4013-af70-b7e12e46cc2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Vectorstores\n",
    "\n",
    "Nombreux et multiples...\n",
    " - FAISS, Chroma : faciles √† ma√Ætriser, d√©ployer...\n",
    " - Milvus : multi-embeddings, BM25, filtrage par colonne...\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/Milvus.png\" alt=\"RAG\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ff316-e795-4cb6-9ac6-4edd4437de66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Les agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f8c84e-b005-4dd3-a906-a49202986f80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Un agent est un syst√®me autonome aliment√© par un mod√®le de langage (comme GPT-4) qui prend des d√©cisions sur les actions √† entreprendre en fonction des donn√©es d'entr√©e et des instructions programm√©es.\n",
    "\n",
    "Fonction :\n",
    "- Prise de d√©cision : L'agent analyse les donn√©es d'entr√©e et utilise des algorithmes et des mod√®les pour d√©cider quelle action entreprendre.\n",
    "- Ex√©cution d'actions : L'agent peut effectuer diverses actions comme r√©pondre √† une question, rechercher des informations, ou interagir avec d'autres syst√®mes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cce33f-8d83-4d9a-9af5-46e06c5526f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Un \"agent\", c'est un LLM avec des \"outils\" :\n",
    " - recherche sur internet,\n",
    " - calculatrice,\n",
    " - interrogation de pdf (RAG),\n",
    " - outil fait maison\n",
    " - ...\n",
    "\n",
    "Le mieux est de faire des agents sp√©cialis√©s, et de les orchestrer ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0fab8a-ede8-4811-bb30-4cfe0b73bafd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Des outils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8355b7-335d-4132-9ee7-598a13f1acbb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea0ea3e-c8fb-4da4-bb41-0ca8a623ff15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install --upgrade --quiet  wikipedia\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "wikipedia.run(\"Alan Turing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f9250e-68a5-43f5-bacd-01ee95a6ea2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Tavili (recherche internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd80876-c248-4d29-a73e-b361472ba727",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install -qU langchain-tavily\n",
    "# Pour une cl√© d'API : https://www.tavily.com/\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "import os\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "search = TavilySearchResults(max_results=2)\n",
    "search_results = search.invoke(\"Quel est le temps √† Belfort ?\")\n",
    "print(search_results[0]['content'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e996e910-9b21-41e6-ac8f-81dff60aa4f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Agents LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6996f99-d3bd-4903-879c-c16338141e43",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Ex√©cuteur d'agent (Agent Executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75632036-6402-4c73-a35f-64989fce0a04",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "L'ex√©cuteur d'agent est un composant ou un syst√®me qui orchestre et ex√©cute les actions d√©termin√©es par l'agent.\n",
    "\n",
    "Fonction :\n",
    " - Gestion de l'ex√©cution : Il re√ßoit les d√©cisions de l'agent, ex√©cute les actions correspondantes et g√®re la transition entre diff√©rentes √©tapes de l'ex√©cution.\n",
    " - Traitement des r√©sultats : Il collecte les r√©sultats des actions ex√©cut√©es et les transmet √† l'agent pour de nouvelles d√©cisions ou √† l'utilisateur final.\n",
    "\n",
    "Exemple : Dans un syst√®me de recommandation, l'ex√©cuteur d'agent pourrait orchestrer l'appel √† diff√©rentes API pour recueillir les informations n√©cessaires (comme les pr√©f√©rences de l'utilisateur et les donn√©es sur les produits) et les combiner pour g√©n√©rer une recommandation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423be8f8-8f9b-48c6-a031-698ac8d13ef8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Avec outil Tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c599b022-cda1-41e3-944f-ac1718152a89",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "search = TavilySearchResults(max_results=2)\n",
    "tools = [search]\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "\n",
    "prompt = hub.pull(\"amalnuaimi/react-mistral\")\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "# Ajout de max_iterations pour √©viter les boucles infinies\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, max_iterations=5)\n",
    "\n",
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        'input': \"Dois-je prendre un parapluie, sachant que je me rends aujourd'hui et demain √† Belfort ?\",\n",
    "        'chat_history': []\n",
    "    })\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0593da3-5ab3-475e-adb9-f077fc50ab03",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://smith.langchain.com/hub\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a201966-1ebe-48a7-9464-b086c0361740",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Version OpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "search = TavilySearchResults(max_results=2)\n",
    "tools = [search]\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1\")\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        'input': \"Dois-je prendre un parapluie, sachant que je me rends aujourd'hui et demain √† Belfort ?\"\n",
    "    })\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d37196e-8e45-4dcc-b3f9-5fb91fbc52c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Avec outil arXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c6254-6f0e-4bfd-bc77-46e078c6a6da",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install arxiv\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "#llm = ChatOpenAI(temperature=0.0)\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "tools = load_tools([\"arxiv\"])\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"R√©sume l'article 1605.08386 en fran√ßais\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b151aecf-48ed-4042-9363-b812319db351",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f045ba-f698-474b-b3eb-6186b55de946",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tools[0].name)\n",
    "print(tools[0].description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8178e85-cb4b-4612-aa5b-34e0817f79dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "####¬†Avec Python REPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5469e-5034-4a73-a1e2-f19b6c23acdc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "tools = [PythonREPLTool()]\n",
    "\n",
    "instructions = \"\"\"You are an agent designed to write and execute python code to answer questions.\n",
    "You have access to a python REPL, which you can use to execute python code.\n",
    "If you get an error, debug your code and try again.\n",
    "Only use the output of your code to answer the question. \n",
    "You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "\"\"\"\n",
    "\n",
    "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)\n",
    "agent = create_openai_functions_agent(ChatOpenAI(temperature=0), tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8950531-534f-402a-b634-a474c6029525",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"Quel est le milli√®me nombre de Fibonacci ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b658b229-7e6f-46f7-95ba-9e09cff78cd8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2b8b2-4ebe-4f1a-aec2-0a941ae3f4f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Plusieurs outils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb01ef0-a7e7-4d50-9d04-35f46503de2d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, create_react_agent\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "tools = load_tools([\"llm-math\", \"wikipedia\"], llm=llm)\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, \n",
    "                               tools=tools, \n",
    "                               handle_parsing_errors=True, \n",
    "                               verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281cf6c1-bb0c-4b45-847d-f404800ae22a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor.invoke({'input': \"Qu'est-ce que 25% de 300?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93bab48-86d3-43ba-b43e-503d69013b0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "###¬†Ses propres outils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6700d8-15cb-4946-83af-a945ed28a613",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"Multiplie deux entiers.\"\"\"\n",
    "    return first_int * second_int\n",
    "\n",
    "print(multiply.name)\n",
    "print(multiply.description)\n",
    "print(multiply.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5432395-1131-407c-82c3-069904e9fba2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(first_int: int, second_int: int) -> int:\n",
    "    \"Ajoute deux entiers.\"\n",
    "    return first_int + second_int\n",
    "\n",
    "@tool\n",
    "def exponentiate(base: int, exponent: int) -> int:\n",
    "    \"Calcule la puissance d'un entier donn√©.\"\n",
    "    return base**exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28db36-9235-4256-906f-3b9645685cb2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1\")\n",
    "\n",
    "tools = [multiply, add, exponentiate]\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Porter 3 √† la puissance 5 et multiplier le r√©sultat par la somme de douze et de trois, puis √©lever le tout au carr√©.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57718046-2846-4ec5-8b77-7098ac55fe81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57b020-d22f-48b1-9bae-110d80e1e376",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Assemblage d'agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712aefa-4e26-4b1e-9ab8-e5d1ba99f8a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![Texte alternatif](images/promptulate.png \"Promptulate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a202d00c-765d-420d-bae9-17fc5f3a6c6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## L'audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af0ac02-c87a-44f0-9b63-ae0560619bb3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896cded-5641-402a-b7c8-2e9a62020ad9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "response = OpenAI().audio.speech.create(\n",
    "  model=\"tts-1-hd\",\n",
    "  voice=\"onyx\", # alloy, onyx, fable, echo\n",
    "  input=\"Coucou, comment allez-vous ?\"\n",
    ")\n",
    "response.with_streaming_response.method('mon_audio.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf0222f-716e-4d3f-9f07-957ab65bd491",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e6de8-2eeb-44e2-88e3-28be8b4f579c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OpenAI().audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=open(\"mon_audio.mp3\", \"rb\"),\n",
    "  language=\"fr\"\n",
    ").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee9c72-f087-4909-b072-43f6f6811c9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydub import AudioSegment\n",
    "import time\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "song = AudioSegment.from_mp3(\"mon_fichier.mp3\")\n",
    "transcription = ''\n",
    "for pas in range(0, 120, 20):\n",
    "    debut, fin = pas * 60 * 1000, (pas+20) * 60 * 1000\n",
    "    extrait = song[debut:fin]\n",
    "    if len(extrait) > 100:\n",
    "        try:\n",
    "            extrait.export(f\"extrait_{pas}.mp3\", format=\"mp3\")\n",
    "            audio_file = open(f\"extrait_{pas}.mp3\", \"rb\")\n",
    "            # Ajout d'un timeout pour √©viter les blocages\n",
    "            start_time = time.time()\n",
    "            result = client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\", \n",
    "                file=audio_file,\n",
    "                timeout=30  # 30 secondes de timeout\n",
    "            )\n",
    "            transcription += result.text\n",
    "            print(f\"Segment {pas} trait√© en {time.time() - start_time:.2f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement du segment {pas}: {e}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e0f2b0-c61a-4411-86c5-448410a9dfc4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0ca3a26-a54d-4b78-a330-3ece1415e665",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image_to_data_url(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Lit l'image et renvoie une data URL pr√™te √† √™tre ins√©r√©e dans le payload.\n",
    "    \"\"\"\n",
    "    # Lecture du fichier image\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        image_bytes = img_file.read()  # Lecture binaire du contenu\n",
    "    # Encodage base64\n",
    "    b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "    # D√©tection du MIME type selon l'extension\n",
    "    ext = os.path.splitext(image_path)[1].lower().lstrip(\".\")\n",
    "    mime = f\"image/{ext if ext != 'jpg' else 'jpeg'}\"\n",
    "    return f\"data:{mime};base64,{b64}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93ae8a23-f595-4254-8b97-f6798878394d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[0;32m----> 5\u001b[0m data_url \u001b[38;5;241m=\u001b[39m \u001b[43mencode_image_to_data_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmon_image.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      8\u001b[0m     {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     }\n\u001b[1;32m     15\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[33], line 13\u001b[0m, in \u001b[0;36mencode_image_to_data_url\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m b64 \u001b[38;5;241m=\u001b[39m base64\u001b[38;5;241m.\u001b[39mb64encode(image_bytes)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# D√©tection du MIME type selon l'extension\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m ext \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(image_path)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m mime \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mext\u001b[38;5;250m \u001b[39m\u001b[38;5;241m!=\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpeg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;base64,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb64\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "data_url = encode_image_to_data_url(\"mon_image.png\")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Que vois-tu sur cette image ?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998bf937-6eeb-41fb-94f3-6bb4621264bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=messages,\n",
    "        max_tokens=500,\n",
    "        temperature=0.0,\n",
    "        timeout=30  # 30 secondes de timeout\n",
    "    )\n",
    "    response.choices[0].message.content\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de l'appel API: {e}\")\n",
    "    \"Erreur lors de la g√©n√©ration de la r√©ponse\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "but3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
