{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "313ed1c5-8452-4b3b-ae5a-28e726ff077f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.display import HTML\n",
    "\n",
    "css_adjustment = \"\"\"\n",
    "<style>\n",
    "/* Cacher les prompts de commande */\n",
    "div.prompt {display: none !important;}\n",
    "\n",
    "/* Ajuster la largeur max des cellules de code et de markdown */\n",
    ".jp-RenderedMarkdown, .jp-InputArea {\n",
    "    max-width: 1500px !important; /* Assurez-vous que cela correspond à la largeur du Markdown si nécessaire */\n",
    "}\n",
    "\n",
    "/* Modifier la taille de la police dans les cellules de code */\n",
    ".jp-Notebook .jp-InputArea .input_area {\n",
    "    font-size: 40px !important; /* Ajustez selon vos besoins */\n",
    "}\n",
    "\n",
    "/* Si vous souhaitez ajuster la taille de la police à l'intérieur des blocs de code eux-mêmes */\n",
    ".jp-Notebook .jp-InputArea .input_area pre, \n",
    ".jp-Notebook .jp-InputArea .input_area code {\n",
    "    font-size: 40px !important; /* Ajustez selon vos besoins */\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "HTML(css_adjustment)\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4226d077-353f-4227-8687-4df42cf0b3a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Programmer avec des LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d98f9-cfeb-4dd1-9a3a-a10138b8b941",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Objectif de ces Travaux Pratiques\n",
    "\n",
    "- Courte introduction à \"comment programmer avec des LLM\"\n",
    "- Uniquement de la pratique (pas le temps pour la théorie)\n",
    "- Evaluation : \n",
    "  - Projet de votre choix de programmation par des LLM\n",
    "  - Note = fonction(quantite_de_travail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d7787-7336-46ec-a985-07dfbb294955",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9bfa5-585a-497e-a580-0a682197168b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### LangChain\n",
    "- Framework Python d'intégration de modèles LLM\n",
    "  - Model agnostic \n",
    "  - Chaînes et Agents\n",
    "  - Automatisation des prompts\n",
    "  - Gestion des vectorstores, embeddings...\n",
    "\n",
    "- Cas d’usage : chatbots (RAG), programmes embarquant des LLM, LangGraphs, multi-agents..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3019664-f310-4123-bb6a-e71f6e33864e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Mistral AI\n",
    "\n",
    "1. Créez un compte sur La Plateforme de Mistral AI. https://mistral.ai/\n",
    "\n",
    "   (\"Try the API\" -> \"S'inscrire\")\n",
    "\n",
    "   (Abonnement : Gratuit / expérimental)\n",
    "   \n",
    "3. Générez une clé API personnelle.\n",
    "\n",
    "   (API -> Clés API -> Créer une nouvelle clé -> ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce133e4-5ed4-4e7c-924e-2951af66c2cf",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Environnement virtuel\n",
    "\n",
    "Il y aura potentiellement des bibliothèques à installer, ce qui peut se faire dans un environnement virtuel python.\n",
    "\n",
    "  $ python -m venv mon\\_env\n",
    "  \n",
    "  $ source mon\\_env/bin/activate\n",
    "  \n",
    "  (mon\\_env) $ pip install nom\\_bibliotheque\n",
    "\n",
    "(Et pour désactiver : deactivate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2addb622-d7a9-4847-bef2-b573f2ee8e27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Références\n",
    "- LangChain\n",
    "  - https://python.langchain.com/docs/introduction/\n",
    "  - https://www.youtube.com/@LangChain\n",
    "- Mistral AI\n",
    "  - https://docs.mistral.ai/\n",
    "- OpenAI\n",
    "  - https://platform.openai.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3ec0b-529d-4b3a-8fb8-aa1bdca2b1ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Introduction à LangChain / Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171308f5-cbab-4a41-b1eb-daa5e85a3bc1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Premières invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e601d0-9ba2-44df-b782-a139ef988eca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capitale de l'Albanie est Tirana.\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialiser le modèle\n",
    "llm = ChatMistralAI(model=\"mistral-medium-2508\", \n",
    "                    temperature=0)\n",
    "                    #api_key=\"....\")\n",
    "\n",
    "# Créer le message utilisateur\n",
    "message = HumanMessage(content=\"Quelle est la capitale de l'Albanie ?\")\n",
    "\n",
    "# Obtenir la réponse\n",
    "response = llm.invoke([message])\n",
    "\n",
    "# Afficher la réponse\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84dbc14e-6d82-4efa-a6e1-1ed98ba17048",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capitale de l’Albanie est Tirana.\n"
     ]
    }
   ],
   "source": [
    "# Version OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1\")\n",
    "response = llm.invoke(\"Quelle est la capitale de l'Albanie ?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b4bcfe1-8c3e-40b6-97f7-a42dc07baac6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourquoi les pompiers ne jouent-ils jamais à cache-cache ?\n",
      "\n",
      "Parce qu'ils trouvent toujours la cachette en feu !\n",
      "----------\n",
      "Pourquoi les policiers n'aiment-ils pas les puzzles ?\n",
      "\n",
      "Parce qu'ils préfèrent les casse-têtes !\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# 1) Initialisation du modèle Mistral\n",
    "model = ChatMistralAI(model=\"mistral-large-latest\", \n",
    "                      temperature=0)\n",
    "\n",
    "# 2) Construction du prompt + parser\n",
    "prompt = ChatPromptTemplate.from_template(\"Fais-moi une blague sur le sujet : {sujet}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 3) Chaînage (LangChain Expression Language)\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# 4) Exécution\n",
    "for sujet in ['pompier', 'police']:\n",
    "    print(chain.invoke({\"sujet\": sujet}))\n",
    "    print('-'*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a07cb0e4-94b6-4e04-8959-40435196d1ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Vous êtes un rédacteur de documentation technique de classe mondiale.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "chain = prompt | llm\n",
    "result = chain.invoke({\"input\": \"Qu'est-ce que le modèle mistral-large-latest ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dfaa652-299d-4d7a-a480-4e17a079cf19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 38, 'output_tokens': 701, 'total_tokens': 739}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "120dac24-febc-4714-ad88-8033d5bd9d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056156"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "38/1000000*2+701*8/100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb7b1825-15de-4b92-8d7e-7ff0e670e212",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral-large-latest est un modèle de langage développé par Mistral AI, une société de pointe dans le domaine de l'intelligence artificielle. Ce modèle est conçu pour comprendre et générer du texte en réponse à une variété d'entrées, ce qui le rend utile pour diverses applications telles que la rédaction de documentation technique, la traduction, la génération de contenu, et bien plus encore.\n",
      "\n",
      "### Caractéristiques principales de Mistral-large-latest :\n",
      "\n",
      "1. **Taille et Capacité** :\n",
      "   - Le modèle est de grande taille, ce qui lui permet de comprendre et de générer du texte de manière très sophistiquée.\n",
      "   - Il est entraîné sur une vaste quantité de données textuelles, ce qui lui confère une large compréhension du langage naturel.\n",
      "\n",
      "2. **Polyvalence** :\n",
      "   - Mistral-large-latest peut être utilisé pour une variété de tâches, y compris la rédaction de documents techniques, la réponse à des questions, la génération de contenu créatif, et même la traduction entre différentes langues.\n",
      "\n",
      "3. **Précision et Cohérence** :\n",
      "   - Le modèle est conçu pour produire des réponses précises et cohérentes, ce qui est crucial pour des applications professionnelles et techniques.\n",
      "\n",
      "4. **Mise à Jour Régulière** :\n",
      "   - Le terme \"latest\" indique que le modèle est régulièrement mis à jour pour inclure les dernières améliorations et corrections, assurant ainsi des performances optimales.\n",
      "\n",
      "### Utilisations Potentielles :\n",
      "\n",
      "1. **Rédaction de Documentation Technique** :\n",
      "   - Mistral-large-latest peut aider à rédiger des manuels, des guides d'utilisation, des spécifications techniques, et d'autres documents nécessitant une grande précision et clarté.\n",
      "\n",
      "2. **Support Client** :\n",
      "   - Il peut être intégré dans des systèmes de support client pour fournir des réponses automatisées aux questions fréquentes des utilisateurs.\n",
      "\n",
      "3. **Génération de Contenu** :\n",
      "   - Le modèle peut être utilisé pour générer du contenu créatif, comme des articles de blog, des scripts, ou même des histoires.\n",
      "\n",
      "4. **Traduction** :\n",
      "   - Il peut aider à traduire des textes entre différentes langues, facilitant ainsi la communication internationale.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Mistral-large-latest est un outil puissant et polyvalent pour toute personne ou organisation ayant besoin de solutions avancées en traitement du langage naturel. Sa capacité à comprendre et générer du texte de manière précise et cohérente en fait un choix idéal pour une variété d'applications professionnelles et techniques.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0079cb0a-8f86-462f-9ed4-92a886d3bcbd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Les sorties structurées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ffa5cd-8e50-4ea5-83a0-32acc0114322",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Cas d'un booléen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd16a7ce-de15-4342-b612-0bec866e82af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noël est en hiver\n",
      "Réponse : True (type : <class 'bool'>)\n",
      "\n",
      "Il pleut quand il pleut pas\n",
      "Réponse : False (type : <class 'bool'>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    answer: bool\n",
    "\n",
    "prompt_answer = [\n",
    "    (\"system\", \"Tu es un assistant chargé de répondre un booléen (True ou False) à la question d'un utilisateur.\"),\n",
    "    ('human', \"{question}\")\n",
    "]\n",
    "\n",
    "prompt_answer_template = ChatPromptTemplate.from_messages(prompt_answer)\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "chain = prompt_answer_template | llm.with_structured_output(schema=Answer)\n",
    "\n",
    "def repond(question):\n",
    "    return chain.invoke({\"question\": question}).answer\n",
    "    \n",
    "for question in [\"Noël est en hiver\", \"Il pleut quand il pleut pas\"]:\n",
    "    print(question)\n",
    "    reponse = repond(question)\n",
    "    print(f\"Réponse : {reponse} (type : {type(reponse)})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57646f4-a101-441d-a491-cde12e06a8f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Cas d'une classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea1dad8b-78fd-4656-ac3e-e4865ef70c4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peux-tu m'en dire plus\n",
      "action=\"Fournir plus d'éléments à la question précédente\"\n",
      "\n",
      "Que sont les PPV ?\n",
      "action='Répondre à une nouvelle question'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "tasks = [\"Répondre à une nouvelle question\", \"Fournir plus d'éléments à la question précédente\"]\n",
    "\n",
    "class NextTask(BaseModel):\n",
    "    \"\"\"Utilise toujours cet outil pour structurer ta réponse to the user.\"\"\"\n",
    "    action: str = Field(..., \n",
    "                        enum=tasks,\n",
    "                        description=\"La prochaine action à mener\")\n",
    "\n",
    "prompt_message = [\n",
    "    (\"system\", \"Tu es un assistant chargé de classifier la demande d'un utilisateur parmi une \"\n",
    "               \"liste réduite d'actions à mener en tant que chatbot. Tu dois déterminer la \"\n",
    "               \"prochaine action à mener.\"),\n",
    "    ('human', \"{text}\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(prompt_message)\n",
    "llm = ChatMistralAI(model='mistral-large-latest', temperature=0)\n",
    "chain = prompt | llm.with_structured_output(schema=NextTask)\n",
    "\n",
    "for text in [\"Peux-tu m'en dire plus\", \"Que sont les PPV ?\"]:\n",
    "    print(text)\n",
    "    print(chain.invoke({\"text\": text}))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3bd17-0379-467b-9be9-06544afcaf0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Cas d'un entier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8b502d-8200-4ca8-b0c0-3969c86dab19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message : Bonjour, pourrais-tu m'aider s'il te plaît ?\n",
      "note_ton=4\n",
      "\n",
      "Message : J'ai besoin de ça immédiatement.\n",
      "note_ton=1\n",
      "\n",
      "Message : Merci beaucoup pour ton aide précieuse !\n",
      "note_ton=5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "class TonMessage(BaseModel):\n",
    "    \"\"\"Évaluation du ton du message de l'utilisateur.\"\"\"\n",
    "    note_ton: int = Field(\n",
    "        ..., \n",
    "        ge=1,\n",
    "        le=5,\n",
    "        description=\"Note attribuée au ton du message : 1 pour neutre, 5 pour très aimable\"\n",
    "    )\n",
    "\n",
    "prompt_message = [\n",
    "    (\"system\", \"Tu es un assistant chargé d'évaluer le ton d'un message donné par l'utilisateur. \"\n",
    "               \"Attribue une note de 1 à 5 au ton du message, où 1 signifie neutre et 5 signifie très aimable.\"),\n",
    "    ('human', \"{text}\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(prompt_message)\n",
    "llm = ChatMistralAI(model='mistral-large-latest', temperature=0)\n",
    "chain = prompt | llm.with_structured_output(schema=TonMessage)\n",
    "\n",
    "messages = [\n",
    "    \"Bonjour, pourrais-tu m'aider s'il te plaît ?\",\n",
    "    \"J'ai besoin de ça immédiatement.\",\n",
    "    \"Merci beaucoup pour ton aide précieuse !\"\n",
    "]\n",
    "\n",
    "for text in messages:\n",
    "    print(f\"Message : {text}\")\n",
    "    print(chain.invoke({\"text\": text}))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d36f5d-0be7-47cf-8b7d-88e0e28bae71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Pourquoi et comment forcer la sortie du LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76a94afe-6764-46bb-9aaf-7b4583eb1f5c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The translation of \"Quelle est la capitale de l\\'Albanie ?\" in English is:\\n\\n**\"What is the capital of Albania?\"**', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 26, 'total_tokens': 56, 'completion_tokens': 30}, 'model_name': 'mistral-medium-latest', 'model': 'mistral-medium-latest', 'finish_reason': 'stop'}, id='run--c4b5c2c7-5a43-4f1d-a93e-0ed2170a7818-0', usage_metadata={'input_tokens': 26, 'output_tokens': 30, 'total_tokens': 56})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-medium-latest\")\n",
    "message = HumanMessage(content=\"Peux-tu me traduire ce qui suit, en anglais ?\\n\\n Quelle est la capitale de l'Albanie ?\")\n",
    "llm.invoke([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e8c85e9-e6fa-4812-8718-50c1ee92a5cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Albania\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Translation(BaseModel):\n",
    "    original_text: str = Field(..., description=\"The original text before translation in another language\")\n",
    "    original_language: str = Field(..., description=\"The original language before translation\")\n",
    "    translated_text: str = Field(..., description=\"The final text after translation in another language\")\n",
    "    translated_language: str = Field(..., description=\"The language into which the translation must be done\")\n",
    "    \n",
    "def traduit(texte, langue_source=\"français\", langue_cible=\"anglais\"):\n",
    "    llm = ChatMistralAI(model_name=\"mistral-medium-latest\")\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"Je souhaite que tu traduises le texte suivant du {langue_source} vers le {langue_cible}. Ta traduction doit être précise, fluide et naturelle, et préserver parfaitement le sens original. \n",
    "    Retourne-moi la réponse sous forme d'objet JSON avec les champs :\n",
    "      - original_text : le texte original\n",
    "      - original_language : la langue du texte original\n",
    "      - translated_text : la traduction du texte\n",
    "      - translated_language : la langue de la traduction\n",
    "\n",
    "    Voici le texte à traduire :\n",
    "    ----\n",
    "    {texte}\"\"\")\n",
    "    output_parser = StrOutputParser()\n",
    "    extract_translation = RunnableLambda(lambda translation: translation.translated_text)\n",
    "    chain0 = prompt | llm.with_structured_output(Translation) | extract_translation\n",
    "    return chain0.invoke({\"langue_source\": langue_source,\n",
    "                            \"langue_cible\": \"anglais\",\n",
    "                            \"texte\": texte      \n",
    "                            })\n",
    "\n",
    "print(traduit(\"Quelle est la capitale de l'Albanie\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2032b-c358-4aec-b673-43b4e1afed83",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Des sorties structurées aux prémices d'un raisonnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "462e27ab-59d3-4bc3-aab0-37c73e620d48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "class Etape(BaseModel):\n",
    "    explication: str\n",
    "    sortie: str\n",
    "\n",
    "class MathReponse(BaseModel):\n",
    "    etapes: list[Etape]\n",
    "    reponse_finale: str\n",
    "\n",
    "prompt_answer = [\n",
    "    (\"system\", \"Tu es un professeur de mathématiques très pédagogue.\"),\n",
    "    ('human', \"{exercice}\")\n",
    "]\n",
    "\n",
    "prompt_answer_template = ChatPromptTemplate.from_messages(prompt_answer)\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "chain = prompt_answer_template | llm.with_structured_output(schema=MathReponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83f49b83-7fe3-475a-818b-43c232ddaea2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Soustrayons 31 des deux côtés de l'équation pour isoler le terme en x.\n",
      "  Le résultat est alors : 8x + 31 - 31 = 2 - 31\n",
      "8x = -29\n",
      "- Divisons les deux côtés par 8 pour résoudre x.\n",
      "  Le résultat est alors : 8x / 8 = -29 / 8\n",
      "x = -29/8 ou x = -3,625\n",
      "Au final, on trouve : x = -3,625\n"
     ]
    }
   ],
   "source": [
    "explications = chain.invoke({\"exercice\": \"Résous  8x + 31 = 2\"})\n",
    "for etape in explications.etapes:\n",
    "    print(f\"- {etape.explication}\")\n",
    "    print(f\"  Le résultat est alors : {etape.sortie}\")\n",
    "\n",
    "print(f\"Au final, on trouve : {explications.reponse_finale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7905089f-3155-4381-91e5-17a2ba2aac9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918186c0-294c-4a8f-b0f8-65afd6f06a42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/rag.png\" alt=\"RAG\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589d138-b105-4567-aed0-ab5fb462ca51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Embeddings et semantique\n",
    "\n",
    "But : encoder un texte sous la forme d'un vecteur, de sorte que deux textes voisins sémantiquement soient encodés en deux vecteurs proches.\n",
    "\n",
    "![Texte alternatif](images/vectors-and-semantics.png \"Vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee12db-412b-43bb-b0bd-9c680335617a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Embeddings : Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6a1c67-5364-4d43-bdb9-cf1f6bf7dc6d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "![Texte alternatif](images/Bag-of-words.png \"BoW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "188424dd-b206-46a9-bcfb-3c5c2277cebb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary : ['and' 'demo' 'demonstration' 'document' 'finally' 'first' 'here' 'is'\n",
      " 'second' 'text' 'the' 'third' 'this']\n",
      "BoW vector:\n",
      " [[0 0 1 1 0 1 0 0 0 1 0 0 0]\n",
      " [1 1 0 1 0 0 1 0 1 1 0 0 0]\n",
      " [1 0 0 1 1 0 0 1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'Demonstration text, first document',\n",
    "    \"Demo text, and here's a second document.\",\n",
    "    'And finally, this is the third document.'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary :\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW vector:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11281bfe-e00f-49e0-8ae1-108605a139cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Embeddings par transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34d487f9-12a6-47ff-8609-0fedc2cb7064",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is an example sentence.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEach sentence is converted into a fixed-sized vector.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Entraîné sur des données essentiellement anglophones.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Conçu pour être léger et rapide, tout en gardant une bonne précision pour l’anglais.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence-transformers/all-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(sentences)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence, embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sentences, embeddings):\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:367\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_hpu_graph_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts:\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 915 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = [\"This is an example sentence.\", \"Each sentence is converted into a fixed-sized vector.\"]\n",
    "\n",
    "# Entraîné sur des données essentiellement anglophones.\n",
    "# Conçu pour être léger et rapide, tout en gardant une bonne précision pour l’anglais.\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(f'\"{sentence}\" -> {embedding[:3]}...')\n",
    "\n",
    "print(f\"Embedding size: {len(embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb7a8bf-fc00-4b03-a5bf-32890b7f45ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#Entraîné avec un objectif de détection de paraphrases sur un corpus multilingue.\n",
    "#Performances équilibrées pour la similarité sémantique, la recherche d’information et la classification zero-shot en plusieurs langues.\n",
    "\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "print(model.encode([\"Texte à encoder\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2bed45-6703-4228-8c0e-0815eb36f7b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Similarité sémantique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "caab2ac4-816b-4b6e-965c-656338c708e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.37034684)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_A = np.linalg.norm(A)\n",
    "    norm_B = np.linalg.norm(B)\n",
    "    return dot_product / (norm_A * norm_B)\n",
    "\n",
    "cosine_similarity(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7e39c-c1af-4389-a8d1-5266facf70e4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Embeddings OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04fc4801-ba9c-49ce-a405-cc15b8480213",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5604925298797377)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "def embed(text, model=\"text-embedding-3-large\", dimensions=3072): #3072: dimension maximale\n",
    "    return openai.embeddings.create(input = [text], model=model, dimensions=dimensions).data[0].embedding\n",
    "\n",
    "vector1 = embed(\"What is Mycobacterium kansasii ?\")\n",
    "vector2 = embed(\"To sum up, we have presented a case of Mycobacterium kansasii monoarthritis of the elbow complicated with unusual clinical and radiological findings. A combination of synovectomy and multidrug antimycobacterial treatment yielded a favorable clinical course without recurrence of arthritis after 10 months of follow-up. This case emphasizes the need to consider this rare infection in the differential diagnosis of intra-articular soft tissue tumor-like lesions of the elbow even in immunocompetent patients.\")\n",
    "cosine_similarity(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f18a96-816d-4c9b-a34b-e6d50220b36f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### RAG : principe de base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283ac8c-eb44-49d9-afe1-a83df312fc48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/rag2.png\" alt=\"RAG\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d363528a-e2de-42b6-b552-2d8d5642df6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mycobacterium kansasii\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(model_name=\"mistral-large-latest\")\n",
    "\n",
    "query = \"What is Mycobacterium kansasii ?\"\n",
    "context = \"To sum up, we have presented a case of Mycobacterium kansasii monoarthritis of the elbow complicated with unusual clinical and radiological findings. A combination of synovectomy and multidrug antimycobacterial treatment yielded a favorable clinical course without recurrence of arthritis after 10 months of follow-up. This case emphasizes the need to consider this rare infection in the differential diagnosis of intra-articular soft tissue tumor-like lesions of the elbow even in immunocompetent patients.\"\n",
    "\n",
    "text = f\"\"\"You are an expert in the Mycobacterium field. \n",
    "Answer to the following question by only using the context below.\n",
    "\n",
    "question: {query}\n",
    "\n",
    "context : {context}\"\"\"\n",
    "\n",
    "response = llm.invoke(text)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d552db-cd59-49c8-9059-8ef5816aebc7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Implémentation d'un vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0dab196-c19e-4681-8851-eaacbdf7e7e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install langchain-community langchain-openai faiss-cpu\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "loader = PyPDFLoader(\"images/Guyeux_2024.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "faiss_index = FAISS.from_documents(pages, embeddings)\n",
    "docs = faiss_index.similarity_search(\"Is there a lineage 10 in M.tuberculosis?\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39a2555d-7d51-43fc-b060-a9890b8aefe6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 3: M. africanum Lineage 10, Central Africa Conclusions Through the extensive mining\n",
      "of WGS and genotyp- ing databases, we newly identified a thus far rare M.\n",
      "tuberculosis complex lineage, L10 (proposed), pres- ent in central Africa. The\n",
      "lineage is characterized by a new region of deletion, IS6110 insertions, and 243\n",
      "SNPs, including gyrA G7901T, recN C1920096T, and dnaG C2621730T. L10 represents\n",
      "a sister clade to L6, found mainly in western Africa, and L9, specifically in\n",
      "eastern Africa, and [...]\n",
      "\n",
      "Page 0: nity of Lille, Lille, France (P. Supply, C. Gaudin); London School of Hygiene\n",
      "and Tropical Medicine, London, UK (J.E. Phelan, T.G. Clark, L. Rigouts, B. de\n",
      "Jong); Université Paris-Saclay, Saint- Aubin, France (C. Sola); Université Paris\n",
      "Cité, Paris (C. Sola) DOI: https://doi.org/10.3201/eid3003.231466 Analysis of\n",
      "genome sequencing data from >100,000 genomes of Mycobacterium tuberculosis\n",
      "complex using TB-Annotator software revealed a previously unknown lineage,\n",
      "proposed name L10, in central [...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textwrap import shorten, fill\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"Page {doc.metadata[\"page\"]}: {fill(shorten(doc.page_content, 500), 80)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b272e73a-28b8-485a-bcce-948f1713cebd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Version OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a429dce-16c1-4744-a246-8c7c63fe4398",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install langchain-community langchain-openai faiss-cpu\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "loader = PyPDFLoader(\"images/Guyeux_2024.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())\n",
    "docs = faiss_index.similarity_search(\"Is there a lineage 10 in M.tuberculosis?\", k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32045cf8-3161-48da-9f0c-bdb3617f2aba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1efcc3d-7d8e-457c-a1a0-6a1786634749",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = '''Vous pouvez partager un article en cliquant sur les icônes de partage en haut à droite de celui-ci. \n",
    "La reproduction totale ou partielle d’un article, sans l’autorisation écrite et préalable du Monde, est strictement interdite. \n",
    "Pour plus d’informations, consultez nos conditions générales de vente. \n",
    "\n",
    "Comme la finance, la politique est parfois affaire d’opportunités. Aux Etats-Unis, l’opposition démocrate à Donald Trump a en tout cas trouvé un nouvel angle d’attaque après l’annonce par le président américain d’une pause dans sa guerre commerciale : elle le soupçonne d’avoir manipulé les marchés boursiers et d’avoir ainsi favorisé des délits d’initié.\n",
    "Lire aussi | Article réservé à nos abonnés Droits de douane : les Bourses rechutent, l’inquiétude s’étend aux emprunts d’Etat\n",
    "\n",
    "Le sénateur Adam Schiff a écrit, jeudi 10 avril, au directeur par intérim du Bureau pour l’éthique gouvernementale (Office of Government Ethics, OGE), une agence fédérale indépendante, et à Susan Wiles, la cheffe de cabinet de la Maison Blanche, pour leur demander d’ouvrir une enquête « urgente » afin de déterminer si « le président Trump, sa famille ou d’autres membres de [son] administration » ont commis la veille des délits d’initié en profitant d’informations confidentielles sur le revirement de sa politique commerciale.\n",
    "\n",
    "'''\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=200,\n",
    "    keep_separator=False,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \"]\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([text])\n",
    "\n",
    "for k in texts[:7]:\n",
    "    print(k.page_content)\n",
    "    print(\"=\"*20+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed54b26-02a2-4d34-b410-b2aed554385a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Loaders (LangChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c28c1c-6e98-4aa6-8a37-e5ccd72331d6",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=YcIbZGTRMjI\", \n",
    "    language=['fr'],\n",
    "    add_video_info=False\n",
    ")\n",
    "\n",
    "print(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02172e7d-480e-4013-af70-b7e12e46cc2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Vectorstores\n",
    "\n",
    "Nombreux et multiples...\n",
    " - FAISS, Chroma : faciles à maîtriser, déployer...\n",
    " - Milvus : multi-embeddings, BM25, filtrage par colonne...\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/Milvus.png\" alt=\"RAG\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ff316-e795-4cb6-9ac6-4edd4437de66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Les agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f8c84e-b005-4dd3-a906-a49202986f80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Un agent est un système autonome alimenté par un modèle de langage (comme GPT-4) qui prend des décisions sur les actions à entreprendre en fonction des données d'entrée et des instructions programmées.\n",
    "\n",
    "Fonction :\n",
    "- Prise de décision : L'agent analyse les données d'entrée et utilise des algorithmes et des modèles pour décider quelle action entreprendre.\n",
    "- Exécution d'actions : L'agent peut effectuer diverses actions comme répondre à une question, rechercher des informations, ou interagir avec d'autres systèmes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cce33f-8d83-4d9a-9af5-46e06c5526f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Un \"agent\", c'est un LLM avec des \"outils\" :\n",
    " - recherche sur internet,\n",
    " - calculatrice,\n",
    " - interrogation de pdf (RAG),\n",
    " - outil fait maison\n",
    " - ...\n",
    "\n",
    "Le mieux est de faire des agents spécialisés, et de les orchestrer ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0fab8a-ede8-4811-bb30-4cfe0b73bafd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Des outils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8355b7-335d-4132-9ee7-598a13f1acbb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea0ea3e-c8fb-4da4-bb41-0ca8a623ff15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install --upgrade --quiet  wikipedia\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "wikipedia.run(\"Alan Turing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f9250e-68a5-43f5-bacd-01ee95a6ea2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Tavili (recherche internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd80876-c248-4d29-a73e-b361472ba727",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install -qU langchain-tavily\n",
    "# Pour une clé d'API : https://www.tavily.com/\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "import os\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "search = TavilySearchResults(max_results=2)\n",
    "search_results = search.invoke(\"Quel est le temps à Belfort ?\")\n",
    "print(search_results[0]['content'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e996e910-9b21-41e6-ac8f-81dff60aa4f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Agents LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6996f99-d3bd-4903-879c-c16338141e43",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Exécuteur d'agent (Agent Executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75632036-6402-4c73-a35f-64989fce0a04",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "L'exécuteur d'agent est un composant ou un système qui orchestre et exécute les actions déterminées par l'agent.\n",
    "\n",
    "Fonction :\n",
    " - Gestion de l'exécution : Il reçoit les décisions de l'agent, exécute les actions correspondantes et gère la transition entre différentes étapes de l'exécution.\n",
    " - Traitement des résultats : Il collecte les résultats des actions exécutées et les transmet à l'agent pour de nouvelles décisions ou à l'utilisateur final.\n",
    "\n",
    "Exemple : Dans un système de recommandation, l'exécuteur d'agent pourrait orchestrer l'appel à différentes API pour recueillir les informations nécessaires (comme les préférences de l'utilisateur et les données sur les produits) et les combiner pour générer une recommandation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423be8f8-8f9b-48c6-a031-698ac8d13ef8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Avec outil Tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c599b022-cda1-41e3-944f-ac1718152a89",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "search = TavilySearchResults(max_results=2)\n",
    "tools = [search]\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "\n",
    "prompt = hub.pull(\"amalnuaimi/react-mistral\")\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        'input': \"Dois-je prendre un parapluie, sachant que je me rends aujourd'hui et demain à Belfort ?\",\n",
    "        'chat_history': []\n",
    "    })\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0593da3-5ab3-475e-adb9-f077fc50ab03",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://smith.langchain.com/hub\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a201966-1ebe-48a7-9464-b086c0361740",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Version OpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "search = TavilySearchResults(max_results=2)\n",
    "tools = [search]\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1\")\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        'input': \"Dois-je prendre un parapluie, sachant que je me rends aujourd'hui et demain à Belfort ?\"\n",
    "    })\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d37196e-8e45-4dcc-b3f9-5fb91fbc52c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Avec outil arXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c6254-6f0e-4bfd-bc77-46e078c6a6da",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install arxiv\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "#llm = ChatOpenAI(temperature=0.0)\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "tools = load_tools([\"arxiv\"])\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"Résume l'article 1605.08386 en français\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b151aecf-48ed-4042-9363-b812319db351",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f045ba-f698-474b-b3eb-6186b55de946",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tools[0].name)\n",
    "print(tools[0].description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8178e85-cb4b-4612-aa5b-34e0817f79dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Avec Python REPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5469e-5034-4a73-a1e2-f19b6c23acdc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "tools = [PythonREPLTool()]\n",
    "\n",
    "instructions = \"\"\"You are an agent designed to write and execute python code to answer questions.\n",
    "You have access to a python REPL, which you can use to execute python code.\n",
    "If you get an error, debug your code and try again.\n",
    "Only use the output of your code to answer the question. \n",
    "You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "\"\"\"\n",
    "\n",
    "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)\n",
    "agent = create_openai_functions_agent(ChatOpenAI(temperature=0), tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8950531-534f-402a-b634-a474c6029525",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"Quel est le millième nombre de Fibonacci ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b658b229-7e6f-46f7-95ba-9e09cff78cd8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2b8b2-4ebe-4f1a-aec2-0a941ae3f4f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Plusieurs outils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb01ef0-a7e7-4d50-9d04-35f46503de2d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, create_react_agent\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "tools = load_tools([\"llm-math\", \"wikipedia\"], llm=llm)\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, \n",
    "                               tools=tools, \n",
    "                               handle_parsing_errors=True, \n",
    "                               verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281cf6c1-bb0c-4b45-847d-f404800ae22a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor.invoke({'input': \"Qu'est-ce que 25% de 300?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93bab48-86d3-43ba-b43e-503d69013b0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Ses propres outils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6700d8-15cb-4946-83af-a945ed28a613",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"Multiplie deux entiers.\"\"\"\n",
    "    return first_int * second_int\n",
    "\n",
    "print(multiply.name)\n",
    "print(multiply.description)\n",
    "print(multiply.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5432395-1131-407c-82c3-069904e9fba2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(first_int: int, second_int: int) -> int:\n",
    "    \"Ajoute deux entiers.\"\n",
    "    return first_int + second_int\n",
    "\n",
    "@tool\n",
    "def exponentiate(base: int, exponent: int) -> int:\n",
    "    \"Calcule la puissance d'un entier donné.\"\n",
    "    return base**exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28db36-9235-4256-906f-3b9645685cb2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1\")\n",
    "\n",
    "tools = [multiply, add, exponentiate]\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Porter 3 à la puissance 5 et multiplier le résultat par la somme de douze et de trois, puis élever le tout au carré.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57718046-2846-4ec5-8b77-7098ac55fe81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57b020-d22f-48b1-9bae-110d80e1e376",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Assemblage d'agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712aefa-4e26-4b1e-9ab8-e5d1ba99f8a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![Texte alternatif](images/promptulate.png \"Promptulate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a202d00c-765d-420d-bae9-17fc5f3a6c6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## L'audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af0ac02-c87a-44f0-9b63-ae0560619bb3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896cded-5641-402a-b7c8-2e9a62020ad9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "response = OpenAI().audio.speech.create(\n",
    "  model=\"tts-1-hd\",\n",
    "  voice=\"onyx\", # alloy, onyx, fable, echo\n",
    "  input=\"Coucou, comment allez-vous ?\"\n",
    ")\n",
    "response.with_streaming_response.method('mon_audio.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf0222f-716e-4d3f-9f07-957ab65bd491",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e6de8-2eeb-44e2-88e3-28be8b4f579c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OpenAI().audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=open(\"mon_audio.mp3\", \"rb\"),\n",
    "  language=\"fr\"\n",
    ").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee9c72-f087-4909-b072-43f6f6811c9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydub import AudioSegment\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "song = AudioSegment.from_mp3(\"mon_fichier.mp3\")\n",
    "transcription = ''\n",
    "for pas in range(0, 120, 20):\n",
    "    debut, fin = pas * 60 * 1000, (pas+20) * 60 * 1000\n",
    "    extrait = song[debut:fin]\n",
    "    if len(extrait) > 100:\n",
    "        extrait.export(f\"extrait_{pas}.mp3\", format=\"mp3\")\n",
    "        audio_file = open(f\"extrait_{pas}.mp3\", \"rb\")\n",
    "        transcription += client.audio.transcriptions.create(\n",
    "          model=\"whisper-1\", \n",
    "          file=audio_file\n",
    "        ).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e0f2b0-c61a-4411-86c5-448410a9dfc4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0ca3a26-a54d-4b78-a330-3ece1415e665",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image_to_data_url(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Lit l'image et renvoie une data URL prête à être insérée dans le payload.\n",
    "    \"\"\"\n",
    "    # Lecture du fichier image\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        image_bytes = img_file.read()  # Lecture binaire du contenu\n",
    "    # Encodage base64\n",
    "    b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "    # Détection du MIME type selon l'extension\n",
    "    ext = os.path.splitext(image_path)[1].lower().lstrip(\".\")\n",
    "    mime = f\"image/{ext if ext != 'jpg' else 'jpeg'}\"\n",
    "    return f\"data:{mime};base64,{b64}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93ae8a23-f595-4254-8b97-f6798878394d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[0;32m----> 5\u001b[0m data_url \u001b[38;5;241m=\u001b[39m \u001b[43mencode_image_to_data_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmon_image.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      8\u001b[0m     {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     }\n\u001b[1;32m     15\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[33], line 13\u001b[0m, in \u001b[0;36mencode_image_to_data_url\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m b64 \u001b[38;5;241m=\u001b[39m base64\u001b[38;5;241m.\u001b[39mb64encode(image_bytes)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Détection du MIME type selon l'extension\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m ext \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(image_path)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m mime \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mext\u001b[38;5;250m \u001b[39m\u001b[38;5;241m!=\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpeg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;base64,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb64\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "data_url = encode_image_to_data_url(\"mon_image.png\")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Que vois-tu sur cette image ?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998bf937-6eeb-41fb-94f3-6bb4621264bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=messages,\n",
    "    max_tokens=500,\n",
    "    temperature=0.0\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
