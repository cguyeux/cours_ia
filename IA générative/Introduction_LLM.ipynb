{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "313ed1c5-8452-4b3b-ae5a-28e726ff077f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.display import HTML\n",
    "\n",
    "css_adjustment = \"\"\"\n",
    "<style>\n",
    "/* Cacher les prompts de commande */\n",
    "div.prompt {display: none !important;}\n",
    "\n",
    "/* Ajuster la largeur max des cellules de code et de markdown */\n",
    ".jp-RenderedMarkdown, .jp-InputArea {\n",
    "    max-width: 1500px !important; /* Assurez-vous que cela correspond Ã  la largeur du Markdown si nÃ©cessaire */\n",
    "}\n",
    "\n",
    "/* Modifier la taille de la police dans les cellules de code */\n",
    ".jp-Notebook .jp-InputArea .input_area {\n",
    "    font-size: 40px !important; /* Ajustez selon vos besoins */\n",
    "}\n",
    "\n",
    "/* Si vous souhaitez ajuster la taille de la police Ã  l'intÃ©rieur des blocs de code eux-mÃªmes */\n",
    ".jp-Notebook .jp-InputArea .input_area pre, \n",
    ".jp-Notebook .jp-InputArea .input_area code {\n",
    "    font-size: 40px !important; /* Ajustez selon vos besoins */\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "HTML(css_adjustment)\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4226d077-353f-4227-8687-4df42cf0b3a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Programmer avec des LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d98f9-cfeb-4dd1-9a3a-a10138b8b941",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Objectif de ces Travaux Pratiques\n",
    "\n",
    "- Courte introduction Ã  \"comment programmer avec des LLM\"\n",
    "- Uniquement de la pratique (pas le temps pour la thÃ©orie)\n",
    "- Evaluation : \n",
    "  - Projet de votre choix de programmation par des LLM\n",
    "  - Note = fonction(quantite_de_travail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d7787-7336-46ec-a985-07dfbb294955",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9bfa5-585a-497e-a580-0a682197168b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### LangChain\n",
    "- Framework Python d'intÃ©gration de modÃ¨les LLM\n",
    "  - Model agnostic \n",
    "  - ChaÃ®nes et Agents\n",
    "  - Automatisation des prompts\n",
    "  - Gestion des vectorstores, embeddings...\n",
    "\n",
    "- Cas dâ€™usage : chatbots (RAG), programmes embarquant des LLM, LangGraphs, multi-agents..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3019664-f310-4123-bb6a-e71f6e33864e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Mistral AI\n",
    "\n",
    "1. CrÃ©ez un compte sur La Plateforme de Mistral AI. https://mistral.ai/\n",
    "\n",
    "   (\"Try the API\" -> \"S'inscrire\")\n",
    "\n",
    "   (Abonnement : Gratuit / expÃ©rimental)\n",
    "   \n",
    "3. GÃ©nÃ©rez une clÃ© API personnelle.\n",
    "\n",
    "   (API -> ClÃ©s API -> CrÃ©er une nouvelle clÃ© -> ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e098df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # lit le fichier .env\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce133e4-5ed4-4e7c-924e-2951af66c2cf",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Environnement virtuel\n",
    "\n",
    "Il y aura potentiellement des bibliothÃ¨ques Ã  installer, ce qui peut se faire dans un environnement virtuel python.\n",
    "\n",
    "  $ python -m venv mon\\_env\n",
    "  \n",
    "  $ source mon\\_env/bin/activate\n",
    "  \n",
    "  (mon\\_env) $ pip install nom\\_bibliotheque\n",
    "\n",
    "(Et pour dÃ©sactiver : deactivate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2addb622-d7a9-4847-bef2-b573f2ee8e27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### RÃ©fÃ©rences\n",
    "- LangChain\n",
    "  - https://python.langchain.com/docs/introduction/\n",
    "  - https://www.youtube.com/@LangChain\n",
    "- Mistral AI\n",
    "  - https://docs.mistral.ai/\n",
    "- OpenAI\n",
    "  - https://platform.openai.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3ec0b-529d-4b3a-8fb8-aa1bdca2b1ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Introduction Ã  LangChain / Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171308f5-cbab-4a41-b1eb-daa5e85a3bc1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### PremiÃ¨res invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e601d0-9ba2-44df-b782-a139ef988eca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capitale de l'Albanie est **Tirana**.\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialiser le modÃ¨le\n",
    "llm = ChatMistralAI(model=\"mistral-medium-2508\", \n",
    "                    temperature=0, api_key=api_key)\n",
    "\n",
    "# CrÃ©er le message utilisateur\n",
    "message = HumanMessage(content=\"Quelle est la capitale de l'Albanie ?\")\n",
    "\n",
    "# Obtenir la rÃ©ponse\n",
    "response = llm.invoke([message])\n",
    "\n",
    "# Afficher la rÃ©ponse\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84dbc14e-6d82-4efa-a6e1-1ed98ba17048",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capitale de lâ€™Albanie est Tirana.\n"
     ]
    }
   ],
   "source": [
    "# Version OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1\")\n",
    "response = llm.invoke(\"Quelle est la capitale de l'Albanie ?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b4bcfe1-8c3e-40b6-97f7-a42dc07baac6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bien sÃ»r ! En voici une :\n",
      "\n",
      "**Pourquoi les pompiers ne jouent-ils jamais Ã  cache-cache ?**\n",
      "*Parce que les autres nâ€™aiment pas quand ils crient : \"Jâ€™ai trouvÃ© !\" en arrivant avec la grande Ã©chelle !* ðŸš’ðŸ”¥\n",
      "\n",
      "(Et une autre pour la route :)\n",
      "**Que dit un pompier quand il arrive en retard ?**\n",
      "*\"DÃ©solÃ©, jâ€™ai eu un feu rouge !\"* ðŸ˜„\n",
      "\n",
      "Tu veux une autre ? ðŸ˜†\n",
      "----------\n",
      "Bien sÃ»r ! En voici une :\n",
      "\n",
      "**Pourquoi les policiers nâ€™aiment-ils pas les livres ?**\n",
      "*Parce quâ€™ils prÃ©fÃ¨rent les* **couvre-feux** ! ðŸ˜„\n",
      "\n",
      "(Jeu de mots entre *couvre-livres* et *couvre-feux*â€¦ bon, ok, câ€™est un peu tirÃ©e par les cheveux !)\n",
      "\n",
      "---\n",
      "**Autre version (plus courte) :**\n",
      "*Un voleur se fait arrÃªter par la police.*\n",
      "*Le policier lui dit :*\n",
      "â€” *\"On vous a vu voler Ã  lâ€™Ã©talage !\"*\n",
      "*â€” \"Non, câ€™est fauxâ€¦ Je nâ€™ai rien pris, je lâ€™ai juste* **empruntÃ©** *!\"*\n",
      "*â€” \"Ah bon ? Alors rendez-le !\"*\n",
      "*â€” \"â€¦ Je ne me souviens plus oÃ¹ je lâ€™ai* **dÃ©posÃ©** *.\"* ðŸ˜†\n",
      "\n",
      "---\n",
      "Tu veux une autre thÃ©matique (gendarme, radar, etc.) ? ðŸ˜‰\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "# 1) Initialisation du modÃ¨le Mistral\n",
    "model = ChatMistralAI(model=\"mistral-medium-2508\", \n",
    "                      temperature=0, api_key=api_key)\n",
    "\n",
    "# 2) Construction du prompt + parser\n",
    "prompt = ChatPromptTemplate.from_template(\"Fais-moi une blague sur le sujet : {sujet}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 3) ChaÃ®nage (LangChain Expression Language)\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# 4) ExÃ©cution\n",
    "for sujet in ['pompier', 'police']:\n",
    "    print(chain.invoke({\"sujet\": sujet}))\n",
    "    print('-'*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a07cb0e4-94b6-4e04-8959-40435196d1ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Mistral-large-latest** est un modÃ¨le de langage large (LLM - *Large Language Model*) dÃ©veloppÃ© par **Mistral AI**, une startup franÃ§aise spÃ©cialisÃ©e dans l\\'intelligence artificielle. Voici une description technique dÃ©taillÃ©e de ce modÃ¨le, adaptÃ©e Ã  une documentation professionnelle :\\n\\n---\\n\\n### **1. PrÃ©sentation GÃ©nÃ©rale**\\n**Mistral-large-latest** est une version optimisÃ©e et mise Ã  jour du modÃ¨le **Mistral Large**, conÃ§u pour offrir des performances de pointe en comprÃ©hension, gÃ©nÃ©ration et raisonnement sur des tÃ¢ches complexes en langage naturel. Il sâ€™appuie sur une **architecture de type *Transformer dÃ©codeur*** (similaire Ã  GPT-4 ou Llama 2), avec des amÃ©liorations spÃ©cifiques pour :\\n- **La prÃ©cision contextuelle** : Meilleure gestion des instructions longues ou ambiguÃ«s.\\n- **La spÃ©cialisation multilingue** : Prise en charge native de plusieurs langues (dont le franÃ§ais, lâ€™anglais, lâ€™espagnol, lâ€™allemand, etc.), avec une attention particuliÃ¨re pour les nuances culturelles et linguistiques.\\n- **Lâ€™efficacitÃ© computationnelle** : Optimisation pour un dÃ©ploiement en production (latence rÃ©duite, consommation mÃ©moire maÃ®trisÃ©e).\\n\\n---\\n\\n### **2. CaractÃ©ristiques Techniques ClÃ©s**\\n#### **a. Architecture et Taille**\\n- **Type** : ModÃ¨le *dÃ©codeur-only* (comme la famille GPT), basÃ© sur lâ€™architecture **Transformer**.\\n- **Taille** :\\n  - Le suffixe *\"large\"* indique un modÃ¨le de grande taille (estimÃ© entre **70B et 120B de paramÃ¨tres**, bien que Mistral AI ne communique pas toujours les dÃ©tails exacts pour des raisons stratÃ©giques).\\n  - Version *\"latest\"* : IntÃ¨gre les derniÃ¨res mises Ã  jour (fine-tuning, corrections de biais, extensions de contexte).\\n- **Contexte** : FenÃªtre contextuelle Ã©tendue (jusquâ€™Ã  **32k tokens** ou plus, selon la version), permettant de traiter des documents longs ou des conversations complexes sans perte dâ€™information.\\n\\n#### **b. EntraÃ®nement**\\n- **DonnÃ©es** :\\n  - Corpus multilingue diversifiÃ© (textes publics, code source, littÃ©rature technique, conversations, etc.).\\n  - Filtres stricts pour limiter les biais et les contenus toxiques (alignement Ã©thique via *Reinforcement Learning from Human Feedback* â€“ RLHF).\\n- **MÃ©thodes** :\\n  - **PrÃ©traÃ®nement auto-supervisÃ©** sur des objectifs de prÃ©diction de tokens (comme le *causal language modeling*).\\n  - **Post-entraÃ®nement** (*fine-tuning*) pour des tÃ¢ches spÃ©cifiques (ex : gÃ©nÃ©ration de code, rÃ©ponse aux questions, rÃ©sumÃ©).\\n  - **Optimisation pour lâ€™infÃ©rence** : Techniques comme la *quantisation* (rÃ©duction de la prÃ©cision des poids pour accÃ©lÃ©rer le calcul) ou le *speculative decoding* (gÃ©nÃ©ration accÃ©lÃ©rÃ©e de tokens).\\n\\n#### **c. Performances**\\n- **Benchmarks** :\\n  - ClassÃ© parmi les **meilleurs modÃ¨les ouverts/commerciaux** sur des tests standardisÃ©s comme **MT-Bench** (pour les tÃ¢ches multi-disciplinaires), **HumanEval** (gÃ©nÃ©ration de code), ou **MMLU** (raisonnement gÃ©nÃ©ral).\\n  - Surpasse souvent des modÃ¨les comme **Llama 2 70B** ou **Claude 2** sur des tÃ¢ches en franÃ§ais.\\n- **Cas dâ€™usage optimaux** :\\n  - GÃ©nÃ©ration de texte crÃ©atif ou technique (documentation, code, emails).\\n  - Assistance conversationnelle (chatbots, support client).\\n  - Analyse et synthÃ¨se de donnÃ©es non structurÃ©es (rapports, articles).\\n  - Traduction et localisation de contenu.\\n\\n---\\n\\n### **3. Points Forts par Rapport aux Autres ModÃ¨les**\\n| **CritÃ¨re**               | **Mistral-large-latest**                          | **Alternatives (ex : GPT-4, Llama 2, Claude)**       |\\n|---------------------------|---------------------------------------------------|------------------------------------------------------|\\n| **Multilingue**           | Excellente prise en charge du franÃ§ais et autres langues europÃ©ennes. | GPT-4 performant mais optimisÃ© pour lâ€™anglais ; Llama 2 moins prÃ©cis en franÃ§ais. |\\n| **Ouverture**             | ModÃ¨le partiellement ouvert (accÃ¨s via API ou auto-hÃ©bergement). | GPT-4 fermÃ© (API seulement) ; Llama 2 ouvert mais moins performant. |\\n| **EfficacitÃ©**            | Latence rÃ©duite et coÃ»t optimisÃ© pour lâ€™infÃ©rence. | GPT-4 coÃ»teux ; Llama 2 nÃ©cessite souvent des optimisations manuelles. |\\n| **Alignement Ã©thique**    | RLHF robuste pour limiter les biais/hallucinations. | Variable selon les modÃ¨les (ex : Claude axÃ© sur la sÃ©curitÃ©). |\\n| **Contexte Ã©tendu**       | Jusquâ€™Ã  32k+ tokens (idÃ©al pour les documents longs). | GPT-4-32k similaire ; Llama 2 limitÃ© Ã  4k tokens. |\\n\\n---\\n\\n### **4. Limites et ConsidÃ©rations**\\n- **Hallucinations** : Comme tous les LLM, peut gÃ©nÃ©rer des informations incorrectes ou non vÃ©rifiÃ©es (nÃ©cessite une **validation humaine** pour les usages critiques).\\n- **DonnÃ©es sensibles** : Non adaptÃ© au traitement de donnÃ©es personnelles ou confidentielles sans garanties de chiffrement/anonimisation.\\n- **CoÃ»t** : La version *\"large\"* peut Ãªtre coÃ»teuse en ressources (GPU/TPU) pour un dÃ©ploiement local. Mistral propose des options hÃ©bergÃ©es via son [API](https://mistral.ai/).\\n- **Mises Ã  jour** : Le suffixe *\"latest\"* implique des changements rÃ©guliers â€“ vÃ©rifier la [documentation officielle](https://docs.mistral.ai/) pour les spÃ©cifications exactes de la version dÃ©ployÃ©e.\\n\\n---\\n\\n### **5. Comment lâ€™Utiliser ?**\\n#### **a. Via lâ€™API Mistral**\\n- **Endpoint** : `https://api.mistral.ai/v1/models`\\n- **Exemple de requÃªte (Python)** :\\n  ```python\\n  from mistralai.client import MistralClient\\n  from mistralai.models.chat_completion import ChatMessage\\n\\n  client = MistralClient(api_key=\"votre_cle_api\")\\n  messages = [ChatMessage(role=\"user\", content=\"Expliquez la relativitÃ© restreinte en 3 phrases.\")]\\n  response = client.chat(model=\"mistral-large-latest\", messages=messages)\\n  print(response.choices[0].message.content)\\n  ```\\n- **Tarification** : Pay-as-you-go (voir [pricing](https://mistral.ai/pricing/)).\\n\\n#### **b. Auto-hÃ©bergement (pour les versions open-weight)**\\n- **PrÃ©requis** : GPU haut de gamme (ex : A100/H100) ou infrastructure cloud (AWS, GCP).\\n- **Outils** :\\n  - Framework **vLLM** pour lâ€™infÃ©rence optimisÃ©e.\\n  - Conteneurs Docker officiels (si disponibles).\\n- **Exemple avec `transformers`** :\\n  ```python\\n  from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n  tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-large-latest\")\\n  model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-large-latest\", device_map=\"auto\")\\n  inputs = tokenizer(\"Pourquoi le ciel est bleu ?\", return_tensors=\"pt\").to(\"cuda\")\\n  outputs = model.generate(**inputs, max_new_tokens=100)\\n  print(tokenizer.decode(outputs[0], skip_special_tokens=True))\\n  ```\\n\\n#### **c. IntÃ©grations**\\n- **Plugins** : Compatible avec des outils comme **LangChain**, **LlamaIndex**, ou **FastAPI** pour construire des applications custom.\\n- **Fine-tuning** : Mistral propose des services pour adapter le modÃ¨le Ã  des domaines spÃ©cifiques (ex : mÃ©dical, juridique).\\n\\n---\\n\\n### **6. Bonnes Pratiques**\\n- **Prompt Engineering** :\\n  - Utiliser des instructions claires et structurÃ©es (ex : *\"RÃ©ponds en 3 points avec des exemples\"*).\\n  - Pour le code : prÃ©ciser le langage (`# Python`, `// JavaScript`).\\n- **Gestion des tokens** :\\n  - Limiter les requÃªtes Ã  la fenÃªtre contextuelle (Ã©viter les dÃ©passements).\\n  - Compresser les prompts longs avec des techniques de *summarization*.\\n- **Ã‰valuation** :\\n  - Tester systÃ©matiquement les sorties (mÃ©triques comme **ROUGE** pour le rÃ©sumÃ©, **BLEU** pour la traduction).\\n  - Surveiller les biais via des outils comme **Delphi** ou **Fairlearn**.\\n\\n---\\n### **7. Ressources Officielles**\\n- **Documentation** : [https://docs.mistral.ai/](https://docs.mistral.ai/)\\n- **Blog technique** : [https://mistral.ai/news/](https://mistral.ai/news/)\\n- **CommunautÃ©** : Serveur Discord [Mistral AI](https://discord.gg/mistralai) pour le support.\\n- **Paper** : PrÃ©publications sur [arXiv](https://arxiv.org/) (rechercher \"Mistral Large\").\\n\\n---\\n### **8. Alternatives et Comparaison**\\n| **ModÃ¨le**               | **Avantages**                                  | **InconvÃ©nients**                          |\\n|--------------------------|-----------------------------------------------|--------------------------------------------|\\n| **Mistral-large-latest** | Meilleur franÃ§ais, ouvert, contexte Ã©tendu.   | CoÃ»t Ã©levÃ© pour lâ€™auto-hÃ©bergement.       |\\n| **GPT-4 (OpenAI)**       | Performances gÃ©nÃ©rales supÃ©rieures.          | FermÃ©, cher, biais vers lâ€™anglais.         |\\n| **Llama 2 70B**          | Open source, gratuit.                         | Moins prÃ©cis en franÃ§ais, contexte limitÃ©.|\\n| **Claude 2 (Anthropic)** | SÃ©curitÃ© et alignement Ã©thique renforcÃ©s.    | AccÃ¨s restreint, moins multilingue.       |\\n\\n---\\n### **9. Cas dâ€™Usage Concrets**\\n1. **GÃ©nÃ©ration de Documentation Technique** :\\n   - Automatiser la rÃ©daction de manuels ou de FAQ Ã  partir de spÃ©cifications.\\n   - Exemple de prompt :\\n     *\"RÃ©dige une section de documentation pour expliquer lâ€™API REST de notre produit. Utilise des exemples en Python et JavaScript. Structure : 1. Authentification 2. Endpoints 3. Gestion des erreurs.\"*\\n\\n2. **Support Client Multilingue** :\\n   - DÃ©ployer un chatbot capable de rÃ©pondre en franÃ§ais, anglais et espagnol avec une tonalitÃ© adaptÃ©e.\\n\\n3. **Analyse de DonnÃ©es Non StructurÃ©es** :\\n   - Extraire des insights Ã  partir de retours clients ou de rapports PDF.\\n\\n4. **DÃ©veloppement AssistÃ©** :\\n   - GÃ©nÃ©ration de tests unitaires, dÃ©bogage, ou explication de code.\\n   - Exemple :\\n     *\"Ce code Python a une fuite mÃ©moire. Identifie le problÃ¨me et propose une correction : [coller le code].\"*\\n\\n---\\n### **10. Roadmap et Ã‰volutions**\\nMistral AI travaille activement sur :\\n- **Mistral Next** : ModÃ¨les encore plus grands (potentiellement >200B paramÃ¨tres).\\n- **AmÃ©lioration du RAG** (*Retrieval-Augmented Generation*) pour rÃ©duire les hallucinations.\\n- **Outils no-code** pour dÃ©mocratiser lâ€™accÃ¨s aux LLM.\\n\\n---\\n### **Conclusion**\\n**Mistral-large-latest** est un choix idÃ©al pour les organisations recherchant un **LLM hautement performant en multilingue**, avec un focus particulier sur lâ€™efficacitÃ© et lâ€™ouverture relative. Son Ã©quilibre entre puissance, coÃ»t et flexibilitÃ© en fait une alternative sÃ©rieuse Ã  GPT-4 pour de nombreux cas dâ€™usage, surtout en Europe.\\n\\nPour un dÃ©ploiement rÃ©ussi, il est recommandÃ© de :\\n1. **Tester en conditions rÃ©elles** avec des jeux de donnÃ©es reprÃ©sentatifs.\\n2. **Surveiller les performances** via des mÃ©triques adaptÃ©es.\\n3. **Combiner avec des outils externes** (bases de connaissances, vÃ©rification humaine) pour les tÃ¢ches critiques.\\n\\n---\\n**Besoin de prÃ©cisions sur un aspect technique spÃ©cifique (ex : optimisation de lâ€™infÃ©rence, fine-tuning, benchmarking) ?** Je peux approfondir !' additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 28, 'total_tokens': 2696, 'completion_tokens': 2668}, 'model_name': 'mistral-medium-2508', 'model': 'mistral-medium-2508', 'finish_reason': 'stop'} id='run--529ab154-6d0e-4f04-9d42-210604ca132f-0' usage_metadata={'input_tokens': 28, 'output_tokens': 2668, 'total_tokens': 2696}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Vous Ãªtes un rÃ©dacteur de documentation technique de classe mondiale.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-medium-2508\", api_key=api_key)\n",
    "chain = prompt | llm \n",
    "result = chain.invoke({\"input\": \"Qu'est-ce que le modÃ¨le mistral-large-latest ?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dfaa652-299d-4d7a-a480-4e17a079cf19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 28, 'output_tokens': 2668, 'total_tokens': 2696}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "120dac24-febc-4714-ad88-8033d5bd9d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056156"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "38/1000000*2+701*8/100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb7b1825-15de-4b92-8d7e-7ff0e670e212",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Mistral-large-latest** est un modÃ¨le de langage large (LLM - *Large Language Model*) dÃ©veloppÃ© par **Mistral AI**, une startup franÃ§aise spÃ©cialisÃ©e dans l'intelligence artificielle. Voici une description technique dÃ©taillÃ©e de ce modÃ¨le, adaptÃ©e Ã  une documentation professionnelle :\n",
      "\n",
      "---\n",
      "\n",
      "### **1. PrÃ©sentation GÃ©nÃ©rale**\n",
      "**Mistral-large-latest** est une version optimisÃ©e et mise Ã  jour du modÃ¨le **Mistral Large**, conÃ§u pour offrir des performances de pointe en comprÃ©hension, gÃ©nÃ©ration et raisonnement sur des tÃ¢ches complexes en langage naturel. Il sâ€™appuie sur une **architecture de type *Transformer dÃ©codeur*** (similaire Ã  GPT-4 ou Llama 2), avec des amÃ©liorations spÃ©cifiques pour :\n",
      "- **La prÃ©cision contextuelle** : Meilleure gestion des instructions longues ou ambiguÃ«s.\n",
      "- **La spÃ©cialisation multilingue** : Prise en charge native de plusieurs langues (dont le franÃ§ais, lâ€™anglais, lâ€™espagnol, lâ€™allemand, etc.), avec une attention particuliÃ¨re pour les nuances culturelles et linguistiques.\n",
      "- **Lâ€™efficacitÃ© computationnelle** : Optimisation pour un dÃ©ploiement en production (latence rÃ©duite, consommation mÃ©moire maÃ®trisÃ©e).\n",
      "\n",
      "---\n",
      "\n",
      "### **2. CaractÃ©ristiques Techniques ClÃ©s**\n",
      "#### **a. Architecture et Taille**\n",
      "- **Type** : ModÃ¨le *dÃ©codeur-only* (comme la famille GPT), basÃ© sur lâ€™architecture **Transformer**.\n",
      "- **Taille** :\n",
      "  - Le suffixe *\"large\"* indique un modÃ¨le de grande taille (estimÃ© entre **70B et 120B de paramÃ¨tres**, bien que Mistral AI ne communique pas toujours les dÃ©tails exacts pour des raisons stratÃ©giques).\n",
      "  - Version *\"latest\"* : IntÃ¨gre les derniÃ¨res mises Ã  jour (fine-tuning, corrections de biais, extensions de contexte).\n",
      "- **Contexte** : FenÃªtre contextuelle Ã©tendue (jusquâ€™Ã  **32k tokens** ou plus, selon la version), permettant de traiter des documents longs ou des conversations complexes sans perte dâ€™information.\n",
      "\n",
      "#### **b. EntraÃ®nement**\n",
      "- **DonnÃ©es** :\n",
      "  - Corpus multilingue diversifiÃ© (textes publics, code source, littÃ©rature technique, conversations, etc.).\n",
      "  - Filtres stricts pour limiter les biais et les contenus toxiques (alignement Ã©thique via *Reinforcement Learning from Human Feedback* â€“ RLHF).\n",
      "- **MÃ©thodes** :\n",
      "  - **PrÃ©traÃ®nement auto-supervisÃ©** sur des objectifs de prÃ©diction de tokens (comme le *causal language modeling*).\n",
      "  - **Post-entraÃ®nement** (*fine-tuning*) pour des tÃ¢ches spÃ©cifiques (ex : gÃ©nÃ©ration de code, rÃ©ponse aux questions, rÃ©sumÃ©).\n",
      "  - **Optimisation pour lâ€™infÃ©rence** : Techniques comme la *quantisation* (rÃ©duction de la prÃ©cision des poids pour accÃ©lÃ©rer le calcul) ou le *speculative decoding* (gÃ©nÃ©ration accÃ©lÃ©rÃ©e de tokens).\n",
      "\n",
      "#### **c. Performances**\n",
      "- **Benchmarks** :\n",
      "  - ClassÃ© parmi les **meilleurs modÃ¨les ouverts/commerciaux** sur des tests standardisÃ©s comme **MT-Bench** (pour les tÃ¢ches multi-disciplinaires), **HumanEval** (gÃ©nÃ©ration de code), ou **MMLU** (raisonnement gÃ©nÃ©ral).\n",
      "  - Surpasse souvent des modÃ¨les comme **Llama 2 70B** ou **Claude 2** sur des tÃ¢ches en franÃ§ais.\n",
      "- **Cas dâ€™usage optimaux** :\n",
      "  - GÃ©nÃ©ration de texte crÃ©atif ou technique (documentation, code, emails).\n",
      "  - Assistance conversationnelle (chatbots, support client).\n",
      "  - Analyse et synthÃ¨se de donnÃ©es non structurÃ©es (rapports, articles).\n",
      "  - Traduction et localisation de contenu.\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Points Forts par Rapport aux Autres ModÃ¨les**\n",
      "| **CritÃ¨re**               | **Mistral-large-latest**                          | **Alternatives (ex : GPT-4, Llama 2, Claude)**       |\n",
      "|---------------------------|---------------------------------------------------|------------------------------------------------------|\n",
      "| **Multilingue**           | Excellente prise en charge du franÃ§ais et autres langues europÃ©ennes. | GPT-4 performant mais optimisÃ© pour lâ€™anglais ; Llama 2 moins prÃ©cis en franÃ§ais. |\n",
      "| **Ouverture**             | ModÃ¨le partiellement ouvert (accÃ¨s via API ou auto-hÃ©bergement). | GPT-4 fermÃ© (API seulement) ; Llama 2 ouvert mais moins performant. |\n",
      "| **EfficacitÃ©**            | Latence rÃ©duite et coÃ»t optimisÃ© pour lâ€™infÃ©rence. | GPT-4 coÃ»teux ; Llama 2 nÃ©cessite souvent des optimisations manuelles. |\n",
      "| **Alignement Ã©thique**    | RLHF robuste pour limiter les biais/hallucinations. | Variable selon les modÃ¨les (ex : Claude axÃ© sur la sÃ©curitÃ©). |\n",
      "| **Contexte Ã©tendu**       | Jusquâ€™Ã  32k+ tokens (idÃ©al pour les documents longs). | GPT-4-32k similaire ; Llama 2 limitÃ© Ã  4k tokens. |\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Limites et ConsidÃ©rations**\n",
      "- **Hallucinations** : Comme tous les LLM, peut gÃ©nÃ©rer des informations incorrectes ou non vÃ©rifiÃ©es (nÃ©cessite une **validation humaine** pour les usages critiques).\n",
      "- **DonnÃ©es sensibles** : Non adaptÃ© au traitement de donnÃ©es personnelles ou confidentielles sans garanties de chiffrement/anonimisation.\n",
      "- **CoÃ»t** : La version *\"large\"* peut Ãªtre coÃ»teuse en ressources (GPU/TPU) pour un dÃ©ploiement local. Mistral propose des options hÃ©bergÃ©es via son [API](https://mistral.ai/).\n",
      "- **Mises Ã  jour** : Le suffixe *\"latest\"* implique des changements rÃ©guliers â€“ vÃ©rifier la [documentation officielle](https://docs.mistral.ai/) pour les spÃ©cifications exactes de la version dÃ©ployÃ©e.\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Comment lâ€™Utiliser ?**\n",
      "#### **a. Via lâ€™API Mistral**\n",
      "- **Endpoint** : `https://api.mistral.ai/v1/models`\n",
      "- **Exemple de requÃªte (Python)** :\n",
      "  ```python\n",
      "  from mistralai.client import MistralClient\n",
      "  from mistralai.models.chat_completion import ChatMessage\n",
      "\n",
      "  client = MistralClient(api_key=\"votre_cle_api\")\n",
      "  messages = [ChatMessage(role=\"user\", content=\"Expliquez la relativitÃ© restreinte en 3 phrases.\")]\n",
      "  response = client.chat(model=\"mistral-large-latest\", messages=messages)\n",
      "  print(response.choices[0].message.content)\n",
      "  ```\n",
      "- **Tarification** : Pay-as-you-go (voir [pricing](https://mistral.ai/pricing/)).\n",
      "\n",
      "#### **b. Auto-hÃ©bergement (pour les versions open-weight)**\n",
      "- **PrÃ©requis** : GPU haut de gamme (ex : A100/H100) ou infrastructure cloud (AWS, GCP).\n",
      "- **Outils** :\n",
      "  - Framework **vLLM** pour lâ€™infÃ©rence optimisÃ©e.\n",
      "  - Conteneurs Docker officiels (si disponibles).\n",
      "- **Exemple avec `transformers`** :\n",
      "  ```python\n",
      "  from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "\n",
      "  tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-large-latest\")\n",
      "  model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-large-latest\", device_map=\"auto\")\n",
      "  inputs = tokenizer(\"Pourquoi le ciel est bleu ?\", return_tensors=\"pt\").to(\"cuda\")\n",
      "  outputs = model.generate(**inputs, max_new_tokens=100)\n",
      "  print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
      "  ```\n",
      "\n",
      "#### **c. IntÃ©grations**\n",
      "- **Plugins** : Compatible avec des outils comme **LangChain**, **LlamaIndex**, ou **FastAPI** pour construire des applications custom.\n",
      "- **Fine-tuning** : Mistral propose des services pour adapter le modÃ¨le Ã  des domaines spÃ©cifiques (ex : mÃ©dical, juridique).\n",
      "\n",
      "---\n",
      "\n",
      "### **6. Bonnes Pratiques**\n",
      "- **Prompt Engineering** :\n",
      "  - Utiliser des instructions claires et structurÃ©es (ex : *\"RÃ©ponds en 3 points avec des exemples\"*).\n",
      "  - Pour le code : prÃ©ciser le langage (`# Python`, `// JavaScript`).\n",
      "- **Gestion des tokens** :\n",
      "  - Limiter les requÃªtes Ã  la fenÃªtre contextuelle (Ã©viter les dÃ©passements).\n",
      "  - Compresser les prompts longs avec des techniques de *summarization*.\n",
      "- **Ã‰valuation** :\n",
      "  - Tester systÃ©matiquement les sorties (mÃ©triques comme **ROUGE** pour le rÃ©sumÃ©, **BLEU** pour la traduction).\n",
      "  - Surveiller les biais via des outils comme **Delphi** ou **Fairlearn**.\n",
      "\n",
      "---\n",
      "### **7. Ressources Officielles**\n",
      "- **Documentation** : [https://docs.mistral.ai/](https://docs.mistral.ai/)\n",
      "- **Blog technique** : [https://mistral.ai/news/](https://mistral.ai/news/)\n",
      "- **CommunautÃ©** : Serveur Discord [Mistral AI](https://discord.gg/mistralai) pour le support.\n",
      "- **Paper** : PrÃ©publications sur [arXiv](https://arxiv.org/) (rechercher \"Mistral Large\").\n",
      "\n",
      "---\n",
      "### **8. Alternatives et Comparaison**\n",
      "| **ModÃ¨le**               | **Avantages**                                  | **InconvÃ©nients**                          |\n",
      "|--------------------------|-----------------------------------------------|--------------------------------------------|\n",
      "| **Mistral-large-latest** | Meilleur franÃ§ais, ouvert, contexte Ã©tendu.   | CoÃ»t Ã©levÃ© pour lâ€™auto-hÃ©bergement.       |\n",
      "| **GPT-4 (OpenAI)**       | Performances gÃ©nÃ©rales supÃ©rieures.          | FermÃ©, cher, biais vers lâ€™anglais.         |\n",
      "| **Llama 2 70B**          | Open source, gratuit.                         | Moins prÃ©cis en franÃ§ais, contexte limitÃ©.|\n",
      "| **Claude 2 (Anthropic)** | SÃ©curitÃ© et alignement Ã©thique renforcÃ©s.    | AccÃ¨s restreint, moins multilingue.       |\n",
      "\n",
      "---\n",
      "### **9. Cas dâ€™Usage Concrets**\n",
      "1. **GÃ©nÃ©ration de Documentation Technique** :\n",
      "   - Automatiser la rÃ©daction de manuels ou de FAQ Ã  partir de spÃ©cifications.\n",
      "   - Exemple de prompt :\n",
      "     *\"RÃ©dige une section de documentation pour expliquer lâ€™API REST de notre produit. Utilise des exemples en Python et JavaScript. Structure : 1. Authentification 2. Endpoints 3. Gestion des erreurs.\"*\n",
      "\n",
      "2. **Support Client Multilingue** :\n",
      "   - DÃ©ployer un chatbot capable de rÃ©pondre en franÃ§ais, anglais et espagnol avec une tonalitÃ© adaptÃ©e.\n",
      "\n",
      "3. **Analyse de DonnÃ©es Non StructurÃ©es** :\n",
      "   - Extraire des insights Ã  partir de retours clients ou de rapports PDF.\n",
      "\n",
      "4. **DÃ©veloppement AssistÃ©** :\n",
      "   - GÃ©nÃ©ration de tests unitaires, dÃ©bogage, ou explication de code.\n",
      "   - Exemple :\n",
      "     *\"Ce code Python a une fuite mÃ©moire. Identifie le problÃ¨me et propose une correction : [coller le code].\"*\n",
      "\n",
      "---\n",
      "### **10. Roadmap et Ã‰volutions**\n",
      "Mistral AI travaille activement sur :\n",
      "- **Mistral Next** : ModÃ¨les encore plus grands (potentiellement >200B paramÃ¨tres).\n",
      "- **AmÃ©lioration du RAG** (*Retrieval-Augmented Generation*) pour rÃ©duire les hallucinations.\n",
      "- **Outils no-code** pour dÃ©mocratiser lâ€™accÃ¨s aux LLM.\n",
      "\n",
      "---\n",
      "### **Conclusion**\n",
      "**Mistral-large-latest** est un choix idÃ©al pour les organisations recherchant un **LLM hautement performant en multilingue**, avec un focus particulier sur lâ€™efficacitÃ© et lâ€™ouverture relative. Son Ã©quilibre entre puissance, coÃ»t et flexibilitÃ© en fait une alternative sÃ©rieuse Ã  GPT-4 pour de nombreux cas dâ€™usage, surtout en Europe.\n",
      "\n",
      "Pour un dÃ©ploiement rÃ©ussi, il est recommandÃ© de :\n",
      "1. **Tester en conditions rÃ©elles** avec des jeux de donnÃ©es reprÃ©sentatifs.\n",
      "2. **Surveiller les performances** via des mÃ©triques adaptÃ©es.\n",
      "3. **Combiner avec des outils externes** (bases de connaissances, vÃ©rification humaine) pour les tÃ¢ches critiques.\n",
      "\n",
      "---\n",
      "**Besoin de prÃ©cisions sur un aspect technique spÃ©cifique (ex : optimisation de lâ€™infÃ©rence, fine-tuning, benchmarking) ?** Je peux approfondir !\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0079cb0a-8f86-462f-9ed4-92a886d3bcbd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Les sorties structurÃ©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ffa5cd-8e50-4ea5-83a0-32acc0114322",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Cas d'un boolÃ©en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd16a7ce-de15-4342-b612-0bec866e82af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoÃ«l est en hiver\n",
      "RÃ©ponse : True (type : <class 'bool'>)\n",
      "\n",
      "Il pleut quand il pleut pas\n",
      "RÃ©ponse : False (type : <class 'bool'>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    answer: bool\n",
    "\n",
    "prompt_answer = [\n",
    "    (\"system\", \"Tu es un assistant chargÃ© de rÃ©pondre un boolÃ©en (True ou False) Ã  la question d'un utilisateur.\"),\n",
    "    ('human', \"{question}\")\n",
    "]\n",
    "\n",
    "prompt_answer_template = ChatPromptTemplate.from_messages(prompt_answer)\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "chain = prompt_answer_template | llm.with_structured_output(schema=Answer)\n",
    "\n",
    "def repond(question):\n",
    "    return chain.invoke({\"question\": question}).answer\n",
    "    \n",
    "for question in [\"NoÃ«l est en hiver\", \"Il pleut quand il pleut pas\"]:\n",
    "    print(question)\n",
    "    reponse = repond(question)\n",
    "    print(f\"RÃ©ponse : {reponse} (type : {type(reponse)})\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57646f4-a101-441d-a491-cde12e06a8f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Cas d'une classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea1dad8b-78fd-4656-ac3e-e4865ef70c4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peux-tu m'en dire plus\n",
      "action=\"Fournir plus d'Ã©lÃ©ments Ã  la question prÃ©cÃ©dente\"\n",
      "\n",
      "Que sont les PPV ?\n",
      "action='RÃ©pondre Ã  une nouvelle question'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "tasks = [\"RÃ©pondre Ã  une nouvelle question\", \"Fournir plus d'Ã©lÃ©ments Ã  la question prÃ©cÃ©dente\"]\n",
    "\n",
    "class NextTask(BaseModel):\n",
    "    \"\"\"Utilise toujours cet outil pour structurer ta rÃ©ponse to the user.\"\"\"\n",
    "    action: str = Field(..., \n",
    "                        enum=tasks,\n",
    "                        description=\"La prochaine action Ã  mener\")\n",
    "\n",
    "prompt_message = [\n",
    "    (\"system\", \"Tu es un assistant chargÃ© de classifier la demande d'un utilisateur parmi une \"\n",
    "               \"liste rÃ©duite d'actions Ã  mener en tant que chatbot. Tu dois dÃ©terminer la \"\n",
    "               \"prochaine action Ã  mener.\"),\n",
    "    ('human', \"{text}\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(prompt_message)\n",
    "llm = ChatMistralAI(model='mistral-large-latest', temperature=0)\n",
    "chain = prompt | llm.with_structured_output(schema=NextTask)\n",
    "\n",
    "for text in [\"Peux-tu m'en dire plus\", \"Que sont les PPV ?\"]:\n",
    "    print(text)\n",
    "    print(chain.invoke({\"text\": text}))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3bd17-0379-467b-9be9-06544afcaf0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Cas d'un entier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d8b502d-8200-4ca8-b0c0-3969c86dab19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message : Bonjour, pourrais-tu m'aider s'il te plaÃ®t ?\n",
      "note_ton=5\n",
      "\n",
      "Message : J'ai besoin de Ã§a immÃ©diatement.\n",
      "note_ton=1\n",
      "\n",
      "Message : Merci beaucoup pour ton aide prÃ©cieuse !\n",
      "note_ton=5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "class TonMessage(BaseModel):\n",
    "    \"\"\"Ã‰valuation du ton du message de l'utilisateur.\"\"\"\n",
    "    note_ton: int = Field(\n",
    "        ..., \n",
    "        ge=1,\n",
    "        le=5,\n",
    "        description=\"Note attribuÃ©e au ton du message : 1 pour neutre, 5 pour trÃ¨s aimable\"\n",
    "    )\n",
    "\n",
    "prompt_message = [\n",
    "    (\"system\", \"Tu es un assistant chargÃ© d'Ã©valuer le ton d'un message donnÃ© par l'utilisateur. \"\n",
    "               \"Attribue une note de 1 Ã  5 au ton du message, oÃ¹ 1 signifie neutre et 5 signifie trÃ¨s aimable.\"),\n",
    "    ('human', \"{text}\")\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(prompt_message)\n",
    "llm = ChatMistralAI(model='mistral-large-latest', temperature=0)\n",
    "chain = prompt | llm.with_structured_output(schema=TonMessage)\n",
    "\n",
    "messages = [\n",
    "    \"Bonjour, pourrais-tu m'aider s'il te plaÃ®t ?\",\n",
    "    \"J'ai besoin de Ã§a immÃ©diatement.\",\n",
    "    \"Merci beaucoup pour ton aide prÃ©cieuse !\"\n",
    "]\n",
    "\n",
    "for text in messages:\n",
    "    print(f\"Message : {text}\")\n",
    "    print(chain.invoke({\"text\": text}))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d36f5d-0be7-47cf-8b7d-88e0e28bae71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Pourquoi et comment forcer la sortie du LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76a94afe-6764-46bb-9aaf-7b4583eb1f5c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The translation of your sentence into English is:\\n\\n**\"What is the capital of Albania?\"**\\n\\n*(The answer is **Tirana**, by the way!)* ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 26, 'total_tokens': 61, 'completion_tokens': 35}, 'model_name': 'mistral-medium-latest', 'model': 'mistral-medium-latest', 'finish_reason': 'stop'}, id='run--8b436054-40b4-4325-92e7-afd444e6bf04-0', usage_metadata={'input_tokens': 26, 'output_tokens': 35, 'total_tokens': 61})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-medium-latest\")\n",
    "message = HumanMessage(content=\"Peux-tu me traduire ce qui suit, en anglais ?\\n\\n Quelle est la capitale de l'Albanie ?\")\n",
    "llm.invoke([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e8c85e9-e6fa-4812-8718-50c1ee92a5cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Albania?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Translation(BaseModel):\n",
    "    original_text: str = Field(..., description=\"The original text before translation in another language\")\n",
    "    original_language: str = Field(..., description=\"The original language before translation\")\n",
    "    translated_text: str = Field(..., description=\"The final text after translation in another language\")\n",
    "    translated_language: str = Field(..., description=\"The language into which the translation must be done\")\n",
    "    \n",
    "def traduit(texte, langue_source=\"franÃ§ais\", langue_cible=\"anglais\"):\n",
    "    llm = ChatMistralAI(model_name=\"mistral-medium-latest\")\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"Je souhaite que tu traduises le texte suivant du {langue_source} vers le {langue_cible}. Ta traduction doit Ãªtre prÃ©cise, fluide et naturelle, et prÃ©server parfaitement le sens original. \n",
    "    Retourne-moi la rÃ©ponse sous forme d'objet JSON avec les champs :\n",
    "      - original_text : le texte original\n",
    "      - original_language : la langue du texte original\n",
    "      - translated_text : la traduction du texte\n",
    "      - translated_language : la langue de la traduction\n",
    "\n",
    "    Voici le texte Ã  traduire :\n",
    "    ----\n",
    "    {texte}\"\"\")\n",
    "    output_parser = StrOutputParser()\n",
    "    extract_translation = RunnableLambda(lambda translation: translation.translated_text)\n",
    "    chain0 = prompt | llm.with_structured_output(Translation) | extract_translation\n",
    "    return chain0.invoke({\"langue_source\": langue_source,\n",
    "                            \"langue_cible\": \"anglais\",\n",
    "                            \"texte\": texte      \n",
    "                            })\n",
    "\n",
    "print(traduit(\"Quelle est la capitale de l'Albanie\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2032b-c358-4aec-b673-43b4e1afed83",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Des sorties structurÃ©es aux prÃ©mices d'un raisonnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "462e27ab-59d3-4bc3-aab0-37c73e620d48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "class Etape(BaseModel):\n",
    "    explication: str\n",
    "    sortie: str\n",
    "\n",
    "class MathReponse(BaseModel):\n",
    "    etapes: list[Etape]\n",
    "    reponse_finale: str\n",
    "\n",
    "prompt_answer = [\n",
    "    (\"system\", \"Tu es un professeur de mathÃ©matiques trÃ¨s pÃ©dagogue.\"),\n",
    "    ('human', \"{exercice}\")\n",
    "]\n",
    "\n",
    "prompt_answer_template = ChatPromptTemplate.from_messages(prompt_answer)\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "chain = prompt_answer_template | llm.with_structured_output(schema=MathReponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83f49b83-7fe3-475a-818b-43c232ddaea2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- L'objectif est d'isoler x. Pour cela, commenÃ§ons par soustraire 31 des deux cÃ´tÃ©s de l'Ã©quation pour Ã©liminer le terme constant du cÃ´tÃ© gauche.\n",
      "  Le rÃ©sultat est alors : 8x = 2 - 31\n",
      "- Effectuons la soustraction Ã  droite pour simplifier l'Ã©quation.\n",
      "  Le rÃ©sultat est alors : 8x = -29\n",
      "- Maintenant, divisons les deux cÃ´tÃ©s de l'Ã©quation par 8 pour isoler x.\n",
      "  Le rÃ©sultat est alors : x = -29 / 8\n",
      "- Effectuons la division pour obtenir la valeur de x.\n",
      "  Le rÃ©sultat est alors : x = -3.625\n",
      "Au final, on trouve : x = -3.625\n"
     ]
    }
   ],
   "source": [
    "explications = chain.invoke({\"exercice\": \"RÃ©sous  8x + 31 = 2\"})\n",
    "for etape in explications.etapes:\n",
    "    print(f\"- {etape.explication}\")\n",
    "    print(f\"  Le rÃ©sultat est alors : {etape.sortie}\")\n",
    "\n",
    "print(f\"Au final, on trouve : {explications.reponse_finale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7905089f-3155-4381-91e5-17a2ba2aac9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Retrieval-Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918186c0-294c-4a8f-b0f8-65afd6f06a42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/rag.png\" alt=\"RAG\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589d138-b105-4567-aed0-ab5fb462ca51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Embeddings et semantique\n",
    "\n",
    "But : encoder un texte sous la forme d'un vecteur, de sorte que deux textes voisins sÃ©mantiquement soient encodÃ©s en deux vecteurs proches.\n",
    "\n",
    "![Texte alternatif](images/vectors-and-semantics.png \"Vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ee12db-412b-43bb-b0bd-9c680335617a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Embeddings : Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6a1c67-5364-4d43-bdb9-cf1f6bf7dc6d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "![Texte alternatif](images/Bag-of-words.png \"BoW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "188424dd-b206-46a9-bcfb-3c5c2277cebb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary : ['and' 'demo' 'demonstration' 'document' 'finally' 'first' 'here' 'is'\n",
      " 'second' 'text' 'the' 'third' 'this']\n",
      "BoW vector:\n",
      " [[0 0 1 1 0 1 0 0 0 1 0 0 0]\n",
      " [1 1 0 1 0 0 1 0 1 1 0 0 0]\n",
      " [1 0 0 1 1 0 0 1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'Demonstration text, first document',\n",
    "    \"Demo text, and here's a second document.\",\n",
    "    'And finally, this is the third document.'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Vocabulary :\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW vector:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11281bfe-e00f-49e0-8ae1-108605a139cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Embeddings par transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34d487f9-12a6-47ff-8609-0fedc2cb7064",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"This is an example sentence.\" -> [0.09812459 0.06781267 0.06252321]...\n",
      "\"Each sentence is converted into a fixed-sized vector.\" -> [0.06216577 0.05840717 0.00820479]...\n",
      "Embedding size: 384\n"
     ]
    }
   ],
   "source": [
    "# pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = [\"This is an example sentence.\", \"Each sentence is converted into a fixed-sized vector.\"]\n",
    "\n",
    "# EntraÃ®nÃ© sur des donnÃ©es essentiellement anglophones.\n",
    "# ConÃ§u pour Ãªtre lÃ©ger et rapide, tout en gardant une bonne prÃ©cision pour lâ€™anglais.\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(f'\"{sentence}\" -> {embedding[:3]}...')\n",
    "\n",
    "print(f\"Embedding size: {len(embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfb7a8bf-fc00-4b03-a5bf-32890b7f45ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.65207729e-02  7.26545528e-02 -3.65566462e-01 -7.02737719e-02\n",
      "  -2.06112370e-01  5.19921295e-02 -1.28497016e-02 -1.39605641e-01\n",
      "  -4.71076183e-02  1.24628298e-01 -7.31549039e-02 -2.52605230e-01\n",
      "   2.85230219e-01 -1.12165160e-01  9.12081599e-02 -1.15145050e-01\n",
      "  -5.70061989e-02  3.20486993e-01 -1.61287278e-01 -4.73647386e-01\n",
      "   3.03176671e-01  4.64675017e-02  3.84551734e-01 -5.37366159e-02\n",
      "  -9.78466347e-02  3.22349221e-01 -3.62097204e-01 -9.99814272e-02\n",
      "  -1.07472390e-02 -1.74769863e-01 -7.64722973e-02 -3.83381434e-02\n",
      "   4.25416440e-01  1.90166920e-01  1.54887754e-02  3.84460360e-01\n",
      "   2.09153861e-01  1.30659854e-02  6.54842705e-02  9.23802704e-02\n",
      "  -2.48034760e-01  2.97549397e-01 -1.82683066e-01  2.15694353e-01\n",
      "   2.30816737e-01  1.02084570e-01 -1.54894486e-01  1.22451119e-01\n",
      "  -1.96864322e-01  5.10899350e-02  8.70069116e-02  1.73673987e-01\n",
      "  -2.90965825e-01  1.98579445e-01  7.86646977e-02 -3.96869481e-01\n",
      "  -8.99151042e-02  1.15304410e-01 -3.47552123e-03  3.48371454e-02\n",
      "  -1.60975322e-01 -1.58165947e-01 -2.76869655e-01  4.26388979e-01\n",
      "   5.98660186e-02  6.18471839e-02  2.72837222e-01  1.96666270e-01\n",
      "  -1.17939189e-01 -1.33643046e-01 -2.67337888e-01 -3.36821347e-01\n",
      "   2.29286119e-01  3.97668362e-01  1.13105297e-01  1.00522399e-01\n",
      "  -2.63185263e-01 -9.64541808e-02 -3.05367380e-01 -9.76834968e-02\n",
      "  -1.13328159e-01  1.54144075e-02  2.95926243e-01  2.19961330e-01\n",
      "   2.46220931e-01  3.16740647e-02  7.73194358e-02  7.18099400e-02\n",
      "   1.40424266e-01  5.11829555e-02 -1.68341532e-01 -5.95128715e-01\n",
      "   2.40329251e-01  3.17607015e-01 -3.88529561e-02 -2.94892848e-01\n",
      "   1.80528611e-01  3.61951888e-02  2.57690400e-01  8.23560715e-01\n",
      "   1.40070647e-01 -8.03383812e-02 -6.18174933e-02 -9.01752189e-02\n",
      "   1.14790730e-01 -2.90556252e-01  3.30534488e-01 -2.12557182e-01\n",
      "   1.58265188e-01 -4.12546784e-01 -8.60647485e-02  1.22264242e-02\n",
      "   7.80155882e-02  4.66110706e-02  1.20251276e-01  1.15379043e-01\n",
      "   3.58826369e-01 -2.95536071e-01  2.40629718e-01  3.28016758e-01\n",
      "  -3.74648608e-02  2.16838513e-02 -3.26381236e-01  2.22172633e-01\n",
      "   5.41193224e-02 -3.46985668e-01  6.41295761e-02  6.68766871e-02\n",
      "   1.61394030e-01  1.44798443e-01  6.42243102e-02  1.70563251e-01\n",
      "   7.97803551e-02  1.90901861e-01 -2.15357751e-01 -2.94394821e-01\n",
      "  -3.04533690e-01  1.91265434e-01  9.96950492e-02 -4.44939196e-01\n",
      "   1.66512564e-01  3.60089242e-01  1.23127781e-01  3.49543057e-02\n",
      "   1.43274534e-02  2.08197068e-02 -6.12885058e-02  1.09736305e-02\n",
      "   8.59057307e-02  1.98545158e-02  1.86347678e-01 -1.14208736e-01\n",
      "  -2.96818405e-01  7.24941269e-02  1.58054382e-01 -8.90302882e-02\n",
      "   2.68423796e-01  4.57218811e-02 -4.18559730e-01 -1.94560215e-01\n",
      "   4.83291186e-02  3.04925977e-03 -1.85847446e-01  9.70826373e-02\n",
      "   1.95084780e-01 -1.30889239e-02 -5.37880622e-02  1.69087157e-01\n",
      "  -5.60183404e-03 -1.03137560e-01  3.72133911e-01 -1.43242285e-01\n",
      "   2.56733239e-01 -2.45170686e-02 -5.95078431e-02 -4.52222303e-02\n",
      "   4.02538963e-02 -4.63011041e-02  3.71608809e-02 -2.88201980e-02\n",
      "   1.08527444e-01 -1.45067647e-01  1.49910212e-01  1.44873178e-02\n",
      "  -2.73075491e-01  3.12966138e-01 -1.00796930e-01  5.74326254e-02\n",
      "  -3.42364833e-02  1.39859542e-01  9.60069895e-02 -2.33742800e-02\n",
      "   1.31520867e-01 -1.51918992e-01 -4.46628571e-01 -4.41853374e-01\n",
      "  -1.53541639e-01 -3.33519489e-01 -5.36727488e-01 -1.15265854e-01\n",
      "   1.35443464e-01  1.03348441e-01 -1.19261213e-01 -3.31817307e-02\n",
      "  -9.43362638e-02 -1.10796221e-01 -1.18562937e-01 -1.50282728e-02\n",
      "   5.34428358e-02  2.86488146e-01 -3.00379563e-02 -4.31574415e-03\n",
      "   1.42691983e-02 -2.72330910e-01  1.84161827e-01 -3.21028322e-01\n",
      "  -1.57563090e-02  1.43873215e-01  6.91008940e-02  2.25662012e-02\n",
      "   7.33844563e-02 -5.94415605e-01  9.29291621e-02 -3.82539004e-01\n",
      "   2.44259927e-02  3.61744791e-01 -3.22392255e-01  5.74960038e-02\n",
      "  -5.39429784e-01  1.41370758e-01  7.32738450e-02  2.56943144e-02\n",
      "   1.37124166e-01 -1.92796811e-01  4.03660610e-02  4.57467958e-02\n",
      "  -7.21263215e-02  1.57910243e-01 -1.81753919e-01  5.73866162e-03\n",
      "   2.18394443e-01  5.03768981e-01  8.34504887e-02 -2.20524713e-01\n",
      "  -2.61221118e-02 -3.34462851e-01  2.70725917e-02  1.15487374e-01\n",
      "  -9.04312655e-02  2.83468306e-01 -1.59079239e-01  4.31691229e-01\n",
      "   1.66640639e-01 -5.51334061e-02  1.90114513e-01  1.41633272e-01\n",
      "  -3.16261560e-01 -1.00674748e-01 -1.34072348e-01 -3.77071232e-01\n",
      "   3.46790850e-01  2.88731426e-01 -1.91390917e-01  2.76690751e-01\n",
      "   2.85495162e-01  9.66794975e-03 -7.72283003e-02  4.94440570e-02\n",
      "  -1.83544159e-01  5.78330085e-02 -6.51681602e-01  2.76058167e-01\n",
      "  -7.00452104e-02 -2.49790195e-02  2.23135099e-01 -1.21700346e-01\n",
      "   6.76466152e-02 -4.78527397e-01 -8.49619880e-02 -8.91137961e-03\n",
      "  -2.15847746e-01 -1.30860299e-01  1.55468106e-01 -2.13819429e-01\n",
      "   1.29306465e-02 -4.41335961e-02  1.71468705e-01 -6.07402213e-02\n",
      "   2.78392822e-01  6.82281330e-02  5.11239693e-02 -5.65553308e-02\n",
      "   2.63293058e-01  4.43367325e-02  2.61353761e-01  2.86699198e-02\n",
      "  -2.76685327e-01  1.05651908e-01 -2.40517855e-02 -1.14407383e-01\n",
      "  -6.48243055e-02 -3.15210968e-01 -5.40970266e-01  2.29274169e-01\n",
      "   9.54980701e-02 -2.26439953e-01 -2.52014518e-01  6.75504804e-02\n",
      "   5.92279971e-01 -8.25073943e-02 -4.54146490e-02  1.38587564e-01\n",
      "  -1.80672966e-02  1.28688782e-01  2.28648260e-01  3.94542277e-01\n",
      "  -2.19153553e-01 -3.34911942e-02 -4.77698967e-02 -3.56259495e-01\n",
      "  -1.75635274e-02 -4.03865039e-01 -4.15371150e-01 -1.81488708e-01\n",
      "   1.29387170e-01  1.23704709e-01 -2.06078604e-01 -5.45871913e-01\n",
      "  -1.79381505e-01 -3.16396087e-01 -5.81365228e-02  1.93893403e-01\n",
      "  -5.74567676e-01 -2.33864054e-01 -2.15599611e-01  9.91214290e-02\n",
      "   2.89086968e-01 -1.24845266e-01  9.15751047e-03  7.45235905e-02\n",
      "  -3.91964393e-04  1.50950104e-01 -3.16433728e-01  7.77892992e-02\n",
      "  -1.11134931e-01  3.33259314e-01  9.66479853e-02  3.06638986e-01\n",
      "  -1.49429291e-02 -8.06261748e-02  3.11280310e-01  1.83992997e-01\n",
      "   1.60052568e-01  3.54117826e-02 -1.60683870e-01  4.72488366e-02\n",
      "   9.96773615e-02 -8.06597713e-03  1.34799749e-01  3.51539582e-01\n",
      "   2.24098280e-01 -2.31395945e-01 -3.65943640e-01 -7.83854537e-03\n",
      "   2.84377038e-01 -2.45100725e-02  7.73569616e-03  4.46129544e-03\n",
      "   1.19799651e-01  1.88376278e-01  1.04297228e-01 -1.66100740e-01\n",
      "   1.11056820e-01  1.03396691e-01 -2.79940903e-01 -9.58340839e-02\n",
      "   1.65699109e-01  7.03751715e-03  2.77101576e-01  4.65342104e-01\n",
      "  -1.23015508e-01 -1.85533315e-02  2.70202458e-01 -2.60436296e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#EntraÃ®nÃ© avec un objectif de dÃ©tection de paraphrases sur un corpus multilingue.\n",
    "#Performances Ã©quilibrÃ©es pour la similaritÃ© sÃ©mantique, la recherche dâ€™information et la classification zero-shot en plusieurs langues.\n",
    "\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "print(model.encode([\"Texte Ã  encoder\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2bed45-6703-4228-8c0e-0815eb36f7b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### SimilaritÃ© sÃ©mantique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "caab2ac4-816b-4b6e-965c-656338c708e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.37034696)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_A = np.linalg.norm(A)\n",
    "    norm_B = np.linalg.norm(B)\n",
    "    return dot_product / (norm_A * norm_B)\n",
    "\n",
    "cosine_similarity(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7e39c-c1af-4389-a8d1-5266facf70e4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Embeddings OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04fc4801-ba9c-49ce-a405-cc15b8480213",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5604925298797377)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "def embed(text, model=\"text-embedding-3-large\", dimensions=3072): #3072: dimension maximale\n",
    "    return openai.embeddings.create(input = [text], model=model, dimensions=dimensions).data[0].embedding\n",
    "\n",
    "vector1 = embed(\"What is Mycobacterium kansasii ?\")\n",
    "vector2 = embed(\"To sum up, we have presented a case of Mycobacterium kansasii monoarthritis of the elbow complicated with unusual clinical and radiological findings. A combination of synovectomy and multidrug antimycobacterial treatment yielded a favorable clinical course without recurrence of arthritis after 10 months of follow-up. This case emphasizes the need to consider this rare infection in the differential diagnosis of intra-articular soft tissue tumor-like lesions of the elbow even in immunocompetent patients.\")\n",
    "cosine_similarity(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f18a96-816d-4c9b-a34b-e6d50220b36f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### RAG : principe de base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283ac8c-eb44-49d9-afe1-a83df312fc48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/rag2.png\" alt=\"RAG\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d363528a-e2de-42b6-b552-2d8d5642df6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mycobacterium kansasii\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "llm = ChatMistralAI(model_name=\"mistral-large-latest\")\n",
    "\n",
    "query = \"What is Mycobacterium kansasii ?\"\n",
    "context = \"To sum up, we have presented a case of Mycobacterium kansasii monoarthritis of the elbow complicated with unusual clinical and radiological findings. A combination of synovectomy and multidrug antimycobacterial treatment yielded a favorable clinical course without recurrence of arthritis after 10 months of follow-up. This case emphasizes the need to consider this rare infection in the differential diagnosis of intra-articular soft tissue tumor-like lesions of the elbow even in immunocompetent patients.\"\n",
    "\n",
    "text = f\"\"\"You are an expert in the Mycobacterium field. \n",
    "Answer to the following question by only using the context below.\n",
    "\n",
    "question: {query}\n",
    "\n",
    "context : {context}\"\"\"\n",
    "\n",
    "response = llm.invoke(text)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d552db-cd59-49c8-9059-8ef5816aebc7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### ImplÃ©mentation d'un vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0dab196-c19e-4681-8851-eaacbdf7e7e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install langchain-community langchain-openai faiss-cpu\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "loader = PyPDFLoader(\"images/Guyeux_2024.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "faiss_index = FAISS.from_documents(pages, embeddings)\n",
    "docs = faiss_index.similarity_search(\"Is there a lineage 10 in M.tuberculosis?\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39a2555d-7d51-43fc-b060-a9890b8aefe6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 3: M. africanum Lineage 10, Central Africa Conclusions Through the extensive mining\n",
      "of WGS and genotyp- ing databases, we newly identified a thus far rare M.\n",
      "tuberculosis complex lineage, L10 (proposed), pres- ent in central Africa. The\n",
      "lineage is characterized by a new region of deletion, IS6110 insertions, and 243\n",
      "SNPs, including gyrA G7901T, recN C1920096T, and dnaG C2621730T. L10 represents\n",
      "a sister clade to L6, found mainly in western Africa, and L9, specifically in\n",
      "eastern Africa, and [...]\n",
      "\n",
      "Page 0: nity of Lille, Lille, France (P. Supply, C. Gaudin); London School of Hygiene\n",
      "and Tropical Medicine, London, UK (J.E. Phelan, T.G. Clark, L. Rigouts, B. de\n",
      "Jong); UniversitÃ© Paris-Saclay, Saint- Aubin, France (C. Sola); UniversitÃ© Paris\n",
      "CitÃ©, Paris (C. Sola) DOI: https://doi.org/10.3201/eid3003.231466 Analysis of\n",
      "genome sequencing data from >100,000 genomes of Mycobacterium tuberculosis\n",
      "complex using TB-Annotator software revealed a previously unknown lineage,\n",
      "proposed name L10, in central [...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textwrap import shorten, fill\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"Page {doc.metadata[\"page\"]}: {fill(shorten(doc.page_content, 500), 80)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b272e73a-28b8-485a-bcce-948f1713cebd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Version OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a429dce-16c1-4744-a246-8c7c63fe4398",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install langchain-community langchain-openai faiss-cpu\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "loader = PyPDFLoader(\"images/Guyeux_2024.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())\n",
    "docs = faiss_index.similarity_search(\"Is there a lineage 10 in M.tuberculosis?\", k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32045cf8-3161-48da-9f0c-bdb3617f2aba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "###Â Text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1efcc3d-7d8e-457c-a1a0-6a1786634749",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = '''Vous pouvez partager un article en cliquant sur les icÃ´nes de partage en haut Ã  droite de celui-ci. \n",
    "La reproduction totale ou partielle dâ€™un article, sans lâ€™autorisation Ã©crite et prÃ©alable du Monde, est strictement interdite. \n",
    "Pour plus dâ€™informations, consultez nos conditions gÃ©nÃ©rales de vente. \n",
    "\n",
    "Comme la finance, la politique est parfois affaire dâ€™opportunitÃ©s. Aux Etats-Unis, lâ€™opposition dÃ©mocrate Ã  Donald Trump a en tout cas trouvÃ© un nouvel angle dâ€™attaque aprÃ¨s lâ€™annonce par le prÃ©sident amÃ©ricain dâ€™une pause dans sa guerre commerciale : elle le soupÃ§onne dâ€™avoir manipulÃ© les marchÃ©s boursiers et dâ€™avoir ainsi favorisÃ© des dÃ©lits dâ€™initiÃ©.\n",
    "Lire aussi | Article rÃ©servÃ© Ã  nos abonnÃ©s Droits de douane : les Bourses rechutent, lâ€™inquiÃ©tude sâ€™Ã©tend aux emprunts dâ€™Etat\n",
    "\n",
    "Le sÃ©nateur Adam Schiff a Ã©crit, jeudi 10 avril, au directeur par intÃ©rim du Bureau pour lâ€™Ã©thique gouvernementale (Office of Government Ethics, OGE), une agence fÃ©dÃ©rale indÃ©pendante, et Ã  Susan Wiles, la cheffe de cabinet de la Maison Blanche, pour leur demander dâ€™ouvrir une enquÃªte Â« urgente Â» afin de dÃ©terminer si Â« le prÃ©sident Trump, sa famille ou dâ€™autres membres de [son] administration Â» ont commis la veille des dÃ©lits dâ€™initiÃ© en profitant dâ€™informations confidentielles sur le revirement de sa politique commerciale.\n",
    "\n",
    "'''\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=200,\n",
    "    keep_separator=False,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \"]\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([text])\n",
    "\n",
    "for k in texts[:7]:\n",
    "    print(k.page_content)\n",
    "    print(\"=\"*20+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed54b26-02a2-4d34-b410-b2aed554385a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Loaders (LangChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c28c1c-6e98-4aa6-8a37-e5ccd72331d6",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=YcIbZGTRMjI\", \n",
    "    language=['fr'],\n",
    "    add_video_info=False\n",
    ")\n",
    "\n",
    "print(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02172e7d-480e-4013-af70-b7e12e46cc2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Vectorstores\n",
    "\n",
    "Nombreux et multiples...\n",
    " - FAISS, Chroma : faciles Ã  maÃ®triser, dÃ©ployer...\n",
    " - Milvus : multi-embeddings, BM25, filtrage par colonne...\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/Milvus.png\" alt=\"RAG\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ff316-e795-4cb6-9ac6-4edd4437de66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Les agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f8c84e-b005-4dd3-a906-a49202986f80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Un agent est un systÃ¨me autonome alimentÃ© par un modÃ¨le de langage (comme GPT-4) qui prend des dÃ©cisions sur les actions Ã  entreprendre en fonction des donnÃ©es d'entrÃ©e et des instructions programmÃ©es.\n",
    "\n",
    "Fonction :\n",
    "- Prise de dÃ©cision : L'agent analyse les donnÃ©es d'entrÃ©e et utilise des algorithmes et des modÃ¨les pour dÃ©cider quelle action entreprendre.\n",
    "- ExÃ©cution d'actions : L'agent peut effectuer diverses actions comme rÃ©pondre Ã  une question, rechercher des informations, ou interagir avec d'autres systÃ¨mes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cce33f-8d83-4d9a-9af5-46e06c5526f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Un \"agent\", c'est un LLM avec des \"outils\" :\n",
    " - recherche sur internet,\n",
    " - calculatrice,\n",
    " - interrogation de pdf (RAG),\n",
    " - outil fait maison\n",
    " - ...\n",
    "\n",
    "Le mieux est de faire des agents spÃ©cialisÃ©s, et de les orchestrer ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0fab8a-ede8-4811-bb30-4cfe0b73bafd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Des outils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8355b7-335d-4132-9ee7-598a13f1acbb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea0ea3e-c8fb-4da4-bb41-0ca8a623ff15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install --upgrade --quiet  wikipedia\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "wikipedia.run(\"Alan Turing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f9250e-68a5-43f5-bacd-01ee95a6ea2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Tavili (recherche internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd80876-c248-4d29-a73e-b361472ba727",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install -qU langchain-tavily\n",
    "# Pour une clÃ© d'API : https://www.tavily.com/\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "import os\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "search = TavilySearchResults(max_results=2)\n",
    "search_results = search.invoke(\"Quel est le temps Ã  Belfort ?\")\n",
    "print(search_results[0]['content'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e996e910-9b21-41e6-ac8f-81dff60aa4f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Agents LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6996f99-d3bd-4903-879c-c16338141e43",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### ExÃ©cuteur d'agent (Agent Executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75632036-6402-4c73-a35f-64989fce0a04",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "L'exÃ©cuteur d'agent est un composant ou un systÃ¨me qui orchestre et exÃ©cute les actions dÃ©terminÃ©es par l'agent.\n",
    "\n",
    "Fonction :\n",
    " - Gestion de l'exÃ©cution : Il reÃ§oit les dÃ©cisions de l'agent, exÃ©cute les actions correspondantes et gÃ¨re la transition entre diffÃ©rentes Ã©tapes de l'exÃ©cution.\n",
    " - Traitement des rÃ©sultats : Il collecte les rÃ©sultats des actions exÃ©cutÃ©es et les transmet Ã  l'agent pour de nouvelles dÃ©cisions ou Ã  l'utilisateur final.\n",
    "\n",
    "Exemple : Dans un systÃ¨me de recommandation, l'exÃ©cuteur d'agent pourrait orchestrer l'appel Ã  diffÃ©rentes API pour recueillir les informations nÃ©cessaires (comme les prÃ©fÃ©rences de l'utilisateur et les donnÃ©es sur les produits) et les combiner pour gÃ©nÃ©rer une recommandation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423be8f8-8f9b-48c6-a031-698ac8d13ef8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Avec outil Tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c599b022-cda1-41e3-944f-ac1718152a89",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "search = TavilySearchResults(max_results=2)\n",
    "tools = [search]\n",
    "\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "\n",
    "prompt = hub.pull(\"amalnuaimi/react-mistral\")\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "# Ajout de max_iterations pour Ã©viter les boucles infinies\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, max_iterations=5)\n",
    "\n",
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        'input': \"Dois-je prendre un parapluie, sachant que je me rends aujourd'hui et demain Ã  Belfort ?\",\n",
    "        'chat_history': []\n",
    "    })\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0593da3-5ab3-475e-adb9-f077fc50ab03",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://smith.langchain.com/hub\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a201966-1ebe-48a7-9464-b086c0361740",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Version OpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "search = TavilySearchResults(max_results=2)\n",
    "tools = [search]\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1\")\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        'input': \"Dois-je prendre un parapluie, sachant que je me rends aujourd'hui et demain Ã  Belfort ?\"\n",
    "    })\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d37196e-8e45-4dcc-b3f9-5fb91fbc52c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Avec outil arXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c6254-6f0e-4bfd-bc77-46e078c6a6da",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install arxiv\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "\n",
    "#llm = ChatOpenAI(temperature=0.0)\n",
    "llm = ChatMistralAI(model=\"mistral-large-latest\", temperature=0)\n",
    "tools = load_tools([\"arxiv\"])\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"RÃ©sume l'article 1605.08386 en franÃ§ais\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b151aecf-48ed-4042-9363-b812319db351",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f045ba-f698-474b-b3eb-6186b55de946",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tools[0].name)\n",
    "print(tools[0].description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8178e85-cb4b-4612-aa5b-34e0817f79dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "####Â Avec Python REPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f5469e-5034-4a73-a1e2-f19b6c23acdc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "tools = [PythonREPLTool()]\n",
    "\n",
    "instructions = \"\"\"You are an agent designed to write and execute python code to answer questions.\n",
    "You have access to a python REPL, which you can use to execute python code.\n",
    "If you get an error, debug your code and try again.\n",
    "Only use the output of your code to answer the question. \n",
    "You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "\"\"\"\n",
    "\n",
    "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)\n",
    "agent = create_openai_functions_agent(ChatOpenAI(temperature=0), tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8950531-534f-402a-b634-a474c6029525",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"Quel est le milliÃ¨me nombre de Fibonacci ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b658b229-7e6f-46f7-95ba-9e09cff78cd8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2b8b2-4ebe-4f1a-aec2-0a941ae3f4f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Plusieurs outils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb01ef0-a7e7-4d50-9d04-35f46503de2d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, create_react_agent\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "tools = load_tools([\"llm-math\", \"wikipedia\"], llm=llm)\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, \n",
    "                               tools=tools, \n",
    "                               handle_parsing_errors=True, \n",
    "                               verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281cf6c1-bb0c-4b45-847d-f404800ae22a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor.invoke({'input': \"Qu'est-ce que 25% de 300?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93bab48-86d3-43ba-b43e-503d69013b0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "###Â Ses propres outils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6700d8-15cb-4946-83af-a945ed28a613",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"Multiplie deux entiers.\"\"\"\n",
    "    return first_int * second_int\n",
    "\n",
    "print(multiply.name)\n",
    "print(multiply.description)\n",
    "print(multiply.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5432395-1131-407c-82c3-069904e9fba2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(first_int: int, second_int: int) -> int:\n",
    "    \"Ajoute deux entiers.\"\n",
    "    return first_int + second_int\n",
    "\n",
    "@tool\n",
    "def exponentiate(base: int, exponent: int) -> int:\n",
    "    \"Calcule la puissance d'un entier donnÃ©.\"\n",
    "    return base**exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28db36-9235-4256-906f-3b9645685cb2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4.1\")\n",
    "\n",
    "tools = [multiply, add, exponentiate]\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Porter 3 Ã  la puissance 5 et multiplier le rÃ©sultat par la somme de douze et de trois, puis Ã©lever le tout au carrÃ©.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57718046-2846-4ec5-8b77-7098ac55fe81",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57b020-d22f-48b1-9bae-110d80e1e376",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Assemblage d'agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712aefa-4e26-4b1e-9ab8-e5d1ba99f8a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![Texte alternatif](images/promptulate.png \"Promptulate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a202d00c-765d-420d-bae9-17fc5f3a6c6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## L'audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af0ac02-c87a-44f0-9b63-ae0560619bb3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896cded-5641-402a-b7c8-2e9a62020ad9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "response = OpenAI().audio.speech.create(\n",
    "  model=\"tts-1-hd\",\n",
    "  voice=\"onyx\", # alloy, onyx, fable, echo\n",
    "  input=\"Coucou, comment allez-vous ?\"\n",
    ")\n",
    "response.with_streaming_response.method('mon_audio.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf0222f-716e-4d3f-9f07-957ab65bd491",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e6de8-2eeb-44e2-88e3-28be8b4f579c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OpenAI().audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=open(\"mon_audio.mp3\", \"rb\"),\n",
    "  language=\"fr\"\n",
    ").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee9c72-f087-4909-b072-43f6f6811c9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydub import AudioSegment\n",
    "import time\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "song = AudioSegment.from_mp3(\"mon_fichier.mp3\")\n",
    "transcription = ''\n",
    "for pas in range(0, 120, 20):\n",
    "    debut, fin = pas * 60 * 1000, (pas+20) * 60 * 1000\n",
    "    extrait = song[debut:fin]\n",
    "    if len(extrait) > 100:\n",
    "        try:\n",
    "            extrait.export(f\"extrait_{pas}.mp3\", format=\"mp3\")\n",
    "            audio_file = open(f\"extrait_{pas}.mp3\", \"rb\")\n",
    "            # Ajout d'un timeout pour Ã©viter les blocages\n",
    "            start_time = time.time()\n",
    "            result = client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\", \n",
    "                file=audio_file,\n",
    "                timeout=30  # 30 secondes de timeout\n",
    "            )\n",
    "            transcription += result.text\n",
    "            print(f\"Segment {pas} traitÃ© en {time.time() - start_time:.2f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement du segment {pas}: {e}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e0f2b0-c61a-4411-86c5-448410a9dfc4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0ca3a26-a54d-4b78-a330-3ece1415e665",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image_to_data_url(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Lit l'image et renvoie une data URL prÃªte Ã  Ãªtre insÃ©rÃ©e dans le payload.\n",
    "    \"\"\"\n",
    "    # Lecture du fichier image\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        image_bytes = img_file.read()  # Lecture binaire du contenu\n",
    "    # Encodage base64\n",
    "    b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "    # DÃ©tection du MIME type selon l'extension\n",
    "    ext = os.path.splitext(image_path)[1].lower().lstrip(\".\")\n",
    "    mime = f\"image/{ext if ext != 'jpg' else 'jpeg'}\"\n",
    "    return f\"data:{mime};base64,{b64}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93ae8a23-f595-4254-8b97-f6798878394d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[0;32m----> 5\u001b[0m data_url \u001b[38;5;241m=\u001b[39m \u001b[43mencode_image_to_data_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmon_image.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      8\u001b[0m     {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     }\n\u001b[1;32m     15\u001b[0m ]\n",
      "Cell \u001b[0;32mIn[33], line 13\u001b[0m, in \u001b[0;36mencode_image_to_data_url\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m b64 \u001b[38;5;241m=\u001b[39m base64\u001b[38;5;241m.\u001b[39mb64encode(image_bytes)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# DÃ©tection du MIME type selon l'extension\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m ext \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(image_path)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m mime \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mext\u001b[38;5;250m \u001b[39m\u001b[38;5;241m!=\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpeg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;base64,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb64\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "data_url = encode_image_to_data_url(\"mon_image.png\")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Que vois-tu sur cette image ?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998bf937-6eeb-41fb-94f3-6bb4621264bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=messages,\n",
    "        max_tokens=500,\n",
    "        temperature=0.0,\n",
    "        timeout=30  # 30 secondes de timeout\n",
    "    )\n",
    "    response.choices[0].message.content\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de l'appel API: {e}\")\n",
    "    \"Erreur lors de la gÃ©nÃ©ration de la rÃ©ponse\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "but3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
