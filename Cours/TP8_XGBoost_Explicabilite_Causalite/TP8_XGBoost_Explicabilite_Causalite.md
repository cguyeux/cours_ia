# TP 8 : XGBoost ‚Äì Explicabilit√© et Causalit√©

[‚¨ÖÔ∏è Retour au README](../../README.md)

## Objectifs p√©dagogiques

- Comprendre les diff√©rents indicateurs d'importance de variables fournis par XGBoost et savoir les visualiser.
- Interpr√©ter le sens des mesures `weight`, `gain` et `cover` et construire un classement pond√©r√© des variables.
- Expliquer une pr√©diction individuelle √† l'aide des valeurs de Shapley (SHAP) et communiquer les r√©sultats √† l'√©crit.
- Manipuler une biblioth√®que d'inf√©rence causale et relier corr√©lations observ√©es et effets causaux estim√©s.
- Mettre en pratique ces notions sur des exemples concrets et proposer ses propres analyses.

## 1. Importance des variables avec `feature_importances_`

Dans la plupart des API de haut niveau (scikit-learn, `xgboost.sklearn`), les mod√®les XGBoost exposent un attribut `feature_importances_`. Celui-ci contient, pour chaque variable, la **proportion du gain total** accumul√© par les arbres qui utilisent cette variable lors des splits. Plus pr√©cis√©ment :

1. Lors de chaque it√©ration, XGBoost ajoute un arbre qui r√©duit la fonction de perte (`objective`).
2. Chaque split dans l'arbre est √©valu√© en fonction du **gain de perte** qu'il procure : la r√©duction de la perte totale gr√¢ce √† ce split.
3. Le gain obtenu est cr√©dit√© √† la variable utilis√©e. √Ä la fin de l'entra√Ænement, on additionne et normalise ces gains par variable.

### 1.1 Exemple pratique

```python
import xgboost as xgb
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

# Charger un jeu de donn√©es supervis√©
X, y = load_breast_cancer(return_X_y=True, as_frame=True)
X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Entra√Æner un mod√®le XGBoost de type sklearn
modele = xgb.XGBClassifier(
    n_estimators=300,
    max_depth=4,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric="logloss",
    random_state=42,
)
modele.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)

# R√©cup√©rer l'importance des variables
importances = modele.feature_importances_
```

### 1.2 Affichage tabulaire et graphique

```python
import pandas as pd

fi = (
    pd.Series(importances, index=X.columns)
    .sort_values(ascending=False)
    .rename("gain_normalise")
)
print(fi.head(10))
```

Pour un graphique pr√™t √† l'emploi, utilisez la fonction utilitaire d'XGBoost :

```python
xgb.plot_importance(modele, importance_type="gain", max_num_features=15)
```

Le param√®tre `importance_type` contr√¥le la m√©trique affich√©e (`gain` par d√©faut). La figure montre la contribution relative de chaque variable √† la r√©duction de la perte totale. Les barres sont normalis√©es pour sommer √† 1.

### 1.3 Interpr√©tation

- Les variables avec un gain √©lev√© sont celles qui, collectivement, ont le plus aid√© √† r√©duire la perte. Elles sont souvent de bons candidats pour l'analyse.
- Une importance proche de z√©ro indique que la variable a √©t√© tr√®s peu, voire jamais, utilis√©e dans les splits.
- Attention : cette mesure refl√®te l'**importance globale** moyenne sur l'entra√Ænement. Elle ne capture pas l'effet sur des observations individuelles ni les interactions complexes entre variables.

## 2. Comprendre `weight`, `gain` et `cover`

Lorsque l'on acc√®de directement √† l'objet `Booster` (via `modele.get_booster()`), XGBoost fournit trois indicateurs compl√©mentaires :

- **`weight`** : nombre de fois o√π la variable appara√Æt dans un split (tous arbres confondus). Indique la fr√©quence d'utilisation.
- **`gain`** : gain moyen de perte apport√© par les splits utilisant cette variable. Refl√®te la qualit√© des splits.
- **`cover`** : nombre moyen d'√©chantillons (ou poids) concern√©s par les splits utilisant cette variable. Mesure la ¬´ couverture ¬ª des donn√©es.

### 2.1 R√©cup√©ration des trois indicateurs

```python
booster = modele.get_booster()
importance_weight = booster.get_score(importance_type="weight")
importance_gain = booster.get_score(importance_type="gain")
importance_cover = booster.get_score(importance_type="cover")
```

Les dictionnaires retourn√©s ont pour cl√©s les noms internes des variables (`f0`, `f1`, ...). Pour les relier aux noms d'origine :

```python
feature_names = booster.feature_names
mapping = {f"f{i}": name for i, name in enumerate(feature_names)}
```

### 2.2 Construction d'un tableau synth√©tique

```python
import pandas as pd

importance_df = (
    pd.DataFrame({
        "feature": feature_names,
        "weight": [importance_weight.get(f"f{i}", 0) for i in range(len(feature_names))],
        "gain": [importance_gain.get(f"f{i}", 0.0) for i in range(len(feature_names))],
        "cover": [importance_cover.get(f"f{i}", 0.0) for i in range(len(feature_names))],
    })
    .assign(
        gain_total=lambda df: df["gain"] * df["weight"],
        cover_total=lambda df: df["cover"] * df["weight"],
    )
)
```

Dans cet exemple :

- `gain_total` = somme des gains pour tous les splits (gain moyen √ó nombre de splits).
- `cover_total` = nombre total d'exemples couverts.

### 2.3 Classement pond√©r√© personnalis√©

Pour obtenir un score composite, on peut combiner ces indicateurs. Par exemple :

```python
from sklearn.preprocessing import minmax_scale

importance_df = importance_df.assign(
    score_combine=lambda df: (
        0.5 * minmax_scale(df["gain_total"]) +
        0.3 * minmax_scale(df["cover_total"]) +
        0.2 * minmax_scale(df["weight"])
    )
)

classement = importance_df.sort_values("score_combine", ascending=False)
print(classement.head(10))
```

Cette moyenne pond√©r√©e privil√©gie les variables qui g√©n√®rent un fort gain total, couvrent beaucoup d'observations et sont r√©guli√®rement utilis√©es. Les pond√©rations (0.5 / 0.3 / 0.2) sont √† ajuster selon les priorit√©s m√©tier.

### 2.4 Limites et bonnes pratiques

- `weight` favorise les variables utilis√©es dans de nombreux splits, m√™me si les gains sont faibles.
- `gain` peut √™tre domin√© par quelques splits tr√®s profitables mais rares.
- `cover` met en avant les variables qui impactent de larges segments des donn√©es.
- Croiser ces trois points de vue permet de mieux comprendre le r√¥le global des variables dans le mod√®le.

## 3. Explications locales avec les valeurs SHAP

Les **SHAP values** (SHapley Additive exPlanations) s'appuient sur la th√©orie des jeux coop√©ratifs. Pour une observation donn√©e :

- On consid√®re chaque variable comme un ¬´ joueur ¬ª qui contribue √† la pr√©diction.
- La valeur SHAP mesure la contribution moyenne marginale de cette variable en comparant tous les sous-ensembles possibles de variables.
- La somme des valeurs SHAP + la valeur de base (baseline) = pr√©diction du mod√®le.

### 3.1 Calculer les valeurs SHAP pour XGBoost

Installez au pr√©alable la biblioth√®que :

```bash
pip install shap
```

```python
import shap
import numpy as np

explainer = shap.TreeExplainer(modele)  # fonctionne pour les mod√®les bas√©s sur des arbres
shap_values = explainer.shap_values(X_valid)
base_value = explainer.expected_value
baseline_proba = 1 / (1 + np.exp(-base_value))
```

Pour une classification binaire, `shap_values` est un tableau de la m√™me taille que `X_valid` (n_observations √ó n_features). Chaque ligne donne la contribution sign√©e de chaque variable √† la pr√©diction en log-odds (sortie brute). `base_value` correspond √† la moyenne des sorties du mod√®le sur le jeu d'entra√Ænement.

### 3.2 Exemple d√©taill√© d'une pr√©diction

```python
import numpy as np

idx = 0  # choisir une observation de validation
x = X_valid.iloc[idx]
y_pred_proba = modele.predict_proba(X_valid.iloc[[idx]])[0, 1]
y_pred_logodds = modele.predict(X_valid.iloc[[idx]], output_margin=True)[0]

contributions = pd.Series(shap_values[idx], index=X.columns)
print("Valeur de base (log-odds):", base_value)
print("Probabilit√© moyenne associ√©e:", baseline_proba)
print("Somme contributions:", contributions.sum())
print("Log-odds pr√©dit:", y_pred_logodds)
print("Probabilit√© pr√©dite (apr√®s sigmo√Øde):", y_pred_proba)
```

La sortie montre que :

- `base_value` est la pr√©diction moyenne en log-odds.
- `contributions.sum()` est exactement √©gale au log-odds pr√©dit.
- Chaque valeur SHAP indique de combien la variable d√©cale la pr√©diction par rapport √† la base (positif = augmente la proba, n√©gatif = la r√©duit).

### 3.3 Visualiser et lire les graphiques SHAP

1. **Waterfall plot** (explication d'une observation) :

    ```python
    shap_explication = shap.Explanation(
        values=shap_values[idx],
        base_values=base_value,
        data=x,
        feature_names=X.columns,
    )
    shap.plots.waterfall(shap_explication)
    ```

    √Ä lire de gauche √† droite : la base, puis les contributions positives (rouge) et n√©gatives (bleu) jusqu'√† la sortie.

2. **Summary plot** (vue globale) :

    ```python
    shap.summary_plot(shap_values, X_valid)
    ```

    - L'axe horizontal indique l'impact (SHAP value).
    - La couleur encode la valeur de la variable (rouge = √©lev√©e, bleu = faible).
    - Les points les plus √† droite sont ceux qui augmentent fortement la pr√©diction.

3. **Dependence plot** (interaction variable) :

    ```python
    shap.dependence_plot("mean radius", shap_values, X_valid)
    ```

    Il montre comment la contribution d'une variable √©volue en fonction de sa valeur et, en couleur, l'une des variables les plus corr√©l√©es (interaction).

### 3.4 G√©n√©rer une explication textuelle

Une fois les contributions calcul√©es, vous pouvez produire un r√©sum√© en langage naturel, par exemple en utilisant un prompt destin√© √† un mod√®le de langage :

```
Contexte : mod√®le XGBoost de classification du cancer du sein.
Observation : {valeurs des variables cl√©s}
Baseline (log-odds) : {base_value:.3f} => probabilit√© moyenne {baseline_proba:.2%}.
Contributions principales :
- {feature_1} : +{shap_1:.3f} (valeur observ√©e {x1}), augmente la probabilit√© car ...
- {feature_2} : -{shap_2:.3f} (valeur observ√©e {x2}), r√©duit la probabilit√© car ...
Pr√©diction finale : log-odds {logodds:.3f} => probabilit√© {proba:.2%}.
Explique en 5 phrases simples √† un public non sp√©cialiste.
```

Il suffit d'alimenter les espaces `{...}` avec les r√©sultats de l'analyse pour obtenir un texte coh√©rent. Cette approche encourage les √©tudiantes et √©tudiants √† traduire des valeurs num√©riques en arguments compr√©hensibles.

> üí° `baseline_proba` peut √™tre calcul√© via `baseline_proba = 1 / (1 + np.exp(-base_value))`, ce qui correspond √† la probabilit√© moyenne pr√©dite par le mod√®le.

## 4. Introduction √† la causalit√© avec DoWhy

La corr√©lation mesur√©e par les importances ou SHAP ne garantit pas un **lien causal**. Pour raisonner sur des sc√©narios contrefactuels (¬´ que se passerait-il si... ¬ª), on s'appuie sur l'inf√©rence causale.

[DoWhy](https://github.com/py-why/dowhy) est une biblioth√®que Python moderne qui propose un cadre en quatre √©tapes : mod√©liser, identifier, estimer et r√©futer.

### 4.1 Installation

```bash
pip install dowhy econml graphviz pygraphviz
```

### 4.2 Exemple concret : impact d'une campagne marketing

Supposons que l'on dispose d'un jeu de donn√©es avec :

- `traitement` : indicateur (0/1) que la cliente a re√ßu une campagne marketing.
- `revenu` : revenu annuel de la cliente.
- `anciennete` : anciennet√© dans le programme fid√©lit√©.
- `depense` : montant d√©pens√© apr√®s la campagne (variable cible).

Nous cherchons l'effet causal de `traitement` sur `depense`, en contr√¥lant les variables de confusion (`revenu`, `anciennete`).

```python
import pandas as pd
import numpy as np
from dowhy import CausalModel

n = 2000
rng = np.random.default_rng(0)
revenu = rng.normal(45000, 10000, size=n)
anciennete = rng.integers(1, 8, size=n)
# Les clientes avec un revenu √©lev√© re√ßoivent plus souvent la campagne
propension = 1 / (1 + np.exp(-0.00008 * (revenu - 42000) + 0.3 * (anciennete - 3)))
traitement = rng.binomial(1, propension)
# Effet causal r√©el : +80 ‚Ç¨ en moyenne
bruit = rng.normal(0, 150, size=n)
depense = 500 + 0.05 * revenu + 15 * anciennete + 80 * traitement + bruit

df = pd.DataFrame({
    "revenu": revenu,
    "anciennete": anciennete,
    "traitement": traitement,
    "depense": depense,
})
```

### 4.3 Sp√©cifier le graphe causal

```python
modele_causal = CausalModel(
    data=df,
    treatment="traitement",
    outcome="depense",
    graph="""
        digraph {
            revenu -> traitement;
            revenu -> depense;
            anciennete -> traitement;
            anciennete -> depense;
            traitement -> depense;
        }
    """
)
modele_causal.view_model(layout="dot")  # n√©cessite graphviz
```

Le graphe encode les relations causales suppos√©es. Les arcs partent des causes vers les effets.

### 4.4 Identification et estimation

```python
identification = modele_causal.identify_effect()
print(identification)

estimateur = modele_causal.estimate_effect(
    identification,
    method_name="backdoor.propensity_score_matching",
)
print("Effet moyen du traitement (ATE) estim√©:", estimateur.value)
```

Ici, DoWhy applique un ajustement par score de propension (`propensity_score_matching`). Si le mod√®le est correctement sp√©cifi√©, l'ATE estim√© doit se rapprocher de la valeur r√©elle (‚âà 80 ‚Ç¨).

### 4.5 Tests de r√©futation

DoWhy propose des tests pour v√©rifier la robustesse de l'estimation :

```python
refutation = modele_causal.refute_estimate(
    identification,
    estimateur,
    method_name="placebo_treatment_refuter",
)
print(refutation)
```

- **Placebo treatment** : remplace le traitement par une variable al√©atoire ; l'effet doit alors √™tre proche de 0.
- D'autres m√©thodes (`data_subset_refuter`, `add_unobserved_common_cause`) permettent de tester la sensibilit√© aux hypoth√®ses.

### 4.6 Lien avec l'explicabilit√©

- Les importances et SHAP nous informent sur **comment** le mod√®le se sert des variables pour pr√©dire.
- L'analyse causale r√©pond √† **ce qui se passerait** si nous intervenions sur une variable (par exemple envoyer ou non une campagne).
- Combiner les deux approches aide √† prioriser les actions : une variable peut √™tre importante pour la pr√©diction mais avoir un effet causal faible (corr√©lation due √† un facteur commun).

## Exercices

1. **Importances multiples** : Entra√Ænez un `XGBRegressor` sur le jeu de donn√©es `CaliforniaHousing`. Comparez les importances `gain`, `weight` et `cover`. Quels groupes de variables ressortent selon chaque m√©trique ?
2. **Score personnalis√©** : Proposez votre propre combinaison pond√©r√©e des trois indicateurs d'importance. Justifiez vos choix et identifiez les variables prioritaires pour un audit.
3. **SHAP d√©taill√©** : Choisissez deux observations (une pr√©diction correcte, une incorrecte) et r√©alisez une explication SHAP compl√®te : tableau de contributions, waterfall plot, r√©sum√© textuel.
4. **Sensibilit√© aux interactions** : Utilisez `shap.dependence_plot` pour explorer l'interaction entre deux variables. Comment interpr√©ter les variations de contributions ?
5. **Exp√©rience causale** : Reprenez l'exemple DoWhy et modifiez le g√©n√©rateur de donn√©es pour introduire une variable omise (non observ√©e) influen√ßant √† la fois le traitement et l'issue. Observez l'impact sur l'estimation de l'ATE et discutez de la robustesse des tests de r√©futation.
6. **√âtude de cas** : Proposez un cas m√©tier (ex : recommandation produit) et d√©crivez comment vous utiliseriez XGBoost pour pr√©dire, SHAP pour expliquer, puis DoWhy (ou autre biblioth√®que causale) pour estimer l'effet d'une intervention. Listez les donn√©es n√©cessaires et les hypoth√®ses √† v√©rifier.

Bon TP !
