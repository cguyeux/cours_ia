# TP7.4 ‚Äî Classification d√©s√©quilibr√©e

[‚¨ÖÔ∏è Retour au README](../../README.md)

## Positionnement p√©dagogique
- **Dur√©e indicative** : 3 heures.
- **Niveau** : BUT3 Informatique.
- **Comp√©tences vis√©es** : traitement des jeux de donn√©es d√©s√©quilibr√©s, choix de m√©triques adapt√©es, param√©trage d'algorithmes (scikit-learn et XGBoost).

## Objectifs d'apprentissage
1. Diagnostiquer le niveau de d√©s√©quilibre et ses impacts sur les m√©triques habituelles.
2. Mettre en place des techniques de r√©√©chantillonnage et de pond√©ration.
3. Ajuster le seuil de d√©cision et interpr√©ter les courbes ROC/PR.
4. Param√©trer XGBoost pour des classes minoritaires.

## Pr√©paration
1. Recharger les datasets `X_train`, `X_test`, `y_train`, `y_test` pr√©par√©s pr√©c√©demment (stratifi√©s).
2. R√©utiliser le pipeline de pr√©traitement valid√© dans TP7.2 (normalisation + encodage) comme bloc de base.
3. Installer `imblearn` si besoin (`pip install imbalanced-learn`).

## Activit√© 1 ‚Äî Diagnostic et m√©triques (25 min)
1. Calculer la proportion de chaque classe dans le train et le test.
2. Entra√Æner une r√©gression logistique *sans* r√©√©quilibrage et √©valuer : `accuracy`, `precision`, `recall`, `f1`, `balanced_accuracy`, `roc_auc`, `average_precision`.
3. Commenter pourquoi l'accuracy peut √™tre trompeuse dans ce contexte.
4. Visualiser la matrice de confusion et identifier les co√ªts m√©tiers associ√©s aux faux positifs/faux n√©gatifs.

## Activit√© 2 ‚Äî R√©√©chantillonnage (60 min)
1. Tester trois strat√©gies dans une `ImbPipeline` (imblearn) :
   - `RandomOverSampler`
   - `SMOTE`
   - `RandomUnderSampler`
2. Pour chaque strat√©gie, utiliser deux mod√®les : r√©gression logistique et `RandomForestClassifier` (avec `class_weight=None`).
3. Utiliser une validation crois√©e stratifi√©e (5 folds) et comparer `f1`, `recall`, `balanced_accuracy`.
4. Discuter :
   - Comment le sur-√©chantillonnage affecte-t-il le temps d'entra√Ænement ?
   - Quelles m√©thodes semblent le mieux pr√©server la diversit√© des donn√©es ?

> üìà *Visualisation sugg√©r√©e* : courbes Precision-Recall pour chaque strat√©gie.

## Activit√© 3 ‚Äî Pond√©ration des classes (35 min)
1. √âvaluer l'effet de `class_weight="balanced"` pour :
   - R√©gression logistique (`LogisticRegression`)
   - SVM lin√©aire (`LinearSVC`)
2. Comparer les r√©sultats √† ceux de l'Activit√© 2.
3. Introduire la notion de `scale_pos_weight` dans XGBoost :
   ```python
   from xgboost import XGBClassifier

   scale_pos_weight = len(y_train[y_train == "<=50K"]) / len(y_train[y_train == ">50K"])
   model = XGBClassifier(scale_pos_weight=scale_pos_weight, eval_metric="aucpr")
   ```
4. Tester XGBoost avec et sans pond√©ration.

## Activit√© 4 ‚Äî Optimisation du seuil de d√©cision (35 min)
1. √Ä partir du meilleur mod√®le (Activit√© 2 ou 3), r√©cup√©rer les probabilit√©s (`predict_proba`).
2. Tracer la courbe Precision-Recall (`sklearn.metrics.precision_recall_curve`).
3. D√©finir un seuil maximisant le `f1` ou minimisant un co√ªt m√©tier d√©fini par les √©tudiants.
4. Impl√©menter une fonction utilitaire qui, pour un seuil donn√©, renvoie la matrice de confusion, `precision`, `recall`, `f1`.
5. Illustrer l'effet de trois seuils distincts (ex. 0.3, 0.5, 0.7) sur les m√©triques.

## Activit√© 5 ‚Äî Synth√®se XGBoost vs mod√®les lin√©aires (25 min)
1. Comparer deux pipelines :
   - Pr√©traitement complet + `LogisticRegression` avec la meilleure strat√©gie (r√©√©chantillonnage ou pond√©ration) + seuil optimis√©.
   - Pr√©traitement minimal + `XGBClassifier` avec `scale_pos_weight` et early stopping (utiliser `eval_set`).
2. √âvaluer sur le jeu de test : `f1`, `recall`, `roc_auc`, `average_precision`, temps d'entra√Ænement.
3. Discuter des avantages/inconv√©nients :
   - Interpr√©tabilit√© vs performance brute.
   - Sensibilit√© aux hyperparam√®tres.
   - Facilit√© de d√©ploiement.

## Synth√®se attendue
- Tableau comparatif des strat√©gies test√©es (r√©√©chantillonnage, pond√©ration, seuils) avec les m√©triques cl√©s.
- Recommandations (8 √† 10 lignes) :
  - Quelle strat√©gie adopter pour un contexte o√π les faux n√©gatifs sont co√ªteux ?
  - Comment monitorer les performances en production (drift de classe) ?
  - Quels indicateurs suivre en continu (precision@k, recall@k, etc.) ?

## Ressources
- Documentation imbalanced-learn : [https://imbalanced-learn.org/stable/](https://imbalanced-learn.org/stable/)
- Documentation scikit-learn : [Imbalanced datasets](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)
- Article : *Dealing with Imbalanced Data in Machine Learning* (Analytics Vidhya).
