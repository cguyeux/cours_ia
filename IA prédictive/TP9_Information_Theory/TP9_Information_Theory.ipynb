{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69e054d",
   "metadata": {},
   "source": [
    "# Théorie de l'information et analyse de données tabulaires\n",
    "\n",
    "Dans ce notebook, nous allons introduire progressivement des notions fondamentales de théorie de l'information et montrer comment elles peuvent aider à analyser des données tabulaires. Nous aborderons notamment :\n",
    "\n",
    "- **Entropie** : mesure de l'incertitude d'une variable aléatoire (interprétation en nombre moyen de questions oui/non).\n",
    "- **Codage de Huffman** : un exemple de code binaire optimal illustrant le lien entre entropie et longueur moyenne minimale d'un code/décision.\n",
    "- **Entropie conditionnelle et information mutuelle** \\(H(Y), H(Y \\mid X), I(Y;X)\\) avec estimation empirique par discrétisation des données.\n",
    "- **Application à un jeu de données synthétique** (\"arrivées aux urgences\") : variables explicatives (jour, température, épidémie de grippe...) et variable cible (comptage de patients).\n",
    "- **Modélisation prédictive avec XGBoost (objectif Poisson)** : construction d'un modèle de régression pour prédire le nombre de patients, comparaison de modèles à faible vs forte profondeur.\n",
    "- **Interprétation des résultats** : lien entre entropie et profondeur d'arbre (réduction d'incertitude au fil des splits), rôle informatif de chaque variable via l'information mutuelle et mesure de la réduction d'incertitude (lien avec la divergence de Kullback-Leibler).\n",
    "\n",
    "Chaque section comprend des explications pédagogiques et du code Python exécutable illustrant les concepts (calculs d'entropie, construction d'arbres, entraînement XGBoost...). Des visualisations et sorties chiffrées aideront à interpréter les résultats. Enfin, un TP \"À vous de jouer\" vous proposera des manipulations pour explorer les effets de ces notions sur d'autres jeux de données ou paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f49693",
   "metadata": {},
   "source": [
    "## 1. Entropie : mesurer l'incertitude d'une variable aléatoire\n",
    "\n",
    "**Définition.** L'entropie \\(H(X)\\) d'une variable aléatoire discrète \\(X\\) mesure l'incertitude moyenne associée à \\(X\\). Mathématiquement, pour une distribution prenant des valeurs \\(x_i\\) avec probabilités \\(p_i = P(X = x_i)\\), l'entropie se définit par :\n",
    "\n",
    "\\[\n",
    "H(X) = - \\sum_i p_i \\log_2(p_i).\n",
    "\\]\n",
    "\n",
    "L'unité est le **bit** si l'on utilise le logarithme en base 2. Intuitivement, l'entropie correspond au nombre moyen de questions oui/non qu'un observateur doit poser pour deviner la valeur de \\(X\\). Plus \\(X\\) est imprévisible (distribution uniforme par exemple), plus son entropie est élevée. À l'inverse, si \\(X\\) prend toujours la même valeur, son entropie est nulle (aucune incertitude)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd6f27",
   "metadata": {},
   "source": [
    "### Exemples simples\n",
    "\n",
    "- Une pièce équilibrée (pile ou face équiprobables) a une entropie de **1 bit**, car une question oui/non suffit en moyenne à deviner l'issue.\n",
    "- Une pièce biaisée (qui tombe face 70 % du temps) a une entropie plus faible : l'incertitude est partiellement réduite car une issue est favorisée.\n",
    "- Un dé à 6 faces équilibré a une entropie plus élevée (~2.585 bits), car il y a 6 issues équiprobables.\n",
    "- Un dé biaisé (40 % une face, 20 % une autre, le reste à 10 %) présente une entropie intermédiaire.\n",
    "\n",
    "Calculons ces entropies avec Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e1ec06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T09:06:06.595442Z",
     "iopub.status.busy": "2025-10-15T09:06:06.595278Z",
     "iopub.status.idle": "2025-10-15T09:06:06.696258Z",
     "shell.execute_reply": "2025-10-15T09:06:06.695691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropie pièce équilibrée   : 1.0 bits\n",
      "Entropie pièce biaisée 70/30 : 0.8812908992306927 bits\n",
      "Entropie dé équilibré        : 2.584962500721156 bits\n",
      "Entropie dé biaisé           : 2.321928094887362 bits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(p_dist: np.ndarray) -> float:\n",
    "    \"\"\"Calcule l'entropie en bits d'une distribution de probabilités.\"\"\"\n",
    "    p = np.array(p_dist, dtype=float)\n",
    "    p = p[p > 0]  # ignore les probabilités nulles pour éviter log(0)\n",
    "    return float(-np.sum(p * np.log2(p)))\n",
    "\n",
    "coin_fair = [0.5, 0.5]\n",
    "coin_biased = [0.7, 0.3]\n",
    "dice_fair = [1/6] * 6\n",
    "dice_biased = [0.4, 0.2, 0.1, 0.1, 0.1, 0.1]\n",
    "\n",
    "print(\"Entropie pièce équilibrée   :\", entropy(coin_fair), \"bits\")\n",
    "print(\"Entropie pièce biaisée 70/30 :\", entropy(coin_biased), \"bits\")\n",
    "print(\"Entropie dé équilibré        :\", entropy(dice_fair), \"bits\")\n",
    "print(\"Entropie dé biaisé           :\", entropy(dice_biased), \"bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d4dd62",
   "metadata": {},
   "source": [
    "### Lecture des résultats\n",
    "\n",
    "- **Pièce équilibrée** : ~1.0 bit (maximum pour deux issues équiprobables).\n",
    "- **Pièce biaisée 70/30** : ~0.881 bit (incertitude réduite car une issue est favorisée).\n",
    "- **Dé équilibré** : ~2.585 bits (incertitude maximale pour six issues équiprobables).\n",
    "- **Dé biaisé** : ~2.322 bits (plus petit que le dé uniforme car certaines issues sont privilégiées).\n",
    "\n",
    "> **À retenir :** l'entropie décroît dès que la distribution devient inégale, car il est plus facile de deviner l'issue. L'entropie atteint 0 bit dans les cas extrêmes où l'issue est certaine d'avance.\n",
    "\n",
    "Dans la suite du notebook, nous prolongerons ces notions avec l'entropie conditionnelle, l'information mutuelle et leur utilisation pratique pour analyser des données tabulaires."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
