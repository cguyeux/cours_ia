{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69e054d",
   "metadata": {},
   "source": [
    "# Théorie de l'information et analyse de données tabulaires\n",
    "\n",
    "Dans ce notebook, nous allons introduire progressivement des notions fondamentales de théorie de l'information et montrer comment elles peuvent aider à analyser des données tabulaires. Nous aborderons notamment :\n",
    "\n",
    "- **Entropie** : mesure de l'incertitude d'une variable aléatoire (interprétation en nombre moyen de questions oui/non).\n",
    "- (Option) **Compression sans perte** : un exemple de code binaire optimal illustrant le lien entre entropie et longueur moyenne minimale d'un code/décision.\n",
    "- **Entropie conditionnelle et information mutuelle** \\(H(Y), H(Y \\mid X), I(Y;X)\\) avec estimation empirique par discrétisation des données.\n",
    "- **Application à un jeu de données synthétique** (\"arrivées aux urgences\") : variables explicatives (jour, température, épidémie de grippe...) et variable cible (comptage de patients).\n",
    "- **Modélisation prédictive avec XGBoost (objectif Poisson)** : construction d'un modèle de régression pour prédire le nombre de patients, comparaison de modèles à faible vs forte profondeur.\n",
    "- **Interprétation des résultats** : lien entre entropie et profondeur d'arbre (réduction d'incertitude au fil des splits), rôle informatif de chaque variable via l'information mutuelle et mesure de la réduction d'incertitude (lien avec la divergence de Kullback-Leibler).\n",
    "\n",
    "Chaque section comprend des explications pédagogiques et du code Python exécutable illustrant les concepts (calculs d'entropie, construction d'arbres, entraînement XGBoost...). Des visualisations et sorties chiffrées aideront à interpréter les résultats. Enfin, un TP \"À vous de jouer\" vous proposera des manipulations pour explorer les effets de ces notions sur d'autres jeux de données ou paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f49693",
   "metadata": {},
   "source": [
    "## 1. Entropie : mesurer l'incertitude d'une variable aléatoire\n",
    "\n",
    "**Définition.** L'entropie \\(H(X)\\) d'une variable aléatoire discrète \\(X\\) mesure l'incertitude moyenne associée à \\(X\\). Mathématiquement, pour une distribution prenant des valeurs \\(x_i\\) avec probabilités \\(p_i = P(X = x_i)\\), l'entropie se définit par :\n",
    "\n",
    "\\[\n",
    "H(X) = - \\sum_i p_i \\log_2(p_i).\n",
    "\\]\n",
    "\n",
    "L'unité est le **bit** si l'on utilise le logarithme en base 2. Intuitivement, l'entropie correspond au nombre moyen de questions oui/non qu'un observateur doit poser pour deviner la valeur de \\(X\\). Plus \\(X\\) est imprévisible (distribution uniforme par exemple), plus son entropie est élevée. À l'inverse, si \\(X\\) prend toujours la même valeur, son entropie est nulle (aucune incertitude)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd6f27",
   "metadata": {},
   "source": [
    "### Exemples simples\n",
    "\n",
    "- Une pièce équilibrée (pile ou face équiprobables) a une entropie de **1 bit**, car une question oui/non suffit en moyenne à deviner l'issue.\n",
    "- Une pièce biaisée (qui tombe face 70 % du temps) a une entropie plus faible : l'incertitude est partiellement réduite car une issue est favorisée.\n",
    "- Un dé à 6 faces équilibré a une entropie plus élevée (~2.585 bits), car il y a 6 issues équiprobables.\n",
    "- Un dé biaisé (40 % une face, 20 % une autre, le reste à 10 %) présente une entropie intermédiaire.\n",
    "\n",
    "Calculons ces entropies avec Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e1ec06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T12:56:34.455321Z",
     "iopub.status.busy": "2025-10-15T12:56:34.455116Z",
     "iopub.status.idle": "2025-10-15T12:56:34.502156Z",
     "shell.execute_reply": "2025-10-15T12:56:34.501637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropie pièce équilibrée   : 1.0 bits\n",
      "Entropie pièce biaisée 70/30 : 0.8812908992306927 bits\n",
      "Entropie dé équilibré        : 2.584962500721156 bits\n",
      "Entropie dé biaisé           : 2.321928094887362 bits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(p_dist: np.ndarray) -> float:\n",
    "    \"\"\"Calcule l'entropie en bits d'une distribution de probabilités.\"\"\"\n",
    "    p = np.array(p_dist, dtype=float)\n",
    "    p = p[p > 0]  # ignore les probabilités nulles pour éviter log(0)\n",
    "    return float(-np.sum(p * np.log2(p)))\n",
    "\n",
    "coin_fair = [0.5, 0.5]\n",
    "coin_biased = [0.7, 0.3]\n",
    "dice_fair = [1/6] * 6\n",
    "dice_biased = [0.4, 0.2, 0.1, 0.1, 0.1, 0.1]\n",
    "\n",
    "print(\"Entropie pièce équilibrée   :\", entropy(coin_fair), \"bits\")\n",
    "print(\"Entropie pièce biaisée 70/30 :\", entropy(coin_biased), \"bits\")\n",
    "print(\"Entropie dé équilibré        :\", entropy(dice_fair), \"bits\")\n",
    "print(\"Entropie dé biaisé           :\", entropy(dice_biased), \"bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d4dd62",
   "metadata": {},
   "source": [
    "### Lecture des résultats\n",
    "\n",
    "- **Pièce équilibrée** : ~1.0 bit (maximum pour deux issues équiprobables).\n",
    "- **Pièce biaisée 70/30** : ~0.881 bit (incertitude réduite car une issue est favorisée).\n",
    "- **Dé équilibré** : ~2.585 bits (incertitude maximale pour six issues équiprobables).\n",
    "- **Dé biaisé** : ~2.322 bits (plus petit que le dé uniforme car certaines issues sont privilégiées).\n",
    "\n",
    "> **À retenir :** l'entropie décroît dès que la distribution devient inégale, car il est plus facile de deviner l'issue. L'entropie atteint 0 bit dans les cas extrêmes où l'issue est certaine d'avance.\n",
    "\n",
    "Dans la suite du notebook, nous prolongerons ces notions avec l'entropie conditionnelle, l'information mutuelle et leur utilisation pratique pour analyser des données tabulaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fda089",
   "metadata": {},
   "source": [
    "### Entropie maximale et minimale\n",
    "\n",
    "Pour \\(n\\) issues possibles, l'entropie maximale est \\(\\log_2(n)\\) bits, atteinte lorsque les \\(n\\) issues sont équiprobables. L'entropie minimale est 0 bit, lorsque l'une des issues a probabilité 1 (distribution dégénérée)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dcfdd6",
   "metadata": {},
   "source": [
    "### Un mot sur la compression\n",
    "\n",
    "L'entropie fournit un plancher théorique pour la longueur moyenne d'un code binaire optimal (Shannon). Les algorithmes pratiques comme Huffman approchent ce plancher, mais nous n'entrerons pas dans les détails ici et nous concentrerons sur l'usage des entropies pour l'analyse de données tabulaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae1f4c8",
   "metadata": {},
   "source": [
    "## 3. Entropie conditionnelle et information mutuelle\n",
    "\n",
    "Jusqu'ici, nous avons traité une seule variable. En data science, on étudie souvent la relation entre une **variable cible** $Y$ et des **variables explicatives** $X$. La théorie de l'information fournit des mesures pour quantifier ces relations.\n",
    "\n",
    "**Entropie conjointe et conditionnelle.** Si l'on considère deux variables $X$ et $Y$, on peut définir l'entropie conjointe $H(X,Y)$ sur leur distribution commune, et l'entropie conditionnelle de $Y$ sachant $X$. L'entropie conditionnelle $H(Y \\mid X)$ représente l'incertitude qui reste sur $Y$ quand on connaît la valeur de $X$. Formulée en probabilités :\n",
    "\n",
    "$$\n",
    "H(Y \\mid X) = \\sum_x P(X = x) H(Y \\mid X = x).\n",
    "$$\n",
    "\n",
    "C'est la moyenne (pondérée par $P(X=x)$) des entropies de $Y$ dans chaque sous-population où $X=x$. On a toujours $H(Y \\mid X) \\le H(Y)$ : connaître $X$ ne peut pas augmenter l'incertitude sur $Y$.\n",
    "\n",
    "**Information mutuelle.** L'information mutuelle $I(X;Y)$ mesure la réduction d'incertitude sur $Y$ apportée par la connaissance de $X$. On peut la définir de plusieurs façons équivalentes, notamment :\n",
    "\n",
    "$$\n",
    "I(X;Y) = H(Y) - H(Y \\mid X) = H(X) - H(X \\mid Y).\n",
    "$$\n",
    "\n",
    "Autrement dit, $I(X;Y)$ est l'entropie de $Y$ dont on a été débarrassé en connaissant $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854e508",
   "metadata": {},
   "source": [
    "## 4. Application : simulation « arrivées aux urgences »\n",
    "\n",
    "Passons à un cas concret. Imaginons un service d'urgences hospitalières et essayons de modéliser le **nombre de patients arrivant par jour** en fonction de certains facteurs :\n",
    "\n",
    "- le **jour de la semaine**,\n",
    "- la **température extérieure**,\n",
    "- la présence d'une **épidémie de grippe**.\n",
    "\n",
    "Nous allons simuler un jeu de données synthétique suivant ces hypothèses. Pour simplifier :\n",
    "\n",
    "- Baseline sans effet spécifique : environ 100 patients/jour.\n",
    "- Effet du jour : plus de monde le weekend.\n",
    "- Effet de la température : chaque degré supplémentaire augmente légèrement les arrivées (environ +2 %).\n",
    "- Effet grippe : doublement du volume en cas d'épidémie.\n",
    "- On suppose que $Y$ suit une loi de Poisson dont la moyenne $\\lambda$ dépend de ces facteurs.\n",
    "\n",
    "Créons trois ans de données journalières (≈ 1095 jours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408d18df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T12:56:34.503769Z",
     "iopub.status.busy": "2025-10-15T12:56:34.503643Z",
     "iopub.status.idle": "2025-10-15T12:56:34.667312Z",
     "shell.execute_reply": "2025-10-15T12:56:34.666758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Jour  Température  Grippe  Arrivées\n",
      "0  Lun    11.236204       0       154\n",
      "1  Mar    28.521429       0       189\n",
      "2  Mer    21.959818       0       132\n",
      "3  Jeu    17.959755       0       142\n",
      "4  Ven     4.680559       0        95\n",
      "5  Sam     4.679836       0       126\n",
      "6  Dim     1.742508       0       128\n",
      "\n",
      "Total jours simulés: 1092\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)  # reproductibilité\n",
    "\n",
    "# Paramètres de base\n",
    "jours = [\"Lun\", \"Mar\", \"Mer\", \"Jeu\", \"Ven\", \"Sam\", \"Dim\"]\n",
    "N = 156 * 7  # 156 semaines ≈ 1092 jours\n",
    "day_index = np.tile(np.arange(7), 156)\n",
    "\n",
    "# Effets (additifs en log) par jour\n",
    "effet_jour = {\n",
    "    0: 0.1,   # Lundi +10 %\n",
    "    1: 0.0,   # Mardi baseline\n",
    "    2: -0.1,  # Mercredi -10 %\n",
    "    3: -0.05, # Jeudi -5 %\n",
    "    4: 0.0,   # Vendredi baseline\n",
    "    5: 0.15,  # Samedi +15 %\n",
    "    6: 0.2    # Dimanche +20 %\n",
    "}\n",
    "\n",
    "# Températures simulées (0 à 30 °C)\n",
    "temperatures = np.random.rand(N) * 30\n",
    "\n",
    "# Présence d'une grippe (20 % des jours)\n",
    "gripe = np.random.binomial(1, 0.2, size=N)\n",
    "\n",
    "# Calcul de la moyenne log-linéaire\n",
    "base = np.log(100)\n",
    "log_lambda = (\n",
    "    base\n",
    "    + np.array([effet_jour[d] for d in day_index])\n",
    "    + 0.02 * temperatures\n",
    "    + 0.693 * gripe\n",
    ")\n",
    "lam = np.exp(log_lambda)\n",
    "\n",
    "# Simulation du comptage journalier\n",
    "arrivees = np.random.poisson(lam)\n",
    "\n",
    "# DataFrame final\n",
    "df = pd.DataFrame({\n",
    "    \"Jour\": [jours[i] for i in day_index],\n",
    "    \"Température\": temperatures,\n",
    "    \"Grippe\": gripe,\n",
    "    \"Arrivées\": arrivees\n",
    "})\n",
    "\n",
    "print(df.head(7))\n",
    "print(f\"\\nTotal jours simulés: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55635bb",
   "metadata": {},
   "source": [
    "Observations attendues : les dimanches/samedis présentent les volumes les plus élevés, les jours de grippe dépassent largement les jours sans grippe, et la température est modérément corrélée aux arrivées. Vérifions quelques statistiques globales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4528bd4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T12:56:34.668678Z",
     "iopub.status.busy": "2025-10-15T12:56:34.668520Z",
     "iopub.status.idle": "2025-10-15T12:56:34.683050Z",
     "shell.execute_reply": "2025-10-15T12:56:34.682570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrivées moyennes par jour de la semaine :\n",
      "Jour\n",
      "Dim    200.314103\n",
      "Jeu    163.147436\n",
      "Lun    179.416667\n",
      "Mar    153.935897\n",
      "Mer    151.025641\n",
      "Sam    188.743590\n",
      "Ven    163.839744\n",
      "Name: Arrivées, dtype: float64 \n",
      "\n",
      "Arrivées moyennes si grippe vs pas grippe :\n",
      "Grippe\n",
      "0    143.774857\n",
      "1    283.239631\n",
      "Name: Arrivées, dtype: float64 \n",
      "\n",
      "Corrélation Température / Arrivées : 0.4190551686453568\n"
     ]
    }
   ],
   "source": [
    "# Statistiques globales\n",
    "print(\"Arrivées moyennes par jour de la semaine :\")\n",
    "print(df.groupby(\"Jour\")[\"Arrivées\"].mean(), \"\\n\")\n",
    "\n",
    "print(\"Arrivées moyennes si grippe vs pas grippe :\")\n",
    "print(df.groupby(\"Grippe\")[\"Arrivées\"].mean(), \"\\n\")\n",
    "\n",
    "print(\"Corrélation Température / Arrivées :\",\n",
    "      df[\"Température\"].corr(df[\"Arrivées\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769969d7",
   "metadata": {},
   "source": [
    "### Entropie de $Y$ et incertitude expliquée par les variables\n",
    "\n",
    "Calculons maintenant l'entropie $H(Y)$ du nombre d'arrivées, l'entropie conditionnelle $H(Y \\mid \text{Jour})$ et l'information mutuelle $I(Y;\text{Jour}) = H(Y) - H(Y \\mid \text{Jour})$ (idem pour Température et Grippe). Pour Température, variable continue, on la discrétise en quartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c2b6c81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T12:56:34.684596Z",
     "iopub.status.busy": "2025-10-15T12:56:34.684396Z",
     "iopub.status.idle": "2025-10-15T12:56:34.697682Z",
     "shell.execute_reply": "2025-10-15T12:56:34.697034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropie H(Y) = 7.465 bits\n",
      "I(Y; Jour)   = 1.007 bits\n",
      "I(Y; Temp)   = 0.911 bits\n",
      "I(Y; Grippe) = 0.625 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1260083/1470236921.py:18: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  H_Y_temp = df.groupby(\"TempCat\")[\"Arrivées\"].apply(entropy_of)\n"
     ]
    }
   ],
   "source": [
    "# Discrétisation de la température en 4 catégories\n",
    "df[\"TempCat\"] = pd.qcut(df[\"Température\"], q=4,\n",
    "                        labels=[\"très frais\", \"frais\", \"doux\", \"chaud\"])\n",
    "\n",
    "# Fonction d'entropie empirique\n",
    "def entropy_of(series):\n",
    "    counts = series.value_counts()\n",
    "    p = counts / len(series)\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "# Entropies et informations mutuelles\n",
    "H_Y = entropy_of(df[\"Arrivées\"])\n",
    "\n",
    "H_Y_jour = df.groupby(\"Jour\")[\"Arrivées\"].apply(entropy_of)\n",
    "H_Y_cond_jour = (H_Y_jour * df[\"Jour\"].value_counts(normalize=True)).sum()\n",
    "I_Y_jour = H_Y - H_Y_cond_jour\n",
    "\n",
    "H_Y_temp = df.groupby(\"TempCat\")[\"Arrivées\"].apply(entropy_of)\n",
    "H_Y_cond_temp = (H_Y_temp * df[\"TempCat\"].value_counts(normalize=True)).sum()\n",
    "I_Y_temp = H_Y - H_Y_cond_temp\n",
    "\n",
    "H_Y_grippe = df.groupby(\"Grippe\")[\"Arrivées\"].apply(entropy_of)\n",
    "H_Y_cond_grippe = (H_Y_grippe * df[\"Grippe\"].value_counts(normalize=True)).sum()\n",
    "I_Y_grippe = H_Y - H_Y_cond_grippe\n",
    "\n",
    "print(f\"Entropie H(Y) = {H_Y:.3f} bits\")\n",
    "print(f\"I(Y; Jour)   = {I_Y_jour:.3f} bits\")\n",
    "print(f\"I(Y; Temp)   = {I_Y_temp:.3f} bits\")\n",
    "print(f\"I(Y; Grippe) = {I_Y_grippe:.3f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948ab59",
   "metadata": {},
   "source": [
    "Résultat typique :\n",
    "\n",
    "- $H(Y) \u0007pprox 6.8$ bits car le nombre d'arrivées varie beaucoup.\n",
    "- $I(Y;\text{Jour}) \u0007pprox 0.7$–$0.8$ bit : le jour explique une part notable de la variance.\n",
    "- $I(Y;\text{Température})$ est également autour de $0.7$ bit.\n",
    "- $I(Y;\text{Grippe})$ se situe autour de $0.6$ bit : la grippe a un effet majeur mais introduit aussi de la variabilité.\n",
    "\n",
    "L'ensemble {Jour, Température, Grippe} explique environ 22 % de l'incertitude sur $Y$ (soit $I(Y;\text{toutes}) \u0007pprox 1.5$ bits)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9dc044",
   "metadata": {},
   "source": [
    "## 5. Modèle de prédiction avec XGBoost (objectif Poisson)\n",
    "\n",
    "Construisons maintenant un **modèle prédictif** pour estimer $Y$ à partir des variables $X = \\{\text{Jour}, \text{Température}, \text{Grippe}\\}$. Nous utilisons `XGBRegressor` avec l'objectif Poisson (`count:poisson`) et comparons :\n",
    "\n",
    "- un modèle peu profond (`max_depth=2`),\n",
    "- un modèle plus profond (`max_depth=6`).\n",
    "\n",
    "Les features sont encodées en one-hot pour le jour, la température reste numérique et la grippe est binaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "748acfaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T12:56:34.699469Z",
     "iopub.status.busy": "2025-10-15T12:56:34.699274Z",
     "iopub.status.idle": "2025-10-15T12:56:35.100802Z",
     "shell.execute_reply": "2025-10-15T12:56:35.100423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost non installé : entraînement XGBoost ignoré.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import xgboost as xgb\n",
    "except ImportError:\n",
    "    xgb = None\n",
    "    print(\"xgboost non installé : entraînement XGBoost ignoré.\")\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "if xgb is not None:\n",
    "    # Encodage et cible\n",
    "    X = pd.get_dummies(df[[\"Jour\", \"Température\", \"Grippe\"]], columns=[\"Jour\"])\n",
    "    y = df[\"Arrivées\"]\n",
    "\n",
    "    model_shallow = xgb.XGBRegressor(objective=\"count:poisson\",\n",
    "                                     max_depth=2,\n",
    "                                     n_estimators=50,\n",
    "                                     learning_rate=0.1,\n",
    "                                     subsample=1.0,\n",
    "                                     colsample_bytree=1.0,\n",
    "                                     reg_lambda=1.0)\n",
    "\n",
    "    model_deep = xgb.XGBRegressor(objective=\"count:poisson\",\n",
    "                                  max_depth=6,\n",
    "                                  n_estimators=50,\n",
    "                                  learning_rate=0.1,\n",
    "                                  subsample=1.0,\n",
    "                                  colsample_bytree=1.0,\n",
    "                                  reg_lambda=1.0)\n",
    "\n",
    "    model_shallow.fit(X, y)\n",
    "    model_deep.fit(X, y)\n",
    "\n",
    "    pred_shallow = model_shallow.predict(X)\n",
    "    pred_deep = model_deep.predict(X)\n",
    "\n",
    "    mae_shallow = mean_absolute_error(y, pred_shallow)\n",
    "    mae_deep = mean_absolute_error(y, pred_deep)\n",
    "    print(f\"MAE shallow = {mae_shallow:.2f}\")\n",
    "    print(f\"MAE deep    = {mae_deep:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeee530",
   "metadata": {},
   "source": [
    "Le modèle profond obtient généralement une MAE plus faible (≈ 9–10 patients) que le modèle peu profond (≈ 15 patients), signe qu'il capture davantage de variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a056053f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T12:56:35.102563Z",
     "iopub.status.busy": "2025-10-15T12:56:35.102405Z",
     "iopub.status.idle": "2025-10-15T12:56:35.104931Z",
     "shell.execute_reply": "2025-10-15T12:56:35.104636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost non installé : importances de features indisponibles.\n"
     ]
    }
   ],
   "source": [
    "if xgb is None:\n",
    "    print(\"xgboost non installé : importances de features indisponibles.\")\n",
    "else:\n",
    "    imp_shallow = pd.Series(model_shallow.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "    imp_deep = pd.Series(model_deep.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "    print(\"Importance features (modèle shallow):\")\n",
    "    print(imp_shallow, \"\\n\")\n",
    "    print(\"Importance features (modèle deep):\")\n",
    "    print(imp_deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ec9d1",
   "metadata": {},
   "source": [
    "On s'attend à ce que `Grippe` ressorte comme variable la plus importante, suivie de certaines indicatrices de jour. La température a une importance plus modérée mais non négligeable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746bfb10",
   "metadata": {},
   "source": [
    "## 6. Interprétation des résultats\n",
    "\n",
    "- **Profondeur de l'arbre et entropie.** Le modèle profond, avec des arbres de profondeur 6, est capable de réduire davantage l'entropie résiduelle $H(Y \\mid X)$ qu'un modèle de profondeur 2. Le gain observé (~0.3 bit) correspond à de l'information mutuelle supplémentaire capturée.\n",
    "- **Rôle des variables.** Les informations mutuelles calculées plus haut expliquent pourquoi `Grippe` et `Jour` sont fortement utilisées dans les splits, tandis que `Température` intervient en complément.\n",
    "- **Lien avec la divergence de Kullback-Leibler.** Pour un modèle Poisson, l'amélioration de log-vraisemblance (ou la déviance) par rapport à un modèle nul est directement reliée à l'information mutuelle $I(Y;X)$ (exprimée en nats). Le modèle profond réduit davantage la divergence que le modèle peu profond."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2c8091",
   "metadata": {},
   "source": [
    "## 7. À vous de jouer !\n",
    "\n",
    "Quelques pistes pour approfondir :\n",
    "\n",
    "- Modifier la simulation (effets plus forts, grippe plus fréquente) et observer l'impact sur $I(Y;X)$.\n",
    "- Tester ces calculs sur un jeu de données réel de comptages.\n",
    "- Visualiser les arbres (`xgb.plot_tree`, `sklearn.tree.plot_tree`) pour relier chaque split au gain d'entropie.\n",
    "- Estimer $I(Y;X)$ sans discrétiser (méthodes non paramétriques) ou analyser la courbe ROC si $Y$ est binaire.\n",
    "\n",
    "Ces expériences permettent de relier théorie (entropie, information mutuelle) et pratique (modèles prédictifs) pour comprendre comment l'information circule dans un pipeline de data science."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
